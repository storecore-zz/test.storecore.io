[
  {
    "path": "/about",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "about us",
    "content": "Vamp is being developed by Magnetic.io from the heart of beautiful Amsterdam.  \n\nWe’re dedicated to helping organisations and companies of all sizes increase efficiency, save time and reduce costs.\nWe provide powerful and easy-to-use solutions to optimise modern architectures and systems, often based around microservices and/or containers.\n\nVision\nTime is precious. We believe people should focus their efforts on the things they excel at and that matter to them. Let computers do the repeatable and automatable tasks - they’re far more suited to that than we are. Smart automation FTW!\n\n Community\nWe encourage anyone to join the Vamp community and pitch in with pull requests, bug reports and feature requests to make Vamp even more awesome.\n\nWorking at Vamp\nWe're alway on the lookout for smart, talented and motivated people that love to work on products and understand real-world problems and how to solve them in the best possible way. If you would love to join our team and work on Vamp and related products, please get in touch!\n\n Contact\nYou can contact us here\n",
    "id": 0
  },
  {
    "path": "/contact",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "contact",
    "content": "\nVamp is being developed by Magnetic.io in the heart of Amsterdam.\n\nOur Amsterdam offices:\n\nSint Antoniesbreestraat 16  \n1011 HB Amsterdam  \nThe Netherlands  \n+31(0)88 555 33 99  \ninfo@magnetic.io\n\nProfessional services and consultancy\nWe can provide professional services and consultancy around the implementation and use of Vamp. Send us an email on info@magnetic.io or call +31(0)88 555 33 99.\n\n Vamp Enterprise Edition (EE)\nWe also provide a commercial Enterprise Edition of Vamp with features specifically tuned to enterprise usage. Contact us to get more information.\n\nSupport\nCheck our support page for SLA's and other forms of support.\n\n Community\nJoin the Vamp community to make Vamp even better.\n",
    "id": 1
  },
  {
    "path": "/documentation/api/v0.9.1/api-reference",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "API Reference",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-0.9.1\"",
    "    weight": "   weight: 12",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nThis page gives full details of all available API calls. See using the Vamp API for details on pagination, JSON and YAML content types and effective use of the API.\n\nResource descriptions: blueprints, breeds, conditions, escalations, scales, slas\nRuntime entities: deployments, deployment scales, deployment SLAs, gateways  \nData: events, health, metrics\nSystem: info, config, haproxy\nDebug: sync, sla, escalation\n\n-----------,\nBlueprints\nRead about using blueprints.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List blueprints\n\nLists all blueprints without any pagination or filtering.\n\n    GET /api/v1/blueprints\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\nGet a single blueprint\n\nLists all details for one specific blueprint.\n\n    GET /api/v1/blueprints/ \n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\n Create blueprint\n\nCreates a new blueprint. Accepts JSON or YAML formatted blueprints. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/blueprints\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 201 Created if the blueprint is valid.This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a blueprint\n\nUpdates the content of a specific blueprint.\n\n    PUT /api/v1/blueprints/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 200 OK if the blueprint is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a blueprint\n\nDeletes a blueprint.        \n\n    DELETE /api/v1/blueprints/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the blueprint.\n\n---------,\nBreeds\nRead about using breeds.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List breeds\n\nLists all breeds without any pagination or filtering.\n\n    GET /api/v1/breeds\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. 400 Bad Request in case some dependencies are not yet fully defined.\n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\nGet a single breed\n\nLists all details for one specific breed.\n\n    GET /api/v1/breeds/ \n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. 400 Bad Request in case some dependencies are not yet fully defined.\n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\n Create breed\n\nCreates a new breed. Accepts JSON or YAML formatted breeds. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/breeds\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the breed is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a breed\n\nUpdates the content of a specific breed.\n\n    PUT /api/v1/breeds/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the breed is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a breed\n\nDeletes a breed.        \n\n    DELETE /api/v1/breeds/ \n    \n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the breed.\n\n------,\nConditions\nRead about using conditions.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List conditions\n\nLists all conditions without any pagination or filtering.\n\n    GET /api/v1/conditions\n\nGet a single condition\n\nLists all details for one specific condition.\n\n    GET /api/v1/conditions/ \n\n Create condition\n\nCreates a new condition. Accepts JSON or YAML formatted conditions. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/conditions\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a condition\n\nUpdates the content of a specific condition.\n\n    PUT /api/v1/conditions/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a condition\n\nDeletes a condition.\n\n    DELETE /api/v1/conditions/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the escalation.\n\n-------,\n\nEscalations\n\nRead about using escalations.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List escalations\n\nLists all escalations without any pagination or filtering.\n\n    GET /api/v1/escalations\n\nGet a single escalation\n\nLists all details for one specific escalation.\n\n    GET /api/v1/escalations/ \n\n Create escalation\n\nCreates a new escalation. Accepts JSON or YAML formatted escalations. Set the Content-Type request header to application/json or application/x-yaml accordingly.   \n\n    POST /api/v1/escalations\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate an escalation\n\nUpdates the content of a specific escalation.\n\n    PUT /api/v1/escalations/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete an escalation\n\nDeletes an escalation.        \n\n    DELETE /api/v1/escalations/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the escalation.\n\n--------,\nScales\n\nRead about using scales.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List scales\n\nLists all scales without any pagination or filtering.\n\n    GET /api/v1/scales\n\nGet a single scale\n\nLists all details for one specific scale.\n\n    GET /api/v1/scales/ \n\n Create scale\n\nCreates a new scale. Accepts JSON or YAML formatted scales. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/scales\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the scale and returns a 201 Created if the scale is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a scale\n\nUpdates the content of a specific scale.\n\n    PUT /api/v1/scales/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the scale and returns a 200 OK if the scale is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a scale\n\nDeletes a scale.        \n\n    DELETE /api/v1/scales/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the scale.\n\n----------,\nSLAs\n\nRead about using SLAs.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List SLAs\n\nLists all slas without any pagination or filtering.\n\n    GET /api/v1/slas\n\nGet a single SLA\n\nLists all details for one specific breed.\n\n    GET /api/v1/slas/ \n\n Create an SLA\n\nCreates a new SLA\n\n    POST /api/v1/slas   \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the SLA and returns a 201 Created if the SLA is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate an SLA\n\nUpdates the content of a specific SLA.\n\n    PUT /api/v1/slas/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the SLA and returns a 200 OK if the SLA is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete an SLA\n\nDeletes an SLA.        \n\n    DELETE /api/v1/slas/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the SLA.\n\n----------,\nDeployments\n\nDeployments are non-static entities in the Vamp eco-system. They represent runtime structures so any changes to them will take time to execute and can possibly fail. Most API calls to the /deployments endpoint will therefore return a 202: Accepted return code, indicating the asynchronous nature of the call.\n\nDeployments have a set of sub resources: SLAs, scales and gateways. These are instantiations of their static counterparts.\n\nRead more about using deployments.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List deployments\n\n\tGET /api/v1/deployments\n\n| parameter         | options           | default          | description      |\n| ----------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint      | true or false     | false            | exports each deployment as a valid blueprint. This can be used together with the header Accept: application/x-yaml to export in YAML format instead of the default JSON. |\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences  | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\nGet a single deployment\n\nLists all details for one specific deployment.\n\n    GET /api/v1/deployments/ \n\n| parameter         | options           | default          | description      |\n| ----------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint     | true or false     | false            | exports the deployment as a valid blueprint. This can be used together with the header Accept: application/x-yaml to export in YAML format instead of the default JSON. |\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences   | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\n Create deployment using a blueprint\n\nCreates a new deployment\n\n\tPOST /api/v1/deployments\n\nCreate a named (non UUID) deployment\n\n\tPUT /api/v1/deployments/ \n\t\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the blueprint is valid for deployment. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a deployment using a blueprint\n\nUpdates the settings of a specific deployment.\n\n    PUT /api/v1/deployments/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the deployment after the update would be still valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a deployment using a blueprint\n\nDeletes all or parts of a deployment.        \n\n    DELETE /api/v1/deployments/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the deployment after the (partial) deletion would be still valid. Actual delete is not performed.\n\nIn contrast to most API's, doing a DELETE in Vamp takes a request body that designates what part of the deployment should be deleted. This allows you to remove specific services, clusters of the whole deployment.\n\n  \nDELETE on deployment with an empty request body will not delete anything.\n  \n\nThe most common way to specify what you want to delete is by exporting the target deployment as a blueprint using the ?as_blueprint=true parameter. You then either programmatically or by hand edit the resulting blueprint and specify which of the services you want to delete. You can also use the blueprint as a whole in the DELETE request. The result is the removal of the full deployment. \n\nexample - delete service\n\nThis is our (abbreviated) deployment in YAML format. We have two clusters. The first cluster 'frontend' has two services.\nWe have left out some keys like scale among others as they have no effect on this specific use case.\n\n\t\tGET /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcd?as_blueprint=true\n\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nendpoints:\n  frontend.port: '9050'\nclusters:\n  frontend:\n    services:\n    breed:\n        name: monarch_front:0.1\n        deployable: magneticio/monarch:0.1\n        ports:\n          port: 8080/http\n        constants: { \n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n    breed:\n        name: monarch_front:0.2\n        deployable: magneticio/monarch:0.2\n        ports:\n          port: 8080/http\n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n  backend:\n    services:\n    breed:\n        name: monarch_backend:0.3\n        deployable: magneticio/monarch:0.3\n        ports:\n          jdbc: 8080/http\n        environment_variables: { \n\nIf we want to delete the first service in the frontend cluster, we use the following blueprint as the request body in the DELETE action.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n\nIf we want to delete the whole deployment, we just specify all the clusters and services.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t  \n\t\t  \nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n    breed:\n        ref: monarch_front:0.2\n  backend:\n    services:\n    breed:\n        ref: monarch_backend:0.3\n\n----------,\n Deployment scales\n\nDeployment scales are singular resources: you only have one scale per service. Deleting a scale is not a meaningful action.\n\nGet a deployment scale\n\nLists all details for a specific deployment scale that's part of a service inside a cluster.\n\n\tGET /api/v1/deployments/ /clusters/ /services/ /scale\n\t\n Set a deployment scale\t\n\nUpdates a deployment scale.\n\n\tPOST|PUT /api/v1/deployments/ /clusters/ /services/ /scale\n\t\n-----------,\nDeployment SLAs\n\n Get a deployment SLA\n\nLists all details for a specific SLA that's part of a specific cluster.\n\n\tGET /api/v1/deployments/ /clusters/ /sla\n\t\nSet a deployment SLA\n\nCreates or updates a specific deployment SLA.\n\n\tPOST|PUT /api/v1/deployments/ /clusters/ /sla\n\t\n Delete a deployment SLA\n\nDeletes as specific deployment SLA.\n\n\tDELETE /api/v1/deployments/ /clusters/ /sla\n\n-----------,\nGateways\nRead about using gateways.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List gateways\n\n    GET /api/v1/gateways\n\nGet a single gateway\n\n    GET /api/v1/gateways/ \n\n Create gateway\nAccepts JSON or YAML formatted gateways. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/gateways\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the gateway and returns a 201 Created if the gateway is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a gateway\n\n    PUT /api/v1/gateways/ \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the gateway and returns a 200 OK if the gateway is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a gateway     \n\n    DELETE /api/v1/gateways/ \n    \n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the gateway.\n\n------,\nEvents\n\nFor specific usage examples, see using events.  \nDetails on pagination, JSON and YAML content types and effective use of the API can be found in using the Vamp API.\n\n List events\n\nLists events (for example health and metrics). You can optionally filter the returned events by tag(s) or type. \n\n    GET /api/v1/events\n\n| parameter     | description      |\n| ------------- |:----------------:|\n| tags           | Filter returned events by tag(s), for example GET /api/v1/events?tags=archiving&tag=breeds\n| type           | Filter returned events by type, for example GET /api/v1/events?type=metrics\n\n  \nsearch criteria can be set in request body, checkout examples for event stream.\n  \n\nCreate events\n\n    POST /api/v1/events    \n    \n Server-sent events (SSE)\n\n    GET  /api/v1/events/stream\n\n--------,## Health\n\nHealth is a specific type of Vamp event, calculated by a Vamp workflow and required for the Vamp UI.\nHealth can be defined on gateways and deployment ports and retrieved:\n\n/api/v1/health/gateways/ \n/api/v1/health/gateways/ /routes/$route\n\n/api/v1/health/deployments/ \n/api/v1/health/deployments/ /clusters/ \n/api/v1/health/deployments/ /clusters/ /services/ \n\nHealth is value between 1 (100% healthy) and 0.\n  \nHealth is calculated using external services, e.g. Vamp workflows.\n  \n\n--------,\nMetrics\n\nMetrics can be defined on gateways and deployment ports and retrieved:\n\n/api/v1/metrics/gateways/ / \n/api/v1/metrics/gateways/ /routes/$route/ \n\n/api/v1/metrics/deployments/ /clusters/ /ports/ / \n/api/v1/metrics/deployments/ /clusters/ /services/ /ports/ / \n\n Example\n    /api/v1/metrics/deployments/sava/clusters/frontend/ports/api/response-time\n\n  \nMetrics are calculated using external services, e.g. Vamp workflows.\n  \n\n------------,\nSystem\n\nVamp provides a set of API endpoints that help with getting general health/configuration status.\n\n Get runtime info\n\nLists information about Vamp's JVM environment and runtime status. \nAlso lists info for configured persistence layer and container driver status.\n\n\tGET /api/v1/info\n\t\nSections are jvm, persistence, keyvalue, pulse, gatewaydriver, containerdriver and workflowdriver:\n\n ,\n    \"persistence\":  ,\n    \"key_value\":  ,\n    \"pulse\":  ,\n    \"gateway_driver\":  ,\n    \"container_driver\":  ,\n    \"workflow_driver\":  \n \n\nExample - explicitly request specific sections\nExplicitly requesting jvm and persistence using parameter(s) on:\n\n\tGET /api/v1/info?on=jvm&on=persistence\n\n Get Vamp configuration\n\n\tGET /api/v1/config\n\nGet HAProxy configuration\nYou can retrieve the HAProxy configuration generated by Vamp. Details will only be returned for the HAProxy version specified in your Vamp configuration.\n\n\tGET /api/v1/haproxy/ \n\n Debug \n\nForce sync\n\nForces Vamp to perform a synchronization cycle, regardless of the configured default interval.\n\n\tGET /api/v1/sync\n\t\n Force SLA check\t\n\nForces Vamp to perform an SLA check, regardless of the configured default interval.\n\n\tGET /api/v1/sla\n\nForce escalation\t\n\nForces Vamp to perform an escalation check, regardless of the configured default interval.\n\n\tGET /api/v1/escalation\n\n------------,See using the Vamp API for details on pagination, JSON and YAML content types and effective use of the API",
    "id": 2
  },
  {
    "path": "/documentation/api/v0.9.1/using-the-api",
    "date": "2016-10-19T09:00:00+00:00",
    "title": "Using the Vamp API",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"using-vamp-api-0.9.1\"",
    "    weight": "   weight: 22",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\n  \nWe've now added support for Websockets to our HTTP API. More information will follow soon.\n  \n\nVamp has one REST API. This page explains how to specify pagination, and json and yaml content types, and how to effectively use the Vamp REST API.\n\nSee also\nFull details of all available API calls\n\n Content types\n\nVamp requests can be in YAML format or JSON format. Set the Content-Type request header to application/x-yaml or application/json accordingly.\nVamp responses can be in YAML format or JSON format. Set the Accept request header to application/x-yaml or application/json accordingly.\n\nPagination\n\nVamp API endpoints support pagination with the following scheme:\n\nRequest parameters page (starting from 1, not 0) and per_page (by default 30) e.g:\n\nGET http://vamp:8080/api/v1/breeds?page=5&per_page=20\n\nResponse headers X-Total-Count giving the total amount of items (e.g. 349673) and a Link header for easy traversing, e.g.\nX-Total-Count: 5522\nLink:\n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=first,\n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=prev,\n  http://vamp:8080/api/v1/events/get?page=2&per_page=5; rel=next,\n  http://vamp:8080/api/v1/events/get?page=19&per_page=5; rel=last\n\nSee Github's implementation for more info.\n\n Return codes\n\nCreate & Delete operations are idempotent: sending the second request with the same content will not result to an error response (4xx).\nAn update will fail (4xx) if a resource does not exist.\nA successful create operation has status code 201 Created and the response body contains the created resource.\nA successful update operation has status code 200 OK or 202 Accepted and the response body contains the updated resource.\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\nSending multiple artifacts (documents) - POST, PUT and DELETE\n\nIt is possible to send YAML document containing more than 1 artifact definition:\n\nGET /api/v1\n\nSupported methods are POST, PUT and DELETE. Example:\n\n,name: ...\nkind: breed\n breed definition ...\n,name: ...\nkind: blueprint\nblueprint definition ...\n\nAdditional kind field is required and it always correspond (singular form) to type of the artifact.\nFor instance if specific endpoint would be /api/v1/deloyments then the same deployment request can be sent to api/v1 with additional kind: deployment.\nIf specific endpoints are used (e.g. /api/v1/blueprints) then kind needs to be ommited.\n\n----------,\n See also\nFull details of all available API calls",
    "id": 3
  },
  {
    "path": "/documentation/api/v0.9.2/api-blueprints",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Blueprints",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-blueprints-092\"",
    "    weight": "   weight: 40",
    "aliases": "aliases:",
    "content": "Blueprints are static artifacts. They describe how breeds work in runtime and what properties they should have. Read about using blueprints.\n\nActions\n \n List - return a list of all stored blueprints\n Get - get a single stored blueprint\n Create - create a new blueprint \n Update - update an existing blueprint\n Delete - delete a stored blueprint\n\n Blueprint resource\n\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully store a blueprint.\n\nname: sava\nclusters:\n  sava:\n    services:\n    breed: sava:1.0.0\n\n API return resource\nThe fields returned by the API for stored blueprints.\n\nname: sava\nkind: blueprint\ngateways: { \nclusters:\n  sava:\n    services:\n    breed:\n        reference: sava:1.0.0\n      environment_variables: { \n      arguments: []\n      dialects: { \n    gateways: { \n    dialects: { \nenvironment_variables: { \n\n Field name    |  Required  | description          \n --------------|---|--------------, name | yes |\n kind | optional | The resource type. Required to send multiple resources to /api/v1\n gateways | optional |  Can be created separately and referenced from here or defined inline as part of the blueprint. See gateway resource\n clusters | yes  |\n services | yes |\n breed |  yes |Can be created separately and referenced from here or defined inline as part of the blueprint. See breed resource\n environment variables | optional |\n scale | optional | Can be created separately and referenced from here or defined inline as part of the blueprint. If omitted, the default scale will be used (See scale resource and reference.conf default scale)\n\n--------,\nList blueprints\n\nReturn a list of all stored blueprint resources. For details on pagination see common parameters\n\n Request\n\nGET \n/api/v1/blueprints\nThe request body should be empty.\nQuery string parameters:\n\n| Request parameters         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\nResponse\nIf successful, will return a list of blueprint resources in the specified accept format (default JSON).  \n\n Errors\n400 bad request - expand_references set to true and some breeds are not yet fully defined.\n\n--------,\nGet single blueprint\n\nReturn details of a single, specified blueprint.\n\n Request \n\nGET \n/api/v1/blueprints/ \nThe request body should be empty.\nQuery string parameters:\n\n| Request parameters         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\nResponse \nIf successful, will return the named blueprint resource in the specified accept format (default JSON).\n\n Errors\n400 bad request - expand_references set to true and some breeds are not yet fully defined.\n\n--------,\nCreate blueprint\n\nStore a new blueprint. Note that create operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\n Request\n\nPOST\n/api/v1/blueprints\nThe request body should contain at least a minimum blueprint resource in the specified content-type format (default JSON).\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 201 Created if the blueprint is valid.  \n\nResponse\nA successful create operation has status code 201 Created and the response body will contain the created blueprint resource in the specified accept format (default JSON). \n\n--------,\n Update blueprint\n\nUpdate the content of a stored blueprint.\n\nRequest\n\nPUT\n/api/v1/blueprints/ \nThe request body should contain at least a minimum blueprint resource in the specified content-type (default JSON). The name field must match the blueprint_name specified in the request path.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 200 OK if the blueprint is valid.  \n\n Response\nA successful update operation has status code 200 OK or 202 Accepted and the response body will contain the updated blueprint resource in the specified accept format (default JSON).\n\nErrors\n\n4xx - an update will fail if a resource does not exist\nInconsistent name - the blueprint_name in the request path does not match the name field in the request body.\n\n--------,\n Delete blueprint\nDelete a stored blueprint. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\nRequest\n\nDELETE \n/api/v1/blueprints/ \nThe request body should be empty.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the blueprint.\n\n Response\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n--------------",
    "id": 4
  },
  {
    "path": "/documentation/api/v0.9.2/api-breeds",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Breeds",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-breeds-092\"",
    "    weight": "   weight: 50",
    "aliases": "aliases:",
    "content": "Breeds are static artifacts that describe individual services and can be referenced from blueprints or workflows. Read about using breeds.\n\nActions\n \n List - return a list of all stored breeds\n Get - get a single stored breed\n Create - create a new breed \n Update - update a stored breed\n Delete - delete a stored breed\n\n Breed resource\nYou can define breeds inline or store them separately under a unique name and reference them from a blueprint, deployment or workflow resource.\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully store a breed.\n\nname: sava:1.0.0\ndeployable: magneticio/sava:1.0.0\n\n API return resource\nThe fields returned by the API for stored breeds.\n\nname: sava:1.0.0\nkind: breed\ndeployable:\n  type: container/docker\n  definition: magneticio/sava:1.0.0\nports: { \nenvironment_variables: { \nconstants: { \narguments: []\ndependencies: { \n\n Field name   |  Options   | Required | description          \n ------------|-----|--------|------,name  |   |  yes  | a unique name to reference the breed\nkind  | breed  |  optional  | The resource type. Required to send multiple resources to /api/v1\ndeployable  |   |  yes  |  the default deployable type is container/docker\nports  |   |  optional  |  \nenvirontment_variables  |   |  optional  |\nconstants  |   |  optional  |\narguments  |   |  optional  |\ndependencies  |   |  optional  |  \n\n--------------  \n  \nList breeds\n\nReturns a list of all stored breeds. For details on pagination see common parameters.\n\n Request\nGET\n/api/v1/breeds\nThe request body should be empty.\nQuery string parameters:\n  \n| Request parameters         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. \n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\nResponse\nIf successful, will return a list of breed resources in the specified accept format (default JSON). For details on pagination see common parameters.\n\n Errors\n400 Bad Request - expand_references set to true and some dependencies are not yet fully defined.\n\n-----------,\nGet single breed\n\nReturns a specific stored breed.\n\n Request\nGET\n/api/v1/breeds/ \nThe request body should be empty.\nQuery string parameters:\n  \n| Request parameters         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. \n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\nResponse\nIf successful, will return the specified breed resource in the specified accept format (default JSON)\n\n Errors\nThe requested resource could not be found.\n400 Bad Request - expand_references set to true and some dependencies are not yet fully defined.\n\n-----------,\nCreate breed\n\nCreates a breed with the specified fields.\n\n Request\nPOST \n/api/v1/breeds\nThe request body should include at least the minimum breed resource in the specified content-type format (default JSON). \nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the breed is valid\n\nResponse\nIf successful, will return the created breed resource in the specified accept format (default JSON).\n\n-----------,\n Update breed\n\nUpdates the specified field(s) of a stored breed.\n\nRequest\nPUT\n/api/v1/breeds/ \nThe request body should include at least the minimum breed resource in the specified content-type format (default JSON). The name field must match the breed_name specified in the request path.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the breed is valid\n\n Response\nIf successful, will return the updated breed resource in the specified accept format (default JSON).\n\nErrors\nInconsistent name - the breed_name in the request path does not match the name field in the request body.\n\n-----------,\n Delete breed\n\nDeletes the specified breed. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\nRequest\n\nDELETE\n/api/v1/breeds/ \nThe request body should be empty.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 204 No Content if the breed is valid, without actual delete of the breed.\n\n Response\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n--------------",
    "id": 5
  },
  {
    "path": "/documentation/api/v0.9.2/api-conditions",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Conditions",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-conditions-092\"",
    "    weight": "   weight: 60",
    "aliases": "aliases:",
    "content": "Condition templates are static artifacts. You can save and manage condition templates through the API, these can then be referenced and applied to gateways. Read about using conditions and using gateways.\n\nActions\n \n List - return a list of all stored condition templates\n Get - get a single stored condition template\n Create - create a new condition template\n Update - update a condition template\n Delete - delete a stored condition template\n\n Condition resource\nYou can define conditions inline or store them separately as templates under a unique name and reference them from a blueprint, breed or gateway resource.\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully store a condition.\n\nname: sava\ncondition: User-Agent = Chrome\n\n API return resource\nThe fields returned by the API for stored conditions.\n\nname: sava\nkind: condition\ncondition: User-Agent = Chrome \n\n Field name    |  Required  | description          \n --------------|---|--------------, name | yes |  Unique name used to reference the condition from a gateway\n kind | optional | The resource type. Required to send multiple resources to /api/v1\n condition | yes | Boolean condition statement. See using conditions for details on how to create a condition.\n  \n--------,\nList conditions\n\nReturns a list of all stored conditions. For details on pagination see common parameters.\n\n Request\nGET \n/api/v1/conditions\nThe request body should be empty.\n\nResponse\nIf successful, will return a list of all stored condition resources in the specified accept format (default JSON).\n\n--------,\n Get condition\n\nReturn a specific stored condition.\n\nRequest \nGET\n/api/v1/conditions/ \nThe request body should be empty.\n\n Response \nIf successful, will return the named condition resource in the specified accept format (default JSON).\n\n--------,\nCreate condition\n\nCreate a condition template.\n\n Request\nPOST\n/api/v1/conditions/\nThe request body should include at least a minimum condition resource in the specified Content-Type format (default JSON).\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the condition is valid\n\nResponse\nIf successful, will return the stored condition resource in the specified accept format (default JSON).\n\n--------,\n Update condition\n\nUpdate a stored condition template.\n\nRequest \nPUT \n/api/v1/conditions/ \nThe request body should include at least a minimum condition resource in the specified Content-Type format (default JSON). The name field must match the   used in the request path.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the condition is valid\n\n Response\nIf successful, will return the updated condition resource in the specified accept format (default JSON).\n\nErrors\nInconsistent name - the condition_name in the request path does not match the name field in the request body.\n\n--------,\n Delete condition\n\nDelete a stored condition template. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\nRequest \nDELETE \n/api/v1/conditions/ \nThe request body should be empty.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 204 No Content if the condition is valid, without actual delete of the breed.\n\n Response\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n--------,",
    "id": 6
  },
  {
    "path": "/documentation/api/v0.9.2/api-config",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Config",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-config-092\"",
    "    weight": "   weight: 70",
    "aliases": "aliases:",
    "content": "\nVamp's parameters can be retrieved from the /config APi endpoint. Read more about Vamp configuration\n\t\nActions\n \n List - returns a full list of all config parameters.\n Get - explicitly request a specific config parameter.\n\n Config parameters\nConfig parameters are described in detail in the configuration reference.  \nThe example below is in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nvamp.container-driver.rancher.workflow-name-prefix:  \nvamp.workflow-driver.workflow.scale.cpu:  \nvamp.http-api.ui.index:  \nvamp.stats.timeout:  \nvamp.operation.synchronization.period:  \nvamp.workflow-driver.type:  \nvamp.common.http.client.tls-check:  \nvamp.http-api.response-timeout:  \nvamp.workflow-driver.workflow.deployables.\"application/javascript\".type:  \nvamp.lifter.artifact.files:\n \n \nvamp.gateway-driver.elasticsearch.metrics.type:  \nvamp.container-driver.rancher.environment.deployment.name-prefix:  \nvamp.lifter.pulse.enabled:  \nvamp.operation.deployment.scale.instances:  \nvamp.persistence.key-value-store.consul.url:  \nvamp.container-driver.mesos.url:  \nvamp.operation.gateway.virtual-hosts.formats.deployment-cluster-port:  \nvamp.lifter.artifact.override:  \nvamp.persistence.key-value-store.zookeeper.servers:  \nvamp.workflow-driver.workflow.command:  \nvamp.container-driver.marathon.password:  \nvamp.gateway-driver.elasticsearch.metrics.index:  \nvamp.container-driver.marathon.user:  \nvamp.container-driver.type:  \nvamp.persistence.database.key-value.caching:  \nvamp.container-driver.marathon.sse:  \nvamp.workflow-driver.workflow.network:  \nvamp.container-driver.kubernetes.create-services:  \nvamp.operation.gateway.response-timeout:  \nvamp.container-driver.rancher.environment.name:  \nvamp.operation.sla.period:  \nvamp.container-driver.marathon.workflow-name-prefix:  \nvamp.workflow-driver.workflow.scale.instances:  \nvamp.gateway-driver.haproxy.virtual-hosts.ip:  \nvamp.operation.synchronization.check.cpu:  \nvamp.workflow-driver.workflow.arguments:  \nvamp.gateway-driver.haproxy.template:  \nvamp.model.default-deployable-type:  \nvamp.persistence.database.elasticsearch.response-timeout:  \nvamp.http-api.port:  \nvamp.container-driver.kubernetes.url:  \nvamp.operation.escalation.period:  \nvamp.container-driver.marathon.url:  \nvamp.persistence.key-value-store.zookeeper.connect-timeout:  \nvamp.pulse.elasticsearch.index.name:  \nvamp.container-driver.docker.repository.password:  \nvamp.persistence.database.elasticsearch.url:  \nvamp.container-driver.rancher.user:  \nvamp.workflow-driver.workflow.environment-variables:\nVAMP_URL= \nVAMPKEYVALUESTOREPATH= \nVAMPKEYVALUESTORETYPE= \nVAMPKEYVALUESTORECONNECTION= \nVAMPWORKFLOWEXECUTION_PERIOD= \nVAMPWORKFLOWEXECUTION_TIMEOUT= \nvamp.container-driver.kubernetes.bearer:  \nvamp.gateway-driver.response-timeout:  \nvamp.gateway-driver.haproxy.http-log-format:  \nvamp.operation.metrics.window:  \nvamp.persistence.database.type:  \nvamp.http-api.ui.directory:  \nvamp.lifter.artifact.postpone:  \nvamp.operation.synchronization.check.instances:  \nvamp.pulse.elasticsearch.url:  \nvamp.container-driver.kubernetes.token:  \nvamp.operation.gateway.virtual-hosts.enabled:  \nvamp.http-api.strip-path-segments:  \nvamp.container-driver.rancher.password:  \nvamp.operation.deployment.scale.memory:  \nvamp.info.timeout:  \nvamp.gateway-driver.haproxy.virtual-hosts.port:  \nvamp.operation.synchronization.initial-delay:  \nvamp.operation.deployment.arguments:\nprivileged= \nvamp.persistence.key-value-store.base-path:  \nvamp.workflow-driver.chronos.url:  \nvamp.operation.check.memory:  \nvamp.container-driver.rancher.url:  \nvamp.operation.synchronization.mailbox.mailbox-capacity:  \nvamp.persistence.key-value-store.type:  \nvamp.workflow-driver.workflow.scale.memory:  \nvamp.lifter.artifact.enabled:  \nvamp.pulse.response-timeout:  \nvamp.operation.synchronization.mailbox.mailbox-type:  \nvamp.http-api.sse.keep-alive-timeout:  \nvamp.gateway-driver.haproxy.tcp-log-format:  \nvamp.pulse.type:  \nvamp.operation.health.window:  \nvamp.container-driver.docker.repository.username:  \nvamp.workflow-driver.workflow.deployables.\"application/javascript\".definition:  \nvamp.operation.synchronization.check.memory:  \nvamp.persistence.key-value-store.zookeeper.session-timeout:  \nvamp.operation.gateway.virtual-hosts.formats.deployment-port:  \nvamp.container-driver.kubernetes.service-type:  \nvamp.container-driver.kubernetes.vamp-gateway-agent-id:  \nvamp.operation.synchronization.timeout.ready-for-undeployment:  \nvamp.container-driver.docker.workflow-name-prefix:  \nvamp.container-driver.kubernetes.workflow-name-prefix:  \nvamp.persistence.key-value-store.etcd.url:  \nvamp.gateway-driver.host:  \nvamp.operation.gateway.virtual-hosts.formats.gateway:  \nvamp.container-driver.docker.repository.server-address:  \nvamp.http-api.host:  \nvamp.lifter.artifact.resources:  \nvamp.lifter.persistence.enabled:  \nvamp.workflow-driver.response-timeout:  \nvamp.gateway-driver.haproxy.socket-path:  \nvamp.operation.check.instances:  \nvamp.info.message:  \nvamp.gateway-driver.haproxy.ip:  \nvamp.gateway-driver.haproxy.version:  \nvamp.persistence.database.elasticsearch.index:  \nvamp.operation.check.cpu:  \nvamp.operation.gateway.port-range:  \nvamp.http-api.interface:  \nvamp.container-driver.docker.repository.email:  \nvamp.persistence.response-timeout:  \nvamp.operation.deployment.scale.cpu:  \nvamp.pulse.elasticsearch.index.time-format.event:  \nvamp.operation.synchronization.timeout.ready-for-deployment:  \n\n---------------,\nList config\n\nReturn details of Vamp's JVM environment and runtime status, the configured persistence layer and the container driver status. \n\n Request\n\nGET\n/api/v1/config\nThe request body should be empty.\n\nResponse\nIf successful, will return a list of all config parameters. Parameters are described in detail in the configuration reference.  \n\n---------------,\n Get specific config parameter\n\nReturn a specific config parameter. \n\nRequest\n\nGET \n/api/v1/config/ \nThe request body should be empty.\n\n Response\nIf successful, will return the specified config parameter. \n\nErrors\nempty response body - the named config parameter does not exist.\n\n Example - explicitly request the configured Vamp info message\nRequest:\n\n\tGET vamp url/api/v1/config/vamp.info.message\n\t\nResponse:\n\n!!map 'Hi, I''m Vamp! How are you?'\n\n------------------",
    "id": 7
  },
  {
    "path": "/documentation/api/v0.9.2/api-debug",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Debug",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-debug-092\"",
    "    weight": "   weight: 80",
    "aliases": "aliases:",
    "content": "\nActions\n \n Force sync\n Force SLA check\n Force escalation \n\n----------------,\n Force sync\n\nForces Vamp to perform a synchronization cycle, regardless of the configured default interval.\n\n\tGET /api/v1/sync\n\t\nForce SLA check\t\n\nForces Vamp to perform an SLA check, regardless of the configured default interval.\n\n\tGET /api/v1/sla\n\n Force escalation\t\n\nForces Vamp to perform an escalation check, regardless of the configured default interval.\n\n\tGET /api/v1/escalation",
    "id": 8
  },
  {
    "path": "/documentation/api/v0.9.2/api-deployment-scales",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deployment scales",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-deployment-scales-092\"",
    "    weight": "   weight: 100",
    "aliases": "aliases:",
    "content": "Deployment scales are singular resources: you only have one scale per service. Deleting a scale is not a meaningful action. Read about using deployments and using scales.\n\nActions\n \n Get - return details of a specific deployment scale\n Update - update a deployment scale\n\n-----------,\n Get single deployment scale\n\nReturn details of a specific deployment scale that’s part of a service inside a cluster.\n\nRequest\nGET\n/api/v1/deployments/ /clusters/ /services/ /scale\nThe request body should be empty.\n\n Response\nIf successful, will return the specified scale resource in the specified accept format (default JSON).\n\nErrors\nThe requested resource could not be found.\n\n-----------,\n Update deployment scale\n\nUpdate the scale of a running deployment.\n\nRequest\nPUT\n/api/v1/deployments/ /clusters/ /services/ /scale\nThe request body should include at least the minimum scale resource in the specified content-type format (default JSON). \n\n Response\nIf successful, will return an empty response.\n\n-----------,",
    "id": 9
  },
  {
    "path": "/documentation/api/v0.9.2/api-deployment-slas",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deployment SLAs",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-deployment-slas-092\"",
    "    weight": "   weight: 110",
    "aliases": "aliases:",
    "content": "Deployment scales are singular resources: you only have one scale per service. Deleting a scale is not a meaningful action. Read about using deployments and using scales.\n\nActions\n \n Get - return details of a specific deployment SLA\n Create - create or update a specific deployment SLA\n Delete - delete a deployment SLA\n\n-----------,\n Get single deployment SLA\n\nReturn details for a specific SLA that’s part of a specific cluster.\n\nRequest\nGET\n/api/v1/deployments/ /clusters/ /sla\nThe request body should be empty.\n\n Response\nIf successful, will return the specified SLA resource in the specified accept format (default JSON).\n\nErrors\nThe requested resource could not be found.\n\n-----------,\n Create deployment SLA\n\nCreate or update a specific deployment SLA.\n\nRequest\nPUT\n/api/v1/deployments/ /clusters/ /sla\nThe request body should include at least the minimum SLA resource in the specified content-type format (default JSON). \n\n Response\nIf successful, will return the updated deployment resource in the specified content-type format (default JSON). \n\n-----------,\nDelete deployment SLA\n\nDelete an SLA from a running deployment. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\n Request\nDELETE\n/api/v1/deployments/ /clusters/ /services/ /scale\nThe request body should be empty. \n\nResponse\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n--------------",
    "id": 10
  },
  {
    "path": "/documentation/api/v0.9.2/api-deployments",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deployments",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-deployments-092\"",
    "    weight": "   weight: 90",
    "aliases": "aliases:",
    "content": "\nDeployments are dynamic runtime structures, so changes to them take time to execute and can possibly fail. Most API calls to the /deployments endpoint will, therefore, return a 202: Accepted return code, indicating the asynchronous nature of the call. Deployments have a set of sub resources: deployment SLAs, deployment scales and gateways. These are instantiations of their static counterparts.\nRead about using deployments.\n\nActions\n \n List - return details of all running deployments\n Get - get details of a single running deployment\n Create - initiate a new deployment \n Create named deployment - initiate a new deployment with a custom name (non UUID)\n Update - add to a running deployment (merge)\n Delete - remove elements from a running deployment\n\n Deployment resource\n\nThe resource example below is in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nname: sava\nkind: deployment\nlookup_name: b745761242ab5566a44b556e62764beed46fa8de\nclusters:\n  sava:\n    services:\n    status:\n        intention: Deployment\n        since: '2017-01-05T14:53:18.428Z'\n        phase:\n          name: Done\n          since: '2017-01-05T14:53:24.534Z'\n      breed:\n        name: sava:1.0.0\n        kind: breed\n        deployable:\n          type: container/docker\n          definition: magneticio/sava:1.0.0\n        ports:\n          webport: 8080/http\n        environment_variables: { \n        constants: { \n        arguments: []\n        dependencies: { \n      environment_variables: { \n      scale:\n        cpu: 0.2\n        memory: 64.00MB\n        instances: 1\n      instances:\n      name: sava_sava-1-0-0-6fd83b1fd01f7dd9eb7f.b1fc514f-d356-11e6-b975-02426d22113f\n        host: 192.168.99.100\n        ports:\n          webport: 31107\n        deployed: true\n      arguments:\n      privileged: 'true'\n      dependencies: { \n      dialects: { \n    gateways:\n      webport:\n        sticky: null\n        virtual_hosts:\n        webport.sava.sava.vamp\n        routes:\n          sava:1.0.0:\n            lookup_name: fef540f1c9e4eb21e045d935eac990d0d5d25825\n            weight: 100%\n            balance: default\n            condition: null\n            condition_strength: 0%\n            rewrites: []\n    dialects: { \nports:\n  sava.webport: '40001'\nenvironment_variables: { \nhosts:\n  sava: 192.168.99.100\n\n Field name        | description          \n -----------------|--------------, name |  \n kind |\n lookup_name |  \n clusters |\n  |  \n ports |\n environment_variables |  \n hosts |\n\n-----------------  \n  \nList deployments\n\nReturn a list of all running deployments. For details on pagination see common parameters.\n\n Request\nGET\n/api/v1/deployments\nThe request body should be empty.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint | true or false     | false            | Exports each deployment as a valid blueprint\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\nResponse\nIf successful, will return a list of deployment resources in the specified accept format (default JSON).\n\n-----------,\n Get single deployment\n\nReturn details of a specific running deployment.\n\nRequest\nGET\n/api/v1/deployments/ \nThe request body should be empty.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint | true or false     | false            | Exports each deployment as a valid blueprint\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\n Response\nIf successful, will return the specified deployment resource in the specified accept format (default JSON).\n\nErrors\nThe requested resource could not be found.\n\n-----------,\n Create deployment\n\nInitiate a deployment.\n\nRequest\nPOST \n/api/v1/deployments\nThe request body should include at least the minimum blueprint resource in the specified content-type format (default JSON). \nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 201 Created if the blueprint is valid.  \n\n Response\nIf successful, will return the created deployment resource in the specified accept format (default JSON).\n\nExamples\n\nSee gateways - A/B TEST TWO DEPLOYMENTS USING ROUTE WEIGHT\n\n-----------,\n Create named deployment\n\nInitiate a deployment with a custom name (non UUID).\n\nRequest\nPOST \n/api/v1/deployments/ \nThe request body should include at least the minimum blueprint resource in the specified content-type format (default JSON). \nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the blueprint is valid for deployment.   \n\n Response\nIf successful, will return the created deployment resource in the specified accept format (default JSON).\n\n---------,\nUpdate deployment\n\nAdd to a running deployment (merge).\n\n Request\nPUT\n/api/v1/deployments/ \nThe request body should include at least the minimum blueprint resource in the specified content-type format (default JSON). The name field must match the deployment_name specified in the request syntax.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted  if the deployment after the update would be still valid.  \n\nResponse\nIf successful, will return the updated deployment resource in the specified accept format (default JSON).\n\n-----------,\n Delete deployment\n\nDelete all or parts of a deployment.\n\nRequest\n\nDELETE\n/api/v1/deployments/ \nThe request body should contain at least a minimum blueprint resource containing the services to be removed from the deployment. To delete a full deployment, include the complete blueprint or deployment resource. Note that DELETE on deployment with an empty request body will not delete anything.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 202 Accepted if the deployment after the (partial) deletion would be still valid. Actual delete is not performed.\n\n Response\n\nExample - Delete service\n\nThis is our (abbreviated) deployment in YAML format. We have two clusters. The first cluster 'frontend' has two services.\nWe have left out some keys like scale among others as they have no effect on this specific use case.\n\n\t\tGET /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcd?as_blueprint=true\n\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nendpoints:\n  frontend.port: '9050'\nclusters:\n  frontend:\n    services:\n    breed:\n        name: monarch_front:0.1\n        deployable: magneticio/monarch:0.1\n        ports:\n          port: 8080/http\n        constants: { \n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n    breed:\n        name: monarch_front:0.2\n        deployable: magneticio/monarch:0.2\n        ports:\n          port: 8080/http\n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n  backend:\n    services:\n    breed:\n        name: monarch_backend:0.3\n        deployable: magneticio/monarch:0.3\n        ports:\n          jdbc: 8080/http\n        environment_variables: { \n\nIf we want to delete the first service in the frontend cluster, we use the following blueprint as the request body in the DELETE action.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n\nIf we want to delete the whole deployment, we just specify all the clusters and services.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t  \n\t\t  \nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n    breed:\n        ref: monarch_front:0.2\n  backend:\n    services:\n    breed:\n        ref: monarch_backend:0.3\n\n-----------,",
    "id": 11
  },
  {
    "path": "/documentation/api/v0.9.2/api-escalations",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Escalations",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-escalations-092\"",
    "    weight": "   weight: 120",
    "aliases": "aliases:",
    "content": "Escalation templates are static artifacts. You can save and manage escalation templates through the API, these can then be referenced in an sla and applied to each cluster in a blueprint. Read about using escalations and using slas.\n\nActions\n \n List - return a list of all escalations\n Get - get a single escalation\n Create - create a new escalation \n Update - update a escalation\n Delete - delete a escalation\n\n Escalation resource\n\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully create an escalation.\n\nname: unique_name\n  type: scale_instances\n  minimum: 1\n  maximum: 3\n  scale_by: 1\n\n API return resource\nThe fields returned by the API after an escalation has been created (also visible in the UI)\n\nname: unique_name\n  kind: escalation\n  type: scale_instances\n  target: cluster_name\n  minimum: 1\n  maximum: 3\n  scale_by: 1\n   \n\n Field name    | Options | Required?  | description          \n -----------------|-----|------|---, name |  -  |  Required  | A unique name to reference the escalation\n kind |  escalation  |  Optional  | The resource type. Required to send multiple resources to /api/v1\n type |  scaleinstances, scalecpu, scale_memory  |  Required  |  Escalation handler. Specifies what is to be scaled.\n target |    | Optional   | The target cluster to scale up/down. If not included, will default to the cluster where escalations are specified.\n minimum |    | Required  | Minimum setting.\n maximum |    | Required   |  Maximum setting.\n scale_by |    |  Required   | Increment to scale up/down by.\n    \n--------------,\nList Escalations\n\nReturn a list of all stored escalation templates. For details on pagination see common parameters\n\n Request\nGET \n/api/v1/escalations\nThe request body should be empty.\n\nResponse\nIf successful, will return a list of escalation resources in the specified accept format (default JSON).  \n\n--------------,\n Get single escalation\n\nReturn a the named escalation resource.\n\nRequest\nGET \n/api/v1/escalations/ \nThe request body should be empty.\n\n Response\nIf successful, will return the named escalation resource in the specified accept format (default JSON).  \n\n--------------,\nCreate escalation\n\nStores a new escalation template.\n\n Request\nPOST \n/api/v1/escalations\nThe request body should include at least a mimnimum escalation resource.\nQuery string parameters:\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. \n\nResponse\nA successful create operation has status code 201 Created and the response body will contain the created escalation resource in the specified accept format (default JSON). \n\n--------------,\n Update escalation\n\nUpdate a stored escalation.\n\nRequest\nPUT \n/api/v1/escalations/ \nThe request body should include at least a mimnimum escalation resource. The name field must match the escalation_name specified in the request path.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid.\n\n Response\nA successful update operation has status code 200 OK or 202 Accepted and the response body will contain the updated escalation resource in the specified accept format (default JSON).\n\n--------------,\nDelete escalation\n\nDelete a stored escalation. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\n Request\nDELETE \n/api/v1/escalations/ \nThe request body should be empty.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content if the escalation is valid, without actual delete of the escalation.\n\nResponse\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n-----------------",
    "id": 12
  },
  {
    "path": "/documentation/api/v0.9.2/api-events",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Events",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-events-092\"",
    "    weight": "   weight: 130",
    "aliases": "aliases:",
    "content": "Read about using events and the Vamp events system.\n\nActions\n \n List - return a list of all events (can be filtered)\n Create - create a new event \n stream - open SSE events stream\n\n Event resource\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this.\n\nMinimum resource\nThe minimum fields required to successfully store a event.\n\ntags:\n  custom_tag\n\n API return resource\nThe fields returned by the API for stored events.\n\ntags:\n  scales\n  scales:large\n  archive\n  archive:delete\n  value: ''\n  timestamp: '2017-01-10T15:33:49.766Z'\n  type: archive\n\n Field name    |  required?  | description          \n --------------|---|--------------, tags |  Required  | An event must contain at least one tag. Combined tags (tag1:tag2) will be stored as tag1 and tag1:tag2. \n value |  Optional  | If not included, will be blank.\n timestamp |  Optional  | If not included the current timestamp will be added.\n type |  Optional  | If not included, will be set to the default type event.\n  \n------------,\nList events\n\nReturn a list of all stored events.  You can optionally filter the events list by type or tag(s).  \nFor details on pagination see common parameters.\n\n Request\nGET\n/api/v1/events  \n  You can optionally filter returned events by tag(s) or type, for example:  \n  GET /api/v1/events?tag=archiving&tag=breeds or  \n  GET /api/v1/events?type=metrics \nThe request body should be empty.\n\nResponse\nWill return a (filtered) list of event resources].\n\n------------,\n Create event\n\nCreate a new event. \n\nRequest\nPOST\n/api/v1/events\nThe request body should include at least a minimum event resource.\n\n Response\nWill return the created event resource in the specified accept format (default JSON). \n\n------------,\nStream events\n\nOpen a Server-sent events (SSE) connection to receive updates to the Vamp events stream, for example in Google Chrome.\n\n Request\nvamp url/api/v1/events/stream  \n  You can optionally filter returned events by tag(s), for example:  \n  GET /api/v1/events/stream?tag=archiving&tag=breeds\n\nResponse\nTransmits (filtered) updates to the Vamp events stream.\n\n---------------",
    "id": 13
  },
  {
    "path": "/documentation/api/v0.9.2/api-gateways",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Gateways",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-gateways-092\"",
    "    weight": "   weight: 140",
    "aliases": "aliases:",
    "content": "Read about using gateways.\n\nMethods\n \n List - return a list of all gateways\n Get - get a single gateway\n Create - create a new gateway \n Update - update a gateway\n Delete - delete a gateway\n\n Gateway resource\nYou can define gateways inline as part of a blueprint, breed or deployment, or create them separately and reference them by name. The resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\n\nname: sava_gateway\n  port: 12345\n\n API return resource\n\nname: sava_gateway\nkind: gateway\nlookup_name: ebcec0d294fc399c5ee972c43a854e3b643d5ece\ninternal: false\nservice:\n  host: 192.168.99.100\n  port: 12345/http\ndeployed: true\nport: '12345'\nsticky: null\nvirtual_hosts:\nsava-gateway.vamp\nroutes: { \n\n Field name        |  Required | Description          \n -----------------|--------|------, name |  yes  | Unique name used to reference the gateway from a breed, blueprint or deployment.\n kind |  optional  | The resource type. Required to send multiple resources to /api/v1\n lookup_name |  -  |\n internal  |  -  |\n service  |  -  |\n deployed  |  -  |\n port  |  yes  | portnumber/porttype.  Port type can be http (default) or tcp.\n sticky |  optional  |\n virtual_hosts  |  -  |\n routes  |  optional  | \n  \n----------- \n    \nList gateways\n\nReturn a list of all gateways. For details on pagination see common parameters\n\n Request\nGET\n/api/v1/gateways\nThe request body should be empty.\n\nResponse\nIf successful, will return a list of all gateway resources in the specified accept format (default JSON).  \n\n Examples\n\nSee gateways - A/B TEST TWO DEPLOYMENTS USING ROUTE WEIGHT\n\n----------- \n    \nGet single gateway\n\nGet details of a single gateway.\n\n Request\nGET\n/api/v1/gateways/ \nThe request body should be empty.\n\nResponse\nIf successful, will return the named gateway resource in the specified accept format (default JSON).  \n\n Errors\nThe requested resource could not be found. - the named gateway does not exist.\n\n----------- \n    \nCreate gateway\n\nCreate a new gateway.\n\n Request\nPOST\n/api/v1/gateways\nThe request body should include at least a minimum gateway resource in the specified Content-Type format (default JSON).\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the gateway is valid\n\nResponse\nIf successful, will return the newly stored gateway resource in the specified accept format (default JSON).  \n\n Errors\nGateway port '...' is/was in use by deployment: '...' - Port already in use. Remove gateway or select another port.\n\n----------- \n    \nUpdate gateway\n\nUpdate a stored gateway.\n\n Request\nPUT\n/api/v1/gateways/ \nThe request body should include at least a minimum gateway resource in the specified Content-Type format (default JSON).  The name field must match the gateway_name specified in the request path.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the gateway is valid\n\nResponse\nIf successful, will return the newly stored gateway resource in the specified accept format (default JSON).  \n\n Errors\nInconsistent name - the gateway_name in the request path does not match the name field in the request body.\n\n----------- \n    \nDelete gateway\n\nDelete a gateway. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\n Request\nDELETE\n/api/v1/gateways/ \nThe request body should be empty.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 204 No Content if the gateway is valid, without actual delete of the breed.\n\nResponse\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n----------- ",
    "id": 14
  },
  {
    "path": "/documentation/api/v0.9.2/api-haproxy",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "HAProxy",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-haproxy-092\"",
    "    weight": "   weight: 150",
    "aliases": "aliases:",
    "content": "\nDetails of Vamp's HAProxy configuration. \n\t\nActions\n\n Get - get the HAProxy configuration for a version.\n\n HAProxy config parameters\n\nThe example below is in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nHAProxy 1.6, Frontends & Backends managed by Vamp\n\n Virtual hosts\n\nfrontend virtual_hosts\n\n  bind 0.0.0.0:80\n  mode http\n\n  option httplog\n  log-format  r \n  \n  # destination: sava:1.0/sava/web\n  acl 210709510c3942c9 hdr(host) -i web.sava.sava-1-0.vamp\n  use_backend 271118ac3851077c564ceb0e75f9c05f28e1acdd  if 210709510c3942c9  \n  \nbackend: sava:1.0/sava/web\nbackend 271118ac3851077c564ceb0e75f9c05f28e1acdd\n\n  balance roundrobin\n  mode http\n\n  option forwardfor\n  http-request set-header X-Forwarded-Port %[dst_port]\n  \n   server: sava:1.0/sava/web\n  server 271118ac3851077c564ceb0e75f9c05f28e1acdd 127.0.0.1:40000\n  \nPort mapping\n\n frontend: sava:1.0/sava/web\nfrontend 271118ac3851077c564ceb0e75f9c05f28e1acdd\n  \n  bind 0.0.0.0:40000\n  \n  option httplog\n  log-format  r \n  mode http\n\n  # backend: other sava:1.0/sava/web\n  defaultbackend o271118ac3851077c564ceb0e75f9c05f28e1acdd\n\nfrontend: other sava:1.0/sava/web\nfrontend o_271118ac3851077c564ceb0e75f9c05f28e1acdd\n  \n  option httplog\n  log-format  r \n  \n  bind unix@/usr/local/vamp/o_271118ac3851077c564ceb0e75f9c05f28e1acdd.sock accept-proxy\n  mode http\n\n   backend: other sava:1.0/sava/web\n  defaultbackend o271118ac3851077c564ceb0e75f9c05f28e1acdd\n\nfrontend: sava:1.0/sava/web//sava:1.0/sava/sava:1.0/web\nfrontend 1f2c9bf68395805af6ecd4c389c061dfdd24d62d\n  \n  option httplog\n  log-format  r \n  \n  bind unix@/usr/local/vamp/1f2c9bf68395805af6ecd4c389c061dfdd24d62d.sock accept-proxy\n  mode http\n\n   backend: sava:1.0/sava/web//sava:1.0/sava/sava:1.0/web\n  default_backend 1f2c9bf68395805af6ecd4c389c061dfdd24d62d\n\nbackend: other sava:1.0/sava/web\nbackend o_271118ac3851077c564ceb0e75f9c05f28e1acdd\n\n  mode http\n  balance roundrobin\n  \n   server: sava:1.0/sava/web//sava:1.0/sava/sava:1.0/web\n  server 1f2c9bf68395805af6ecd4c389c061dfdd24d62d unix@/usr/local/vamp/1f2c9bf68395805af6ecd4c389c061dfdd24d62d.sock send-proxy weight 100 check \n  \nbackend: sava:1.0/sava/web//sava:1.0/sava/sava:1.0/web\nbackend 1f2c9bf68395805af6ecd4c389c061dfdd24d62d\n\n  mode http\n  balance roundrobin\n  \n  option forwardfor\n   server: e0fac2d5bb3be249d937_sava-1-0-e0fac2d5bb3be249d937.f8de60da-dbdc-11e6-bdac-0242846bb8a0\n  server cd48eaba831ec9ef2ea951c3d273e0fb06dac84f 192.168.99.100:31610 cookie cd48eaba831ec9ef2ea951c3d273e0fb06dac84f weight 100 check  \n  \n  # server: e0fac2d5bb3be249d937_sava-1-0-e0fac2d5bb3be249d937.f8da6938-dbdc-11e6-bdac-0242846bb8a0\n  server 3a62d05fca87ce3d018d0020f778e05923d1b8eb 192.168.99.100:31727 cookie 3a62d05fca87ce3d018d0020f778e05923d1b8eb weight 100 check  \n  \n  # server: e0fac2d5bb3be249d937_sava-1-0-e0fac2d5bb3be249d937.f8e083bb-dbdc-11e6-bdac-0242846bb8a0\n  server fff8c1fbb50c35d619b3d4cf2fdd4f4c73e2ce9d 192.168.99.100:31134 cookie fff8c1fbb50c35d619b3d4cf2fdd4f4c73e2ce9d weight 100 check  \n  \n  # server: e0fac2d5bb3be249d937_sava-1-0-e0fac2d5bb3be249d937.f8dd2859-dbdc-11e6-bdac-0242846bb8a0\n  server 10f3595fedfdd135af60e06d738c802cbd23564a 192.168.99.100:31473 cookie 10f3595fedfdd135af60e06d738c802cbd23564a weight 100 check  \n\n---------------,\nGet HAProxy configuration\n\nReturn the configuration for the specified HAProxy version,\n\n Request\n\nGET \n/api/v1/haproxy/ \nThe request body should be empty.\n\nResponse\nIf successful, will return the HA proxy configuration for the HAProxy version_number specified in the request path. \n\n Errors\nblank response - There is no configuration available for the specified HAProxy version number.\n\n------------------",
    "id": 15
  },
  {
    "path": "/documentation/api/v0.9.2/api-health",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Health",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-health-092\"",
    "    weight": "   weight: 160",
    "aliases": "aliases:",
    "content": "Health is a specific type of Vamp event, calculated by a Vamp workflow and required for the Vamp UI. Health can be defined on gateways and deployment ports and retrieved via the API. Read about using workflows.  \n\nHealth is returned as a value between 1 and 0, where 1 is 100% healthy.\n\nHealth can be defined on gateways and deployment ports and retrieved.:\n\n/api/v1/health/gateways/ \n/api/v1/health/gateways/ /routes/$route\n\n/api/v1/health/deployments/ \n/api/v1/health/deployments/ /clusters/ \n/api/v1/health/deployments/ /clusters/ /services/ \n`",
    "id": 16
  },
  {
    "path": "/documentation/api/v0.9.2/api-info",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Info",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-info-092\"",
    "    weight": "   weight: 170",
    "aliases": "aliases:",
    "content": "\nDetails of Vamp's JVM environment and runtime status, the configured persistence layer and container driver status can be retrieved from the /info APi endpoint. \n\t\nActions\n \n List - returns a full list of all info parameters.\n Get - explicitly request a specific info section.\n\n Info parameters\nThe example below is in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nmessage:  \nversion:  \nuuid:  \nrunning_since:  \njvm:  \npersistence:  \nkey_value:  \npulse:  \ngateway_driver:  \ncontainer_driver:  \nworkflow_driver:  \n Parameter name        | optional | description          \n -----------------|-----------------|, message |  Always returned | The Vamp welcome message vamp.info.message\n version | Always returned | The running Vamp version\n uuid |   Always returned | \n running_since | Always returned | \n jvm | optional | operating_system, runtime, memory, non-heap, threads\n persistence | optional | database, archiving\n key_value | optional |  type and details\n pulse | optional | type and details\n gateway_driver | optional |  \n container_driver | optional |\n workflow_driver | optional | type(s) and url\n\n---------------,\nList info\n\nReturn details of Vamp's JVM environment and runtime status, the configured persistence layer and the container driver status. \n\n Request\n\nGET\n/api/v1/info\nThe request body should be empty.\n\nResponse\nIf successful, will return a list of all info parameters.\n\n Errors\n500 - one of the /info response sections retured an error.\n\n---------------,\nGet specific info section\n\nReturn a specific info section, see info parameters. \n\n Request\n\nGET \n/api/v1/info?on= \nThe request body should be empty.\n\nResponse\nIf successful, will return a list with only the standard and specified info parameters. \n\n Errors\n500 - one of the /info response sections retured an error.\n\nExample - explicitly request jvm and persistence info\nRequest:\n\n\tGET vamp url/api/v1/info?on=jvm&on=persistence\n\t\nResponse:\n\n ,\n    \"persistence\":  \n \n\n------------------",
    "id": 17
  },
  {
    "path": "/documentation/api/v0.9.2/api-metrics",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Metrics",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-metrics-092\"",
    "    weight": "   weight: 180",
    "aliases": "aliases:",
    "content": "Metrics can be defined on gateways and deployment ports and retrieved via the API. Metrics are calculated using external services such as workflows. Read about using workflows.\n\n/api/v1/metrics/gateways/ / \n/api/v1/metrics/gateways/ /routes/$route/ \n\n/api/v1/metrics/deployments/ /clusters/ /ports/ / \n/api/v1/metrics/deployments/ /clusters/ /services/ /ports/ / \n\nExample\n    GET /api/v1/metrics/deployments/sava/clusters/frontend/ports/api/response-time\n\n",
    "id": 18
  },
  {
    "path": "/documentation/api/v0.9.2/api-reference",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Overview",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-overview-092\"",
    "    weight": "   weight: 10",
    "aliases": "aliases:",
    "content": "\nVamp has one REST API. For details on pagination, and request and response formats see common parameters.\n\nAPI endpoints and resource descriptions\n\nArtifacts: blueprints, breeds, conditions, escalations, scales, slas\nRuntime entities: deployments, deployment scales, deployment SLAs, gateways, workflows  \nData: events, health, metrics\nSystem: info, config, haproxy\nDebug: sync, sla, escalation\n\n Send multiple resources\n\nIt is possible to POST, PUT or DELETE YAML or JSON documents containing more than one artifact definition.\n\nSimilar artifacts can be sent to a specific endpoint, such as /api/v1/breeds. Different artifact types can also be sent together by using the general endpoint api/v1 and including a kind field in each artifact definition. The artifact kind corresponds to the singular form of the artifact type (for example blueprint, breed, condition).\n\nExample (YAML) - post multiple artifacts to a specific endpoint \n\nPOST /api/v1/breeds\n\n,name: ...\n breed 1 definition ...\n,name: ...\nbreed 2 definition ....\n,name: ...\n breed 3 definition ....\n\nExample (YAML) - post multiple artifact types to /api/v1\nWhen using the general api/v1 endpoint, each artifact description  must include a kind field.\n\nPOST /api/v1\n\n,name: ...\nkind: blueprint\n blueprint definition ...\n,name: ...\nkind: breed\nbreed definition ...\n,name: ...\nkind: condition\n condition definition ...\n",
    "id": 19
  },
  {
    "path": "/documentation/api/v0.9.2/api-scales",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Scales",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-scales-092\"",
    "    weight": "   weight: 190",
    "aliases": "aliases:",
    "content": "Read about using scales.\n\nActions\n \n List - return a list of all stored scales\n Get - get a single stored scale\n Create - create a new scale \n Update - update a stored scale\n Delete - delete a stored scale\n\n Scale resource\n\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully store a scale.\n\nname: sava_scale\ncpu: 0.2 \nmemory: 64MB\n\n API return resource\nThe fields returned by the API for stored scales.\n\nname: sava_scale\n  kind: scale\n  cpu: 0.2\n  memory: 64.00MB\n  instances: 1\n\n Field name      | Required  | description          \n -----------------|----------|----, name | yes  | Unique name used to reference the scale from a breed, blueprint, deployment or workflow.\n kind |  optional  |   The resource type. Required to send multiple resources to /api/v1\n cpu |  yes |   \n memory | yes  |  \n instances | optional  |  \n\n---------------,\nList scales\n\nReturns a list of all stored scales. For details on pagination see common parameters\n\n Request\n GET\n /api/v1/scales\n The request body should be empty.\n\nResponse\nIf successful, will return a list of all stored scale resources in the specified accept format (default JSON).  \n\n---------------,\n Get single scale\n\nReturns a single stored scale.\n\nRequest\n GET\n /api/v1/scales/ \n The request body should be empty.\n\n Response\nIf successful, will return the named scale resource in the specified accept format (default JSON).  \n\n---------------,\nCreate scale\n\nCreate a new scale. Scales can be stored individually and then referenced from a breed, blueprint, deployment or workflow.\n\n Request\n POST\n /api/v1/scales\n The request body should inclde at least a minimum scale resource in the specified Content-Type format (default JSON).\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the condition is valid\n\nResponse\nIf successful, will return the newly stored scale resource in the specified accept format (default JSON).  \n\n---------------,\n Update scale\n\nUpdate a stored scale.\n\nRequest\n PUT\n /api/v1/scales/ \n The request body should inclde at least a minimum scale resource in the specified Content-Type format (default JSON). The name field must match the scale_name specified in the request path.\nQuery string parameters:\n\n| Request parameters     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the scale is valid\n\n Response\nIf successful, will return the newly stored scale resource in the specified accept format (default JSON).  \n\nErrors\nInconsistent name - the scale_name in the request path does not match the name field in the request body.\n\n---------------,\n Delete scale\n\nDelete a stored scale. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\nRequest\n DELETE\n /api/v1/scales/ \n The request body should be empty.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content if the escalation is valid, without actual delete of the scale.\n\n Response\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n------------------",
    "id": 20
  },
  {
    "path": "/documentation/api/v0.9.2/api-slas",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "SLAs",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-slas-092\"",
    "    weight": "   weight: 200",
    "aliases": "aliases:",
    "content": "SLAs (Service Level Aggreemets) can be used to define a pre-described set of boundaries to a service and the actions that should take place once the service crosses those boundaries. You can save and manage SLA templates through the API, these can then be referenced in a blueprint or deployment. Read about using SLAs.\n\nActions\n \n List - return a list of all SLAs\n Get - get a single SLA\n Create - create a new SLA \n Update - update a SLA\n Delete - delete a SLA\n\n SLA resource\nYou can define SLAs inline or store them separately under a unique name and reference them from a blueprintor deployment resource.\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully create a SLA.\n\nname: sla_name\ntype: responsetimesliding_window\nwindow:\n interval: 50\n cooldown: 50\nthreshold:\n upper: 100\n lower: 10\n\n API return resource\nThe fields returned by the API after a SLA has been created (also visible in the UI)\n\n name: sla_name\n   kind: sla\n   type: responsetimesliding_window\n   window:\n     interval: 50\n     cooldown: 50\n   threshold:\n     upper: 100\n     lower: 10\n   escalations: []\n\n Field name   | Options  |  Required   | description          \n -----------------|----|----|------, name |   | Required  | Unique name, used to reference the SLA.\n kind | sla  | Optional  | The resource type. Required to send multiple resources to /api/v1.\n type | responsetimesliding_window  | Required  |\n window |   | Required  |  \n threshold |   | Required  |\n escalations |   | Optional  |\n\n--------------,\nList SLAs\n\nReturn a list of all stored SLA templates. For details on pagination see common parameters.\n\n Request\n GET \n /api/v1/slas\n The request body should be empty.\n\nResponse \nIf successful, will return a list of all stored SLA resources in the specified accept format (default JSON).  \n\n--------------,\n Get single SLA\n\nReturn a the named SLA resource.\n\nRequest\nGET \n/api/v1/slas/ \nThe request body should be empty.\n\n Response\nIf successful, will return the named SLA resource in the specified accept format (default JSON).  \n\n--------------,\nCreate SLA\n\nStore a new SLA template.\n\n Request\nPOST \n/api/v1/slas\nThe request body should include at least a mimnimum SLA resource.\nQuery string parameters:\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the SLA is valid.\n\nResponse\nA successful create operation has status code 201 Created and the response body will contain the created SLA resource in the specified accept format (default JSON). \n\n--------------,\n Update SLA\n\nUpdate a stored SLA template.\n\nRequest\nPUT \n/api/v1/slas/ \nThe request body should include at least a mimnimum SLA resource. The name field must match the sla_name specified in the request path.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the SLA is valid.\n\n Response\nA successful update operation has status code 200 OK or 202 Accepted and the response body will contain the updated SLA resource in the specified accept format (default JSON).\n\n--------------,\nDelete SLA\n\nDelete a stored SLA template. Note that delete operations are idempotent: sending a second request with the same content will not result in an error response (4xx).\n\n Request\nDELETE \n/api/v1/slas/ \nThe request body should be empty.\nQuery string parameters:\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the SLA.\n\nResponse\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n-----------------",
    "id": 21
  },
  {
    "path": "/documentation/api/v0.9.2/api-websockets",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Websockets",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-websockets-092\"",
    "    weight": "   weight: 30",
    "aliases": "aliases:",
    "content": "\nWebSocket API requests and responses are transmitted as text messages containing both meta-information and a payload.  \n\nWebSocket API requests\nWebsocket API responses\n\nWebSocket API requests\nREST API requests can be mapped 1-on-1 to WebSocket API requests using the properties described below. Note that all enum (symbolic) values like Json, Peek etc. are case-insensitive. The Vamp API accepts requests in JSON or YAML. \n\n Example - excplicitly request gateway driver info\n\nREST API request:\n  GET /api/v1/info?on=gateway_driver\nWebSocket API request:\n\n         \n    \nWebSocket API request properties\n\nProperty | Options | Description\n------|------|---, api   |  v1  |    API version, only v1 is supported right now (corresponds to api/v1).\n path   |    |    The full REST path (without API version) for example breeds or workflows/health.\n action   |  Peek, Put or Remove  |    Corresponding to the REST methods: Peek =  GET, Put = POST or PUT, Remove = DELETE.\n accept   |   Json, Yaml, PlainText, Javascript  |  Expected response payload content type. REST: similar to the Accept header. PlainText and Javascript will be retrieved \"as is\" without any data encoding.      \n content   |  Json, Yaml, PlainText, Javascript   |  Content type of the payload (data), REST: similar to Content-Type header.       \n transaction   |    |   Arbitrary ID (defined by client), server will set the same value in its response(s).     \n data   |    |    Payload, REST: body.\n parameters   |    |   Map of additional parameters, REST: query string, e.g. validate_only=true.     \n    \n\n WebSocket API responses\nREST API responses can be mapped 1-on-1 to WebSocket API responses using the properties described below. Note that all enum (symbolic) values like Json, Peek etc. are case-insensitive. Vamp API responses can be formatted in JSON or YAML. \n\nProperty | Options | Description\n------|------|---, api   |  v1  |    API version, only v1 is supported right now (corresponds to api/v1).\n path   |    |    The same path as in the corresponding request.\n action   |  Peek, Put or Remove  |    Action value from the request.\n status   |   Ok, Accepted, NoContent or Error  |  Response status.      \n content   |  Json, Yaml, PlainText, Javascript   |  Content type of the payload (data) - same as in the corresponding request.     \n transaction   |    |   Request ID.\n data   |    |    Payload, text encoded as specified in content parameter.\n parameters   |    |   Map of additional parameters and response headers (as would be expected from equivalent REST response)",
    "id": 22
  },
  {
    "path": "/documentation/api/v0.9.2/api-workflows",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Workflows",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-workflows-092\"",
    "    weight": "   weight: 210",
    "aliases": "aliases:",
    "content": "Read about using workflows.\n\nActions\n \n List - return a list of all workflows\n Get - get a single workflow\n Create - create a new workflow \n Update - update a workflow\n Delete - delete a workflow\n\n Workflow resource\nThe resource examples shown below are in YAML format. Vamp API requests and responses can be in JSON (default) or YAML format, see common parameters for details on how to set this. \n\nMinimum resource\nThe minimum fields required to successfully create a workflow.\n\nname: workflow_name\nschedule: daemon\nbreed: workflow_breed\n\n API return resource\nThe fields returned by the API after a workflow has been created (also visible in the Vamp UI workflow editor)\n\nname: health\n  kind: workflow\n  breed:\n    reference: health\n  status: running\n  schedule: daemon\n  environment_variables:\n    VAMPWORKFLOWEXECUTION_TIMEOUT: '7'\n    VAMPKEYVALUESTORECONNECTION: 192.168.99.100:2181\n    VAMPKEYVALUESTOREPATH: /vamp/workflows/health\n    VAMPWORKFLOWEXECUTION_PERIOD: '5'\n    VAMPKEYVALUESTORETYPE: zookeeper\n    VAMP_URL: http://192.168.99.100:8080\n  scale:\n    cpu: 0.1\n    memory: 128.00MB\n    instances: 1\n  network: HOST\n  arguments: [] \n\n    \nField name  |  Options  |  Required |  Description  \n------------|-------|--------|-----,name  | - |   Required |  \nkind |  -  | Optional | The resource type. Required to send multiple resources to /api/v1\nbreed  | - |   Required |  either a reference or inline definition, similar to blueprints. Best practice would be to store the breed separately and reference it from the workflow\nstatus  |  running, stopping, suspending, starting, restarting |   Optional |  restarting will first suspend and then start the workflow (applying any changes since last start). suspending will stop a workflow from running without deleting it. stopping a workflow will delete it (not reversible).\nschedule  | daemon, event, time |   Required |  The workflow schedule. See using workflows - schedules.\nenvironmentvariables (or env) | - |   Optional |  Overrides breed environment variables. You can provide your own variabes here. The following variables are required when using Vamp workflow agent, if not specified here, the configured defaults will be applied: VAMPWORKFLOWEXECUTIONTIMEOUT, VAMPKEYVALUESTORECONNECTION, VAMPKEYVALUESTOREPATH,  VAMPWORKFLOWEXECUTIONPERIOD, VAMPKEYVALUESTORETYPE, VAMPURL. For a workflow that will run forever, also set VAMPAPICACHE=false (by default this is set to true).\nscale  | - |   Optional |  when not specified, the default scale will be applied.    \n network |   |  Optional |\n arguments |    | Optional |    \n    \n    \n------------ \n\nList workflows\n\nReturn a list of all stored workflows. For details on pagination see common parameters\n\n Request\nGET\n/api/v1/workflows\nThe request body should be empty.\n\nResponse\nIf successful, will return a list of workflow resources in the specified accept format (default JSON).\n\n-----------,\n Get single workflow\n\nReturn details of a specific workflow.\n\nRequest\nGET\n/api/v1/worflows/ \nThe request body should be empty.\n\n Response\nIf successful, will return the specified workflow resource in the specified accept format (default JSON).\n\nErrors\nThe requested resource could not be found - there is no stored workflow with the specified workflow_name.\n\n-----------,\n Create workflow\n\nInitiate a workflow.\n\nRequest\nPOST \n/api/v1/workflows\nThe request body should include at least the minimum workflow resource in the specified content-type format (default JSON). \n\n Response\nIf successful, will return the created workflow resource in the specified accept format (default JSON).\n\n-----------,\nUpdate workflow\n\nUpdate a stored workflow.\n\n Request\nPUT\n/api/v1/workflows/ \nThe request body should include at least the minimum workflow resource in the specified content-type format (default JSON). The name field must match the workflow_name specified in the request syntax.\n\nResponse\nIf successful, will return the updated deployment resource in the specified accept format (default JSON).\n\n Example - restart a running workflow\nRequest:\n\nPUT\n/api/v1/workflows/ \nThe request body should include the current workflow resource (GET vamp url/api/v1/workflows/ ) with the status adjusted to status: restarting\n\nResponse:  \nWill return the updated workflow resource.\n\n-----------,\nDelete workflow\n\nDelete a stored workflow.\n\n Request\n\nDELETE\n/api/v1/workflows/ \nThe request body should be empty.\n\nResponse\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\n--------------",
    "id": 23
  },
  {
    "path": "/documentation/api/v0.9.2/using-the-api",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Common parameters",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"API\"",
    "    identifier": "   identifier: \"api-reference-common-parameters-092\"",
    "    weight": "   weight: 20",
    "aliases": "aliases:",
    "content": "\nThe headers described below can be added to all API requests to specify JSON or YAML for requests and responses, and manage pagination for responses.\n\nRequest and response format\nRequests and responses can be formatted in JSON or YAML (default JSON). \n\n| Request parameters         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| content-type   | application/x-yaml or application/json   |      application/json       |  format of the request\n| accept   | application/x-yaml or application/json   |      application/json            | format of the response\n\n Pagination\n\nVamp API endpoints support pagination. See Github's implementation for more info (developer.github.com - traversing with pagination).\n\n| Request parameters         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| page   | integer   |      1            | page to return. Starting from 1, not 0\n| per_page   | integer   |      30            | results to return per page\n| X-Total-Count   | integer   |                  |  the total amount of items (e.g. 349673)\n| Link   |    |                  |  for easy traversing. For example, X-Total-Count: 5522 Link: http://vamp:8080/api/v1/events/get?page=1&perpage=5; rel=first, http://vamp:8080/api/v1/events/get?page=1&perpage=5; rel=prev, http://vamp:8080/api/v1/events/get?page=2&perpage=5; rel=next, http://vamp:8080/api/v1/events/get?page=19&perpage=5; rel=last\n\nExample request\n\nGET http://vamp:8080/api/v1/breeds?page=5&per_page=20\n`",
    "id": 24
  },
  {
    "path": "/documentation/cli/cli-reference",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "CLI reference",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"CLI\"",
    "    weight": "   weight: 10",
    "content": "\nThe VAMP CLI supports the following commands:  \ncreate, deploy, generate, help, info, inspect, list, merge, remove, undeploy, update, version  \nSee using the Vamp CLI for details on installation, configuration and effective use of the CLI\n\nFor details about a specific command, use vamp COMMAND --help\n\n----------,## Create\n\nCreate an artifact read from the specified filename or read from stdin.\n\nvamp create blueprint|breed|deployment|escalation|condition|scale|sla [--file|--stdin]\n\nParameter | purpose\n----------|-----,--file        |       Name of the yaml file [Optional]\n--stdin        |      Read file from stdin [Optional]\n  \nExample\n vamp create scale --file my_scale.yaml\nname: my_scale\ncpu: 2.0\nmemory: 2GB\ninstances: 2\n\n----------, Deploy\n\nDeploys a blueprint\n\nvamp deploy NAME --deployment [--file|--stdin]\n\nParameter | purpose\n----------|-----,--file      |         Name of the yaml file [Optional]\n--stdin     |         Read file from stdin [Optional]\n--deployment|         Name of the deployment to update [Optional]\n\nExample\n vamp deploy --deployment 1111-2222-3333-4444 --file mynewblueprint.yaml\n\n----------, Generate\n\nGenerates an artifact\n\nvamp generate breed|blueprint|condition|scale [NAME] [--file|--stdin]\n| Parameter | purpose |\n|-----------|---------|\n--file    |           Name of the yaml file to preload the generation [Optional]\n--stdin   |           Read file from stdin [Optional]\n\ngenerate breed\n\n| Parameter | purpose |\n|-----------|---------|\n--deployable  |       Deployable specification [Optional]\n\n Example\n vamp generate breed mynewbreed --json\n ,\n  \"environment_variables\": ,\n  \"constants\": ,\n  \"dependencies\": \n \n\ngenerate blueprint\n\n| Parameter | purpose |\n|-----------|---------|\n--cluster   |         Name of the cluster\n--breed     |         Name of the breed   [Optional, requires --cluster]\n--scale     |         Name of the scale   [Optional, requires --breed]\n\n----------, Help\n\nDisplays the Vamp help message\n\nExample\n vamp help\nUsage: vamp COMMAND [args..]\n\nCommands:\n  create              Create an artifact\n  deploy              Deploys a blueprint\n  help                This message\n  generate            Generates an artifact\n  info                Information from Vamp\n  inspect             Shows the details of the specified artifact\n  list                Shows a list of artifacts\n  merge               Merge a blueprint with an existing deployment or blueprint\n  remove              Removes an artifact\n  undeploy            Removes (part of) a deployment\n  update              Update an artifact\n  version             Shows the version of the VAMP CLI client\n  \nRun vamp COMMMAND --help  for additional help about the different command options\n\n----------, Info\n\nDisplays the Vamp Info message\n\nExample\n vamp info\nmessage: Hi, I'm Vamp! How are you?\njvm:\n  operating_system:\n    name: Mac OS X\n    architecture: x86_64\n    version: 10.9.5\n    available_processors: 8.0\n    systemloadaverage: 4.8095703125\n  runtime:\n    process: 12871@MacMatthijs-4.local\n    virtualmachinename: Java HotSpot(TM) 64-Bit Server VM\n    virtualmachinevendor: Oracle Corporation\n    virtualmachineversion: 25.31-b07\n    start_time: 1433415167162\n    up_time: 1305115\n...    \n\n----------, Inspect\nShows the details of the specified artifact\n\nvamp inspect blueprint|breed|deployment|escalation|condition|scale|sla NAME --json\n\n| Parameter | purpose |\n|-----------|---------|\n--as_blueprint | Returns a blueprint (only for inspect deployment) [Optional]|\n--json    |  Output Json instead of Yaml [Optional]|\n\nExample\n vamp inspect breed sava:1.0.0\nname: sava:1.0.0\ndeployable: magneticio/sava:1.0.0\nports:\n  port: 80/http\nenvironment_variables: { \nconstants: { \ndependencies: { \n\n----------, List\nShows a list of artifacts\n\nvamp list blueprints|breeds|deployments|escalations|conditions|gateways|scales|slas\n\nExample \n vamp list deployments\nNAME                                    CLUSTERS\n80b310eb-027e-44e8-b170-5bf004119ef4    sava\n06e4ace5-41ce-46d7-b32d-01ee2c48f436    sava\na1e2a68b-295f-4c9b-bec5-64158d84cd00    sava, backend1, backend2\n\n----------, Merge\n\nMerges a blueprint with an existing deployment or blueprint.\nEither specify a deployment or blueprint in which the blueprint should be merged\nThe blueprint can be specified by NAME, read from the specified filename or read from stdin.\n\nvamp merge --deployment|--blueprint [NAME] [--file|--stdin] \n      \n| Parameter | purpose |\n|-----------|---------|\n--file     | Name of the yaml file [Optional]\n--stdin    | Read file from stdin [Optional]\n\nExample\nvamp merge --blueprint myexistingblueprint -- file addthisblueprint.yaml\n\n----------, Remove\n\nRemoves artifact\n\nvamp remove blueprint|breed|escalation|condition|scale|sla NAME\n\nExample\n vamp remove scale my_scale\n----------, Undeploy\n\nRemoves (part of) a deployment.\nBy only specifying the name, the whole deployment will be removed. To remove part of a deployment, specify a blueprint. The contents of the blueprint will be subtracted from the active deployment.\n\nvamp undeploy NAME [--blueprint|--file|--stdin] \n\nParameter | purpose\n----------|-----,--blueprint|    Name of the stored blueprint to subtract from the deployment\n--file   |       Name of the yaml file [Optional]\n--stdin  |      Read file from stdin [Optional]\n\nExample\n vamp undeploy 9ec50a2a-33d7-4dd3-a027-9eeaeaf925c1 --blueprint sava:1.0\n----------, Update\n\nUpdates an existing artifact read from the specified filename or read from stdin.\n\nvamp update blueprint|breed|deployment|escalation|condition|scale|sla NAME [--file] [--stdin]\n\nParameter | purpose\n----------|-----,--file   |      Name of the yaml file [Optional]\n--stdin  |      Read file from stdin [Optional]\n\n----------,## Version\n\nDisplays the Vamp CLI version information \n\nExample\n vamp version\nCLI version: 0.7.9\n----------,\n See also\n\nUsing the Vamp CLI - installation, configuration and effective use of the CLI",
    "id": 25
  },
  {
    "path": "/documentation/cli/using-the-cli",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Using the Vamp CLI",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"CLI\"",
    "    weight": "   weight: 20",
    "content": "\nVamp's command line interface (CLI) can be used to perform basic actions against the Vamp API. The CLI was\nprimarily developed to work in continuous delivery situations. In these setups, the CLI takes care of automating (canary) releasing new artifacts to Vamp deployments and clusters.\n\nSee also\nFull list of available CLI commands\n\n Installation\n\nCheck the download page for details on how to install the Vamp CLI on your platform. \n\nConfiguration\n\nAfter installation, set Vamp's host location. This location can be specified as a command line option (--host)\n\nvamp list breeds --host=http://192.168.59.103:8080\n\n...or via the environment variable VAMP_HOST\nexport VAMP_HOST=http://192.168.59.103:8080\n\n Simple commands\n\nThe basic commands of the CLI, like list, allow you to do exactly what you would expect:\n\n vamp list breeds\nNAME                     DEPLOYABLE\ncatalog                  docker://zutherb/catalog-frontend\ncheckout                 docker://zutherb/monolithic-shop\nproduct                  docker://zutherb/product-service\nnavigation               docker://magneticio/navigation-service:latest\ncart                     docker://zutherb/cart-service\nredis                    docker://redis:latest\nmongodb                  docker://mongo:latest\nmonarch_front:0.1        docker://magneticio/monarch:0.1\nmonarch_front:0.2        docker://magneticio/monarch:0.2\nmonarch_backend:0.3      docker://magneticio/monarch:0.3\n\n vamp list deployments\nNAME                                    CLUSTERS\n1272c91b-ba29-4ad1-8d09-33cbaa8f6ac2    frontend, backend\n\nCI and chaining\n\nIn more complex continuous integration situations you can use the CLI with the --stdin flag to chain a bunch of commands together. You could for instance:\n\nget an \"old\" version of a breed with inspect\ngenerate a new breed based on the previous one, while inserting a new deployable\ncreate the breed in the backend\n\nvamp inspect breed frontend:$  | \\\nvamp generate breed --deployable mycompany/frontend:$  frontend:$  --stdin | \\\nvamp create breed --stdin\n\nOnce you have the new breed stored, you can insert it into a running deployment at the right position, i.e:\n\nget a blueprint from a running deployment with inspect and --as_blueprint\ngenerate a new blueprint with generate while inserting a new breed\ndeploying the result with deploy\n\nvamp inspect deployment $DEPLOYMENT --as_blueprint | \\\nvamp generate blueprint --cluster frontend --breed frontend:$  --stdin | \\\nvamp deploy --deployment $DEPLOYMENT --stdin\n\n------, See also\nFull list of available CLI commands",
    "id": 26
  },
  {
    "path": "/documentation/how vamp works/architecture-and-components",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Architecture and components",
    "aliases": "",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"How Vamp works\"",
    "    weight": "   weight: 10",
    "content": "\nArchitecture\nVamp and the Vamp Gateway Agent require specific elements in your architecture to handle orchestration, routing, persistence and metrics aggregation. There is no set architecture required for running Vamp and every use case or specific combination of tools and platforms can have its own set up.\n\n Example topology\nThe below diagram should be used more as an overview than required architecture. For example, in this diagram the Mesos/Marathon stack and Elasticsearch are included even though these are not a hard dependency. Vamp can be configured to run with other container schedulers, log-aggregators, key-value and event-stores.\n\nVamp components\n\nVamp consists of server- and client-side components that work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation.\n\n Vamp UI  \nThe Vamp UI is a graphical web interface for managing Vamp in a web browser. It is packaged with Vamp.\n\nVamp CLI  \nThe Vamp CLI is a command line interface for managing Vamp and providing integration with (shell) scripts. It's currently not very well maintained but still can be useful if our REST API cannot be used for your integration requirements.\n\n Vamp  \nVamp is the main API endpoint, business logic and service coordinator. Vamp talks to the configured container manager (Docker, Marathon, Kubernetes etc.) and synchronizes it with Vamp Gateway Agent (VGA)  via ZooKeeper, etcd or Consul (distributed key-value stores). Vamp can use Elasticsearch for artifact persistence and to store events (e.g. changes in deployments). Typically, there should be one Vamp instance and one or more VGA instances. Vamp is not a realtime application and only updates deployments and routing when asked to (reactive) and thus doesn't need to run with multiple instances in HA mode. If this is a hard requirement of your project please contact us for the Vamp Enterprise Edition.\n\nVamp workflows\nVamp workflows are small applications or scripts (for example using JavaScript or your own containers) that automate changes of the running system, and its deployments and gateways. We have included a set of useful workflows out of the box, such as health and metrics, which are used by the Vamp UI to report system status and to enable autoscaling and self-healing. Our Vamp Runner project provides more advanced workflow recipes as an example.\n\n Vamp Gateway Agent (VGA)  \nVamp Gateway Agent (VGA) reads the HAProxy configuration from ZooKeeper, etcd or Consul and reloads HAProxy on each configuration change with as close to zero client request interruptions as possible. Typically, there should be one Vamp instance and one or more VGA instances.     \nLogs from HAProxy are read over socket and pushed to Logstash over UDP.  VGA will handle and recover from ZooKeeper, etcd, Consul and Logstash outages without interrupting the HAProxy process and client requests.  \n\n  \nRead about the requirements to run Vamp\n  \n",
    "id": 27
  },
  {
    "path": "/documentation/how vamp works/events-and-metrics",
    "date": "2016-10-21T09:00:00+00:00",
    "title": "Events and metrics",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"How Vamp works\"",
    "    weight": "   weight: 60",
    "content": "\nTo provide an effective feedback loop, HTTP/TCP logs should be collected, stored and analyzed. Collection and storing is done with a combination of HAProxy, VGA and Logstash setup. Logs are stored in Elasticsearch and can be later visualised by Kibana.\n\nHow traffic is logged\n\nVamp uses Logstash to format and store logs from running applications HAProxy (via Vamp Gateway Agent) in Elasticsearch indices. Logstash listens on UDP port and formats incoming log raw data in json format. Vamp API actions (including those generated by Vamp workflows and the Vamp UI) and running service events are stored by the Vamp API to specific Elasticsearch indices. \nData and events are read by Vamp components either directly from Elasticsearch or via the Vamp API:\n\nThe Vamp UI reads data and events via the Vamp API. Health and Metrics events (generated by Vamp workflows) are required by the Vamp UI.\nVamp workflows read formatted log data directly from Elasticsearch and events are via the Vamp API. In theory, workflows could also read events directly from Elasticsearch.\n\n Formatted raw data (logs)\n\nHAProxy\nHAProxy generates logs and makes them accessible via open socket - check the HAProxy configuration of log (github.com/magneticio - haproxy.cfg).\nVGA listens on log socket and any new messages are forwarded to the Logstash instance. The HAProxy log format is configurable in Vamp configuration vamp.gateway-driver.haproxy (github.com/magneticio - reference.conf).\nIn general, for each HTTP/TCP request to HAProxy, several log messages are created (e.g. for gateway, service and instance level). \n\n Logstash\n\nA simple Logstash configuration should be sufficient for dozens of requests per second - or even more, depending on whether Elastic Stack (ELK) is also used for custom application/service logs etc. This example Logstash configuration (github.com/magneticio - logstash.conf), together with the default vamp.gateway-driver.haproxy log format, will transform logs to plain JSON, which can be parsed easily later on (e.g. for Kibana visualisation).  \nFor alternative Logstash/Elasticsearch setups you can check these examples (elastic.co - Deploying and Scaling Logstash) and the Logstash command line parameter (github.com/magneticio - Logstash section).\n\n  \nLogstash listens on UDP port, but in principle any other listener can receive logs forwarded by VGA.\nDifferent VGAs can use different Logstash instances.\n  \n\nEvents\n\nVamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions.\nAll events are stored and retrieved using the Event API, part of the Vamp API.\n\n Kibana\n\n  Vamp can be configured to create Kibana searches, visualisations and dashboards automatically with the vamp.gateway-driver.kibana.enabled configuration parameter.\n  Vamp will do this by inserting ES documents to the Kibana index, so only the URL to access ES is needed (by default reusing the same as for persistence). Read more about Vamp configuration\n\n  \nLet's install Vamp \n  ",
    "id": 28
  },
  {
    "path": "/documentation/how vamp works/persistence-key-value-store",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Persistence and key-value (KV) store",
    "content": "\nPersistence \nVamp uses Elasticsearch (ES) as main persistence (e.g. for artifacts and events). \nVamp is not demanding in ES resources, so a small ES installation is sufficient for Vamp indices (index names are configurable). Vamp can also use an existing ES cluster.\n\n Key-value (KV) store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances - all communication is done by managing specific KV in the store.  Currently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\n  \nRead about routing and load balancing\n  \n\n",
    "id": 29
  },
  {
    "path": "/documentation/how vamp works/requirements",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Requirements",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"How Vamp works\"",
    "    weight": "   weight: 20",
    "content": "\nVamp's components work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. To achieve this, Vamp requires access to a container scheduler, key value store, Elastic Search and HAProxy.\n\nContainer scheduler  (orchestration)\nVamp talks directly to your choice of container scheduler. Currently we support Mesos/Marathon, DC/OS, Kubernetes and Rancher. In case you’re “greenfield” and don’t have anything selected or running yet, check which container scheduler?\n\n Key value store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances, all communication is done by managing specific KV in the store.  When Vamp needs to update the HAProxy configuration (e.g. when a new service has been deployed) Vamp will generate the new configuration and store it in the KV store. The VGAs read specific valuea and reload HAProxy instances accordingly.\nCurrently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\nElastic Search (persistence and metrics)\nVamp uses Elastic Search (ES) for persistence (e.g. for artifacts and events) and for aggregating the metrics used by Vamp workflows and the Vamp UI. As Vamp is not demanding in ES resources, it can comfortably work with an existing ES cluster.  \nCurrently we use Logstash to format and send data to Elastic Search, but you could also opt for an alternative solution.\n\n HAProxy  (routing)\nEach Vamp Gateway Agent (VGA) requires its own instance of HAProxy. This is a hard requirement, so to keep things simple we provide a Docker container with both Vamp Gateway Agent (VGA) and HAProxy (hub.docker.com - magneticio/vamp-gateway-agent).  \n\n  \nFind out how to install Vamp\nHigh level pointers for choosing a container scheduler\n  \n\n",
    "id": 30
  },
  {
    "path": "/documentation/how vamp works/routing-and-load-balancing",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Routing and load balancing",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"How Vamp works\"",
    "    weight": "   weight: 50",
    "content": "Vamp uses the tried and tested HAProxy reverse proxy software for routing/proxying and load balancing (haproxy.com). Vamp Gateway Agent (VGA) manages the HAProxy configuration and HAProxy routes incoming traffic to endpoints (explicitly defined external gateways) or handles intra-service routing. By applying some iptables magic, Vamp makes sure that HAProxy configuration updates won't introduce dropped packages., that means zero-downtime reloads.  \n\nRouting\n\nSo how does Vamp exactly route traffic to the designated destinations? First we look for the conditions that might have been set for a route or gateway. This can be none, one or more conditions (see boolean expression in conditions). There are built-in short codes for common conditions, or you can use HAProxy ACLs directly.\n\nIf the condition is met, we evaluate the condition strength percentage. A 100% setting means everybody that meets the condition is sent to this route. A 5% setting means 5% of all visitors that meet the condition are sent to this route, the remaining 95% are returned into the \"bucket\" and distributed using the general weight settings. A weight setting for each available route defines the distribution of all remaining traffic not matching a condition or not targetted by condition strength.\n\n Load balancing\n\nVamp load balancing is done transparently. Based on the scale setting of the running services, Vamp will make sure all instances are load balanced automatically. By default we use a round-robin algorithm, but other HAProxy balancing mechanisms are also supported. We also support sticky routing. The cool thing is that weight and condition percentage settings are applied independently from the number of instances running. I.e. a 50 / 50 weight distribution over two service versions that run with a scale of four and eight instances respectively will still be distributed 50% / 50%. Changing the number of instances will have no effect on the distribution, as Vamp tries to achieve the configured weight and condition strength distributions as closely as possible.\n\nTopology and performance\n\nHAProxy can run as a container or as a standalone service. A Vamp Gateway Agent (VGA) Docker image (including HAProxy with specific logstash configuration to provide proxy logs to logstash for the Vamp UI) can be pulled from the Docker hub (hub.docker.com - magneticio Vamp Gateway Agent).\n\nHAProxy can run inside your cluster or on separate machines outside of your container cluster:\n\nHAProxy on one node of a cluster - not advised for production setups as it introduces a single point of failure\nHAProxy on multiple nodes of a cluster - for example, three instances for failover and high-availability \nHAProxy on all nodes of a cluster - the so-called SmartStack pattern (nerds.airbnb.com - smartstack service discovery cloud) \nHAProxy on separate machines outside of your container cluster - Vamp can connect to these instances and can add its routing rules to your custom HAProxy configuration templates if needed.\n\nPerformance-wise, HAProxy is very efficient and uses few resources. In our experiments we have seen a sub-millisecond overhead. Even with very complex and combined routing rules, the total overhead stays in the microseconds range. This means a single HAProxy running on a small VM (for example, an AWS micro instance) would process enough network traffic that you would notice a bottleneck from your network and applications first.\n\nVamp is not a realtime system. As long as at least one HAProxy and one container-node are running your visitors will be able to reach the container - even with no Vamp or VGA running. On restart, Vamp and VGA will automatically sync and update themselves. \n\n  \nRead about how Vamp works with events and metrics\nFind out more about using Vamp conditions and gateways\n  \n",
    "id": 31
  },
  {
    "path": "/documentation/how vamp works/service-discovery",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Service discovery",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"How Vamp works\"",
    "    weight": "   weight: 40",
    "content": "\nVamp uses a service discovery pattern called server-side service discovery, which allows for service discovery without the need to change your code or run any other daemon or agent (microservices.io - server side discovery). In addition to service discovery, Vamp also functions as a service registry (microservices.io - service registry).\n\nFor Vamp, we recognise the following benefits of this pattern:\n\nNo code injection needed.\nNo extra libraries or agents needed.\nplatform/language agnostic: it’s just HTTP.\nEasy integration using ENV variables.\n\n  \nMore about service discovery\nRead about how Vamp works with routing and load balancing\n  ",
    "id": 32
  },
  {
    "path": "/documentation/how vamp works/which-container-scheduler",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Which container scheduler?",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"How Vamp works\"",
    "    weight": "   weight: 30",
    "content": "Vamp can run on top of Mesos/Marathon, DC/OS, Kubernetes and Rancher (Docker Swarm support is coming soon). In case you’re “greenfield” and don’t have anything selected or running yet, here are some high-level pointers to help you make an informed decision: \n\nWorking with big data - Mesos/Marathon, DC/OS, Azure Container Service\nRunning web-based applications - Kubernetes, Google Container Engine\nManaging (virtual) infrastructure - Rancher with Docker or Kubernetes\nJust running Docker\n\nWhichever option you choose now, Vamp is container systems agnostic, so all your blueprints and workflows will keep on working if you decide to switch in the future *.\n\n----,\nWorking with big data \nIf you are working with typical big data solutions like Kafka, Cassandra or Spark (often combined in something called SMACK stack), and/or want to run not only containers on your cluster it makes sense to investigate Mesos/Marathon first. A lot of big data frameworks can run as native Mesos frameworks and you can combine the underlying infrastructure to share resources between these frameworks running on Mesos and your containers running inside Marathon (which is a Mesos framework in itself).\n\n Add in commercial support\nIf you have the same requirements as described above, but you're more comfortable buying commercial support, moving towards DC/OS makes sense. DC/OS is based on Mesos/Marathon, but adds additional features and a fancy UI. You can purchase commercial DC/OS support or buy an enterprise version from Mesosphere (the company that initiated Mesos and Marathon).\n\nHosted solution\nIf you're looking for a hosted version of DC/OS you could investigate Azure Container Service which let's you choose between DC/OS or Docker Swarm.\n\n----,\n Running web-based applications  \nIf you're solely interested in running (micro)services, APIs and other web-based applications, Kubernetes is an integrated cluster-manager and -scheduler, and is specifically designed for running containers with web-focused payloads. \n\nAdd in commercial support\nAt this point, commercial support and fancy dashboards are less easy to find for Kubernetes. However, Kubernetes is the scheduler used in Redhat software (Openshift V3), so if your company used Redhat software this might make sense to investigate.\n\n hosted solution\nA hosted version of Kubernetes is available from Google (Google Container Engine). \n\n----,\nManaging (virtual) infrastructure\nIf you want to manage and provision (virtual) infrastructure as well as manage and run containers, Rancher is a viable option. Rancher provides a Docker or Kubernetes based container scheduler and adds infrastructure provisioning with a nice graphical UI.\n\n----,\n Just running Docker\nIf you want to stay within the Docker ecosystem, Vamp works nicely with (single machine) Docker. Docker Swarm support is coming up soon.\n\n----,  \n* Note that Vamp dialects and some specific metric store settings are scheduler or container cloud specific.\n\n  \nFind out how to install Vamp \nRead about the requirments to run Vamp \n  ",
    "id": 33
  },
  {
    "path": "/documentation/installation/azure-container-service",
    "date": "2016-09-30T12:00:00+00:00",
    "title": "Azure Container Service",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 80",
    "content": "\nTo run Vamp together with Azure Container Service (azure.microsoft.com - Container Service), you need to use DC/OS as the default ACS Docker container scheduler. \n\nTo install DC/OS in Azure you should follow these steps: https://dcos.io/docs/1.8/administration/installing/cloud/azure/\n\nAfter you have activated your ACS setup with DC/OS, go to your DC/OS admin environment and install Vamp using our DC/OS installation instructions.\n\n  \n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n  \n",
    "id": 34
  },
  {
    "path": "/documentation/installation/configuration-reference",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Configuration reference",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 110",
    "content": "\nThis page describes the structure and parameters in the Vamp configuration files (reference.conf and application.conf).  \nFor details on how to customise your Vamp configuration, see how to configure Vamp.\n\nVamp configuration parameters\nThe full reference.conf file can be found in the Vamp project repo (github.com/magneticio - Vamp reference.conf). \n\n Vamp  \nVamp configuration is described in sections, nested inside a parent vamp {  tag. Usage, defaults and requirements for each section are outlined below: info, stats, model, persistence, container driver, workflow driver, http-api, gateway driver, pulse, operation, lifter, common\n\nakka  \nVamp is based on the Akka library. Akka configuration is included in reference.conf inside the akka {  tag. These settings can be tweaked in application.conf (advanced use only). Refer to the akka documentation for details.\n\n----,\n Info \nSettings required for gathering information on Vamp and components (memory and load). You can also specify an intro message here.\ninfo  \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,message  | - |    Hi, I'm Vamp! How are you? |  The welcome message displayed in the Vamp info pane\ntimeout    | - |    3 seconds |  Response timeout for each component (e.g. Persistance, Container Driver...). How long we will wait for components to reply on an API info call. Should be less than http-api.response-timeout.\n\n----,\nStats\nAggrgated statistics for Vamp.\nstats  \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  timeout  | - |  5 seconds  |  Response timeout for each component\n\n----, Model\n\nmodel  \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  default-deployable-type  | container/docker, container/rkt  |  container/docker  |  The default container type\n\n----,\nPersistence\nVamp uses Elasticsearch or a key-value store for persistence. The key-value store is also used to hold the HAProxy configuration. Currently supported options: \n                                                                                                \nZooKeeper (apache.org - ZooKeeper)\netcd (coreos.com  - etcd documentation) \nConsul (consul.io)\n\npersistence  \n\n    key-value  \n   \n\n  key-value-store  \n\n    etcd.url = \"\"\n\n    consul.url = \"\"\n   \n \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  response-timeout   |  -  |   5 seconds  |  \n  database.type   |  elasticsearch, key-value, in-memory  |   -  |  set in application.conf\n  database.elasticsearch.url               |   |  -   |  set in application.conf\n  database.elasticsearch.response-timeout  | -  |  5 seconds   |  Timeout for elasticsearch operations\n  database.elasticsearch.index             | -  |  vamp-persistence   |  \n  database.key-value.caching               | true, false   |  false  |  set this to true to make it easier on the persistence store load\n  key-value-store.type                     | zookeeper, etcd, consul   |   -  |  set in application.conf\n  key-value-store.base-path                | -  |  /vamp  |  \n  key-value-store.zookeeper.servers        | - |  -   |  Required when persistence.key-value-store.type = \"zookeeper\" \n  key-value-store.zookeeper.session-timeout | - |   5000  |   \n  key-value-store.zookeeper.connect-timeout | - |   5000  |  \n  etcd.url                                 | - |  -   |  Rquired when persistence.key-value-store.type = \"etcd\"   \n  consul.url                               | -  |   -  |  Required when persistence.key-value-store.type = \"consul\"  \n\n----,\n Container driver\nVamp can be configured to work with Docker, Mesos/Marathon, Kubernetes or Rancher container drivers. Only configuration for the specified container-driver.type is required.  \nSee the example configurations.\ncontainer-driver  \n   \n\n  mesos.url = \"\"\n  marathon  \n\n  kubernetes  \n\n  rancher  \n   \n\n  response-timeout = 30 seconds\n \nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----, type            |  docker, kubernetes, marathon, rancher     |  -   |  set in application.conf. Also include the configuration section for the specified container driver type (see below).\n response-timeout  | -   |  30 seconds   |  Timeout for container operations\n mesos.url         | -   |   -  |  Used for information. Required only when container-driver.type = \"marathon\" \n\nContainer-driver.docker\nRequired only when container-driver.type = \"docker\" .  \n  docker  \n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  workflow-name-prefix      | -  |   vamp-workflow-  |  \n  repository.email          | -   |  -   |  Docker repository credentials\n  repository.username       | -   |   -  |  Docker repository credentials\n  repository.password       | -   |   -  |  Docker repository credentials    \n  repository.server-address  | -  |   -  |  Docker repository credentials \n          \n Container-driver.kubernetes\nRequired only when container-driver.type = \"kubernetes\" .  \nSee the example configuration.\n  kubernetes  \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  url                   | -   |   -  |  Kubernetes API URL\n  workflow-name-prefix   | -  |  vamp-workflow-   |  \n  service-type          | NodePort, LoadBalancer   |   NodePort  |                 \n  create-services        | true, false   |   true  |  when set to false, gateways will not be exposed as a service (access will only be possible through gateway agent)\n  vamp-gateway-agent-id  | -   |  vamp-gateway-agent   |  \n  bearer   | - | - |\n  token                  | -  |  /var/run/secrets/kubernetes.io/serviceaccount/token   |  \n\nContainer-driver.marathon\nRequired only when container-driver.type = \"marathon\" . note that you should also set container-driver.mesos.url.  \n\n  marathon  \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  user             | -         |   -  |  \n  password           | -      |   -  |       \n  url                  | -     |   -  |  \n  sse                    | true, false   |   true  |  When set to true, Vamp will listen on Marathon events which allows for quicker reaction to Marathon changes\n  workflow-name-prefix   | -   |   vamp/workflow-  |   \n\n Container-driver.rancher\nRequired only when container-driver.type = \"rancher\" .  \nSee the example configuration.\n  rancher  \n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  url  | - |    -  |  Rancher API URL\n  workflow-name-prefix  | -  |   vamp-workflow-  |            \n  user  | - |    -  |  API authentication credentials (if required)\n  password | - |    -  |       API authentication credentials (if required)\n  environment.name | - |   -   |       \n  environment.deployment.name-prefix | - |   -   |       \n\n-------     \n     \nWorkflow driver\nUsed for Vamp workflows. \nworkflow-driver  \n    scale  \n    arguments: []\n    network = \"BRIDGE\"\n    command = \"\"\n   \n \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  type  | docker, marathon, kubernetes, rancher, chronos, none |   none   |  Daemon (docker, marathon, kubernetes, rancher), time and event triggered (chronos). Can be combined (csv), e.g. marathon,chronos\n  response-timeout  | -  |  30 seconds   |  Timeout for container operations\n  vamp-url   | -|    -   |  The URL that workflow agent (workflows) will use to access Vamp. Set in application.conf. Required for all workflows which need to access Vamp.\n  chronos.url   | - |     -  |  if you use chronos, set the URL in application.conf\n\n Workflow-driver.workflow\nApplied when a worklow is deployed (run).\n\n  workflow  \n     \n    environment-variables = []\n    scale  \n    arguments: []\n    network = \"BRIDGE\"\n    command = \"\"\n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  deployables  | - |    \"application/javascript\" =  \n \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  interface  | -   |   0.0.0.0  |  \n  host   | - |    localhost  |  \n  port   | - |   8080   |  The port Vamp runs on\n  response-timeout   | - |    10 seconds  |  HTTP response timeout\n  strip-path-segments   | - |    0  |  \n  sse.keep-alive-timeout   | - |    15 seconds  |  timeout after an empty comment (\":\\n\") will be sent in order keep connection alive\n  ui.directory   | - |   -  |  set in application.conf to use the Vamp UI\n  ui.index  | - |    -  |  index file, e.g. $ \"/index.html\". Set in application.conf to use the Vamp UI\n\n----,\n Gateway driver\nThe gateway-driver section configures how traffic should be routed through Vamp Gateway Agent. Read more about how Vamp uses these parameters for service discovery.\ngateway-driver  \n    tcp-log-format = \"\"\" \"\"\"\n    http-log-format = \"\"\" r \"\"\"\n   \n\n  elasticsearch.metrics  \n \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  host    | - |   localhost  |  The Vamp Gateway Agent/HAProxy, internal IP. To simplify service discovery, Vamp supports using specific environment parameters.  .host will have the value of this parameter (vamp.gateway-driver.host)\n  response-timeout    | - |  30 seconds   |  timeout for gateway operations\n  elasticsearch.metrics.index   | -  |  -   |  \n  elasticsearch.metrics.type  | -   |   -  |  \n\nGateway-driver.haproxy\n  haproxy  \n    tcp-log-format = \"\"\" \"\"\"\n    http-log-format = \"\"\" r \"\"\"\n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  version   |  1.5, 1.6 |   1.6  | \n  ip    | - |   127.0.0.1  |  local IP used for chaining gateways.  HAProxy backend server IP\n  template    | -  |  -  |  HAProxy configuration template file, can be edited to customise HAProxy configuration. If not specified the default will be used /io/vamp/gateway_driver/haproxy/template.twig\n  socket-path   | -  |   /usr/local/vamp  |  \n  virtual-hosts.ip   | -  |   0.0.0.0  |  IP, if virtual hosts are enabled\n  virtual-hosts.port  | -   |  80   |  Port, if virtual hosts are enabled\n  tcp-log-format   | -  |   see above  |  \n  http-log-format  | -   |  see above   |  \n\n----,\n Pulse\nHandles all Vamp events.\n\npulse  \n   \n  response-timeout = 30 seconds\n \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  elasticsearch.url   | -   |  -   |  e.g http://localhost:9200\n  elasticsearch.index.name   | -   |   vamp-pulse  |  \n  elasticsearch.index.time-format.event   | |  YYYY-MM-dd   |  \n  response-timeout   | -   |  30 seconds   |  timeout for pulse operations\n\n----,\nOperation\nThe operation section holds all parameters that control how Vamp executes against “external” services: this also includes Vamp Pulse and Vamp Gateway Agent.\noperation  \n\n    timeout  \n\n    check  \n   \n\n  deployment  \n\n    arguments = [] \n   \n\n  gateway  \n\n  sla.period = 5 seconds \n  escalation.period = 5 seconds \n\n  health.window = 30 seconds \n\n  metrics.window = 30 seconds \n\n  gateway.virtual-hosts =  \n   \n \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  sla.period    | -   |  5 seconds  |  controls how often an SLA checks against metrics\n  escalation.period   | -    |  5 seconds  |  controls how often Vamp checks for escalation events\n  health.window    | -   |  30 seconds  |  \n  metrics.window    | -   |  30 seconds  | \n     \noperation.synchronisation\n\n  synchronization  \n\n    timeout  \n\n    check  \n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----, initial-delay  | -   |   5 seconds  |  \n period  | -    |   6 seconds  |  controls how often Vamp performs a sync between Vamp and the container driver. synchronization will be active only if period is greater than 0\n mailbox.mailbox-type   | -   |   akka.dispatch.NonBlockingBoundedMailbox   |  \n mailbox.mailbox-capacity | -   |     100  |  Queue for operational tasks (deployments etc.)\n timeout.ready-for-deployment   | -   |    600 seconds  |  controls how long Vamp waits for a service to start. If the service is not started before this time, the service is registered as \"error\". If set to 0, Vamp will keep trying forever.\n timeout.ready-for-undeployment  | -    |   600 seconds   |  similar to \"ready-for-deployment\" (above), but for the removal of services.\n check.cpu   | true, false   |    false  |  \n check.memory  | true, false |    false   |  \n check.instances  | true, false |    false   |  \n     \n operation.deployment\n\n  deployment  \n\n    arguments = []  \n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  scale   | - |  instances = 1 cpu = 1  memory = 1GB  |  default scale, used if not specified in blueprint\n  arguments | -  |   -  |  Docker command line arguments, e.g. \"security-opt=seccomp:unconfined\". Split by first '=' \n\noperation.gateway\nFor each cluster and service port within the same cluster a gateway is created - this is exactly as one that can be created using Gateway API.\nThat means specific conditions and weights can be applied on traffic to/from cluster services - A/B testing and canary releases support. \nvamp.operation.gateway.port-range is range of port values that can be used for these cluster/port gateways. These ports need to be available on all Vamp Gateway Agent hosts.\n  gateway  \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  port-range   | - |  40000-45000   |   range of port values that can be used for Vamp internal gateways. These ports need to be available on all Vamp Gateway Agent hosts\n  response-timeout  | -   |   5 seconds  |  timeout for container operations\n\n operation.gateway.virtual-hosts\nDefines the standard Vamp virtual host gateway format.\n\n  gateway.virtual-hosts =  \n   \n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  enabled  | true, false |  true   |  if set to false, Vamp will not automatically generate gateway virtual host names. You can still specify in gateways/blueprints.\n  formats.gateways  | -  |   $gateway.vamp  |  name format\n  formats.deployment-port   | - |  $port.$deployment.vamp   |  name format\n  formats.deployment-cluster-port   | - |  $port.$cluster.$deployment.vamp   |  name format\n     \n---,\nLifter\nLifter is the Vamp bootstrap installer. The lifter configuration specifies items that Vamp should run on startup. No items are included by default in reference.conf, these need to be added in application.conf. We advise that you at least run the Vamp Health and Metrics workflows, as these are required for the Vamp UI.\n\nFor example (taken from the DC/OS template application.conf):\n\n  lifter.artifact.files = [\n    \"/usr/local/vamp/artifacts/breeds/health.js\",\n    \"/usr/local/vamp/artifacts/workflows/health.yml\",\n    \"/usr/local/vamp/artifacts/breeds/metrics.js\",\n    \"/usr/local/vamp/artifacts/workflows/metrics.yml\",\n    \"/usr/local/vamp/artifacts/breeds/kibana.js\",\n    \"/usr/local/vamp/artifacts/workflows/kibana.yml\",\n    \"/usr/local/vamp/artifacts/breeds/vga.js\",\n    \"/usr/local/vamp/artifacts/workflows/vga.yml\"\n    ]\n   \n----, Common\n\ncommon.http.client.tls-check = true\n\nParameter  |  Options  |  Default |  Details  \n------------|-------|--------|-----,  http.client.tls-check  | true, false  |  true  |  If set to false tls-check will be disabled, for example to allow Vamp to accept invalid certificates.\n\n  \nRead about how to configure Vamp\nLook at some example configurations\nFollow the tutorials\nYou can read in depth about using Vamp or browse the API reference or CLI reference docs.\n  \n",
    "id": 35
  },
  {
    "path": "/documentation/installation/configure-vamp",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "How to configure Vamp",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 100",
    "content": "\nVamp configuration is held in a combination of the Vamp application.conf and reference.conf files following the HOCON file standard (github.com/typesafehub - config). You can override settings in the configuration files using Vamp environment variables or Java/JVM system properties. Vamp configuration is built in layers following this order:\n\nreference.conf - part of the Vamp code. Contains generic, default settings. Not a full configuration.\napplication.conf - adds environment specifics to the generic reference.conf defaults.\nJava system properties - advised for advanced use only.\nEnvironment variables - override all other settings.\n\nOn this page:\n\nOverride specific configuration parameters\nUse a custom application.conf file\nParameterize application.conf\nInclude configuration not intended for Vamp\nAccess configuration parameters through the API\n\n Override specific configuration parameters\nYou can override specific parameters set in the application.conf and reference.conf configuration files using Vamp environment variables or Java/JVM system properties. It is advisable to use environment variables when overriding specific parameters.\n\nEnvironment variables\nEnvironment variables override all other settings. Convert the configuration parameter name to upper case and replace all non-alphanumerics with an underscore `.  So, vamp.gateway-driver.timeout becomes VAMPGATEWAYDRIVERTIMEOUT`.  \n\nFor example, to change the vamp.info.message you would set the environment variable VAMPINFOMESSAGE :\n\ndocker run --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v docker-machine ssh default \"which docker\":/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=$(docker-machine ip default)\" \\\n           -e \"VAMPINFOMESSAGE=hey YOU! \" \\\n           magneticio/vamp-docker:0.9.2\n\n Java system properties\nConfiguration by system properties is advised for advanced use only. Overriding specific settings can be best handled using environment variables. If you require extensive customisation, consider creating a new docker image with a custom application.conf file. For example:\n\nexport VAMPINFOMESSAGE=Hello # overriding Vamp info message (vamp.info.message)\n\njava -Dvamp.gateway-driver.host=localhost \\\n     -Dlogback.configurationFile=logback.xml \\\n     -Dconfig.file=application.conf \\\n     -jar vamp.jar\n\nUse a custom application.conf file\nFor more extensive customisations, you can create a new Docker image, extending one of the provided Vamp images with a custom application.conf file. The below example explains the steps for creating a Docker image with a custom DCOS config, if you are using a different container management platform you should use the associated application.conf and adjust the Docker file accordingly.\n\nCopy application.conf (github.com/magneticio - Vamp DCOS application.conf)\nAdjust as required. Check the list of configuration settings (below) for details of the available optionsa\nCreate a Dockerfile with the lines:  \n  FROM magneticio/vamp-dcos:0.9.2  \n  ADD application.conf /usr/local/vamp/conf/\nBuild the image with docker build --tag username/vamp\n\n Parameterize application.conf\n\nTo avoid duplication of configuration and make it easier to overwrite specific settings you can define global variables outside the vamp   stanza, at the top of application.conf. \n\nFor example:\nvamp_host = \"localhost\"    # Default value\nvamphost = $   # If environment variable exists, use this value\n\nvamp  \n    timeout = 3 seconds\n     \n \n\nThis is as of 0.9.2 how we configure our DC/OS Docker image\n\n(Typesafe documentation on the topic covering system or env variable overrides)[https://github.com/typesafehub/config#optional-system-or-env-variable-overrides]\n\nInclude configuration not intended for Vamp\nIt is possible to store configuration parameters not intended for use by Vamp itself in the Vamp application.conf file, such as configuration for Logstash or workflows. For example, you could chose to include the logstash URL in your custom application.conf file - Vamp would ignore the parameter, but it would be available to all workflows through the API. This is useful for storing shared local configuration parameters. Configuration specific to a single workflow is best set using environment variables or by hard coding the parameter.\n\n Access configuration parameters through the API\nAll configuration parameters can be retrieved from the Vamp API endpoint config or configuration. \n\nReturn all configuration parameters as a JSON object:  \n  GET /api/v1/config .\n  \nReturn a single paramater:  \n  GET /api/v1/config/configuration parameter name  \n\nFor example GET vamp url/api/v1/config/vamp.info.message \n\nOr, from a workflow using Vamp node.JS client (github.com/magneticio - Vamp Node.js Client):\n\napi.config().each(function (config)  );\n\n  \nCheck the configuration reference\nLook at some example configurations\nFollow the tutorials\nYou can read in depth about using Vamp or browse the API reference or CLI reference docs.\n  \n",
    "id": 36
  },
  {
    "path": "/documentation/installation/dcos",
    "date": "2016-09-30T12:00:00+00:00",
    "title": "DC/OS 1.7 and 1.8",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 30",
    "content": "\nThere are different ways to install Vamp on DC.OS. On this page we start out with the most common setup, but if you are interested in doing a custom install or working with public and private nodes you should jump to that section.\n\nStandard install\nCustom install\nPublic and private nodes\n\nStandard install\nThis setup will run Vamp, Mesos and Marathon, together with Zookeeper, Elasticsearch and Logstash on DC/OS. \n\n Tested against\nThis guide has been tested on both 1.7 and the latest 1.8 version of DC/OS.\n\nRequirements\nBefore you start you need to have a DC/OS cluster up and running, as well as the its CLI configured to use it. We assume you have it up and running on http://dcos.example.com/.\nSetting up DC/OS is outside the scope of this document, for that you need to refer to the official documentation:\n\nhttps://dcos.io/docs/1.7/administration/installing/\nhttps://dcos.io/docs/1.7/usage/cli/\nhttps://dcos.io/docs/1.8/administration/installing/\nhttps://dcos.io/docs/1.8/usage/cli/\n\n Step 1: Install Elasticsearch + Logstash\n\nMesos, Marathon and ZooKeeper are all installed by DC/OS. In addition to these, Vamp requires Elasticsearch and Logstash for metrics collection and aggregation.\n\nYou could install Elasticsearch on DC/OS by following the Mesos Elasticsearch documentation (mesos-elasticsearch - Elasticsearch Mesos Framework).\nHowever, Vamp will also need Logstash (not currently available as a DC/OS package) with a specific Vamp Logstash configuration (github.com/magneticio - Vamp Docker logstash.conf).  \n\nTo make life easier, we have created compatible Docker images for a Vamp Elastic Stack (hub.docker.com - magneticio elastic) that you can use with the Mesos elasticsearch documentation (mesos-elasticsearch - How to install on Marathon).\nOur advice is to use our custom Elasticsearch+Logstash Docker image. Let's get started!\n\nCreate elasticsearch.json with the following content:\n\n \n   ,\n  \"healthChecks\": [\n     \n  ]\n \n\nThis will run the container with 1G of RAM and a basic health check on the elasticsearch port.\n\nUsing the CLI we can install this in our cluster:\n\n$ dcos marathon app add elasticsearch.json\n\nIf you get no error message you should now be able to see it being deployed:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    0/1    0/0      scale       DOCKER   None  \n\nOnce it's fully up and running you should see all tasks and health checks being up:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n\nStep 2: Deploy Vamp\n\nOnce you have elasticsearch up and running it's time to move on to Vamp. The Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.\n\nCreate vamp.json with the following content:\n\n \n         \n      ],\n      \"forcePullImage\": true\n     \n   ,\n  \"labels\":  ,\n  \"env\":  ,  \n  \"healthChecks\": [\n     \n  ]\n \n\nThis service definition will download our Vamp container and spin it up in your DC/OS cluster on a private node in bridge networking mode. It will also configure the apporiate labels for the AdminRouter to expose the UI through DC/OS, as well as an internal VIP for other applications to talk to Vamp, adjusting some defaults to work inside DC/OS, and finally a health check for monitoring.\n\nDeploy it with the CLI, like with did with elasticsearch:\n\n$ dcos marathon app add vamp.json\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n/vamp/vamp      1024  0.5    0/1    0/0      scale       DOCKER   None  \n\nIt will take a minute for Vamp to deploy all its components, you can see that by looking in the \"tasks\" column, where Vamp is listed as 0/1. Run the list command again and you should see all the components coming online:\n\n$ dcos marathon app list\nID                        MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD\n/elasticsearch            1024  0.2    1/1    1/1       ---        DOCKER   None\n/vamp/vamp                1024  0.5    1/1    1/1       ---        DOCKER   None\n/vamp/vamp-gateway-agent  256   0.2    3/3    ---       ---        DOCKER   ['--storeType=zookeeper', '--storeConnection=zk-1.zk:2181', '--storeKey=/vamp/gateways/haproxy/1.6', '--logstash=elasticsearch.marathon.mesos:10001']\n/vamp/workflow-health      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-kibana      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-metrics     64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-vga         64   0.1    1/1    ---       ---        DOCKER   None\n\nVamp has now spun up all it's components and you should be able to access the ui by opening http://dcos.example.com/service/vamp/ in your browser.\n\nNow you're ready to follow our Vamp getting started tutorials.\nThings still not running? We're here to help →\n\n NB If you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\n Custom install\n\nThe Vamp DC/OS Docker image (github.com/magneticio - Vamp DC/OS) contains configuration (github.com/magneticio - Vamp DC/OS configuration) that can be overridden for specific needs by:\n\nMaking a new Docker image based on the Vamp DC/OS image\nUsing environment variables\n\nExample 1 - Remove the metrics and health workflows by Vamp configuration and keep the kibana workflow:\n\nvamp.lifter.artifact.resources = [\n    \"breeds/kibana.js\", \"workflows/kibana.yml\"\n  ]\n\nor doing the same using Marathon JSON\n\n\"env\":  \n\n Example 2 - Avoid automatic deployment of Vamp Gateway Agent\n\nRemove vga-marathon breed and workflow from vamp.lifter.artifact.files:\n\nvamp.lifter.artifact.files = []\n\nor using Marathon JSON\n\n\"env\":  \n\nPublic and private nodes\n\nRunning Vamp on public Mesos agent node(s) and disabling automatic Vamp Gateway Agent deployments (but keeping other default workflows) can be done with the following Marathon JSON:\n\n \n         \n      ],\n      \"forcePullImage\": true\n     \n   ,\n  \"labels\":  ,\n  \"env\":  ,\n  \"acceptedResourceRoles\": [\n    \"slave_public\"\n  ],\n  \"healthChecks\": [\n     \n  ]\n \n\nDeploying Vamp Gateway Agent on all public and private Mesos agent nodes through Marathon JSON - NB replace $INSTANCES (e.g. to be the same as total number of Mesos agent nodes) and optionally other parameters:\n\n \n   ,\n  \"args\": [\n    \"--storeType=zookeeper\",\n    \"--storeConnection=zk-1.zk:2181\",\n    \"--storeKey=/vamp/gateways/haproxy/1.6\",\n    \"--logstash=elasticsearch.marathon.mesos:10001\"\n  ],\n  \"constraints\": [\n    [\n      \"hostname\",\n      \"UNIQUE\"\n    ]\n  ],\n  \"acceptedResourceRoles\": [\n    \"slave_public\",\n    \"*\"\n  ]\n \n\n  \n\nOnce you have Vamp up and running you can follow our getting started tutorials.\nChcek the Vamp documentation\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n  \n",
    "id": 37
  },
  {
    "path": "/documentation/installation/docker",
    "date": "2016-09-30T12:00:00+00:00",
    "title": "Docker",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 70",
    "content": "\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing. Vamp can even run inside Docker while deploying to Docker.  You can pass native Docker options by using the Docker dialect in a Vamp blueprint.. Docker Swarm support is coming soon.\n\nSet Docker as the Vamp container driver\nInstall Docker as per Docker's installation manual (docs.docker.com - install Docker engine)\nCheck the DOCKER_* environment variables Vamp uses to connect to Docker, i.e.\n\n        DOCKER_HOST=tcp://192.168.99.100:2376\n    export DOCKERMACHINENAME=default\n    DOCKERTLSVERIFY=1\n    DOCKERCERTPATH=/Users/tim/.docker/machine/machines/default\n    \nIf Vamp can't find these environment variables, it falls back to using the unix:///var/run/docker.sock Unix socket for communicating with Docker.\nUpdate the container-driver section in the Vamp application.conf config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf\n\n        ...\n    container-driver  \n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\n  \nFind out how to configure Vamp\nCheck the configuration reference\nLook at some example configurations\nFollow the tutorials\n  ",
    "id": 38
  },
  {
    "path": "/documentation/installation/example-configurations",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Example configurations",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 120",
    "content": "\n\nreference.conf\nreference.conf is part of the Vamp code and should not be modified. It contains generic defaults for many parameters, but does not constitute a full Vamp configuration. Environment-specific settings need to be added in in application.conf or using environment variables and/or Java system properties.  \n\nThe full reference.conf file can be found in the Vamp project repo (github.com/magneticio - Vamp reference.conf).\n\n application.conf\nYou can use application.conf to tailor the Vamp configuration to fit your environment. Settings specified here, or in environment variables, will override any defaults included in reference.conf. Template configuration files are provided, which complete the standard required settings and include the Vamp Health and Metrics workflows (required by the Vamp UI).  If your environment requires extensive customisation, you can use a custom application.conf file.\n\nDC/OS application.conf  \n  Container driver: Marathon  \n  Key-value store: Zookeeper\n\n  \nKubernetes application.conf  \n  Container driver: Kubernetes  \n  Key-value store: etcd\n  \nRancher application.conf  \n  Container driver: Rancher  \n  Key-value store: consul\n\n  \nRead about how to configure Vamp\nCheck the configuration reference\nFollow the tutorials\nYou can read in depth about using Vamp or browse the API reference or CLI reference docs.\n  \n",
    "id": 39
  },
  {
    "path": "/documentation/installation/hello-world",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Hello world",
    "aliases": "aliases:",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 20",
    "content": "\nThe Vamp hello world setup will run Mesos, Marathon (mesosphere.github.io - Marathon) and Vamp 0.9.2 inside a local Docker container with Vamp's Marathon driver.  We will do this in three simple steps (although it's really just one docker run command). You can use the hello world setup to work through the getting started tutorials and try out some of Vamp's core features.\n\n  \nThis hello world set up is designed for demo purposes only - it is not production grade.\n  \n\nRequirements\nAt least 8GB of memory\n\n Get Docker\n\nPlease install one of the following for your platform/architecture\n\nDocker 1.9.x (Linux) or higher (Vamp works with Docker 1.12 too), OR\n[Docker Toolbox 1.12.x] (https://github.com/docker/toolbox/releases) if on Mac OS X 10.8+ or Windows 7+ \n\nVamp hello world on Docker for Mac or Windows is currently not supported. We're working on this so please check back. \n\nRun Vamp\n\nStart the magneticio/vamp-docker:0.9.2-marathon container, taking care to pass in the right parameters for your system: \n\n Mac OS X 10.8+ or Windows 7+\n\nIf you installed Docker Toolbox, please use the Docker Quickstart Terminal. We don't currently support Kitematic. A typical command on Mac OS X running Docker Toolbox would be:\ndocker run --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v docker-machine ssh default \"which docker\":/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=$(docker-machine ip default)\" \\\n           magneticio/vamp-docker:0.9.2\n\nLinux\n\ndocker run --privileged \\\n           --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v $(which docker):/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=$(hostname -I | awk ' ')\" \\\n           magneticio/vamp-docker:0.9.2\n\nMounting volumes is important. Read this great article about starting Docker containers from/within another Docker container.\n\n Check Vamp is up and running\n\nAfter some downloading and booting, your Docker log will show the Vamp has launched and report:  \n...Binding: 0.0.0.0:8080\n\nNow you can check if Vamp is home on http:// :8080/ and you're ready for the Vamp getting started tutorials\n\n  \nClick on the *?* in the top right corner of any screen for quick access to page help, related documentation and tutorials.\n\nAccess the exposed services\n\nAll the services exposed in this demo are listed below. Note that if you run on Docker machine you will need to switch localhost for docker-machine ip default.\n\nExposed services |  \n----------|-----,HAProxy statistics        |       http://localhost:1988 (username/password: haproxy)\nElasticsearch HTTP        |      http://localhost:9200\nKibana        |       http://localhost:5601\nSense        |      http://localhost:5601/app/sense\nMesos        |       http://localhost:5050\nMarathon       |      http://localhost:9090 (Note that the Marathon port is 9090 and not the default 8080)\nChronos        |       http://localhost:4400\nVamp UI       |      http://localhost:8080\n\n Summing up\n\nThis set up runs all of Vamp's components in one container. You will run into cpu, memory and storage issues pretty soon though. Also, random ports from 31000 - 32000 and 40000 - 45000 are assigned by Vamp which you might not have exposed on either Docker or your Docker Toolbox Vagrant box.  This is definitely not ideal, but works fine for kicking the tires.\nNow you're all set to follow our getting started tutorials.\n\n  \nFollow the getting started tutorials.\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)\n  ",
    "id": 40
  },
  {
    "path": "/documentation/installation/kubernetes",
    "date": "2016-10-04T09:00:00+00:00",
    "title": "Kubernetes",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 50",
    "content": "\nThe installation will run Vamp together with etcd, Elasticsearch and Logstash on Google container engine and kubernetes. (We will also deploy our demo Sava application to give you something to play around on). Before you begin, it is advisable to try out the official Quickstart for Google Container Engine tutorial first (google.com - container engine quickstart).  \n\n  \nKubernetes support is still in Alpha.\n  \n\nTested against\nThis guide has been tested on Kubernetes 1.4.x and 1.5. Minikube 0.13.x or higher can also be used. (github.com - minikube)\n\n Requirements\n\nGoogle Container Engine cluster or Minikube (0.13.1 or later)\nKey-value store (like ZooKeeper, Consul or etcd)\nElasticsearch and Logstash\nEnough (CPU and memory) resources on your K8s cluster to deploy the Vamp dependencies AND the containers at the scale you define. NB take a look into the available resources when a deployment keeps \"hanging\" to see if you actually have enough resources available.\nVamp currently only supports the 'default' namespace, so this should be available.\n\nCreate a new GKE cluster\n\nThe simple way to create a new GKE cluster:\n\nopen Google Cloud Shell\nset a zone, e.g. gcloud config set compute/zone europe-west1-b\ncreate a cluster vamp using default parameters: gcloud container clusters create vamp\n\nAfter the (new) Kubernetes cluster is setup, we are going to continue with the installation using the Kubernetes CLI kubectl.\nYou can use kubectl directly from the Google Cloud Shell, e.g. to check the Kubernetes client and server version:\n\nkubectl version\n Quickstart\n\nTo quickly get started with Vamp on Kubernetes use the following command to automate the quick start described below (requires curl):\n\ncurl -s \\\n  https://raw.githubusercontent.com/magneticio/vamp.io/master/static/res/vampkubequickstart.sh \\\n  | bash\nThe script will poll for the external ip of Vamp, note that this process will take a while. if the installation was successful the ip will be displayed:\n\n$ [OK] Quickstart finished, Vamp is running on http://104.xxx.xxx.xxx:8080\n\nWe don't recommend running this setup in production. You might want to add a HTTPS proxy in front of Vamp with at least basic authentication.\n\nTo remove the quickstart deployment, use the following command:\n\ncurl -s \\\n  https://raw.githubusercontent.com/magneticio/vamp.io/master/static/res/vampkubeuninstall.sh \\\n  | bash\n\nManual deployment\n\n Deploy etcd, Elasticsearch and Logstash\n\nNow let's deploy etcd - this installation is based on the tutorial (github.com/coreos - etcd on Kubernetes).  Note that this is not a production grade setup - you would also need to take care of persistence and running multiple replicas of each pod.\nFirst, execute:\n\nkubectl create -f https://raw.githubusercontent.com/magneticio/vamp.io/master/static/res/etcd.yml\n\nThen deploy Elasticsearch and Logstash with a proper Vamp Logstash configuration (github.com/magneticio - elastic) using:\n\nkubectl run elastic --image=magneticio/elastic:2.2\nkubectl expose deployment elastic --protocol=TCP --port=9200 --name=elasticsearch\nkubectl expose deployment elastic --protocol=UDP --port=10001 --name=logstash\nkubectl expose deployment elastic --protocol=TCP --port=5601 --name=kibana\n\nRun Vamp\n\nNow we can run Vamp gateway agent as a daemon set:\nkubectl create -f https://raw.githubusercontent.com/magneticio/vamp.io/master/static/res/vga.yml\n\nTo deploy Vamp, execute:\n\nkubectl run vamp --image=magneticio/vamp:0.9.1-kubernetes\nkubectl expose deployment vamp --protocol=TCP --port=8080 --name=vamp --type=\"LoadBalancer\"\n\nAs a reference, you can find the latest Vamp katana  Kubernetes configuration here: github.com/magneticio - Vamp Kubernetes configuration. Vamp katana includes all changes since the last official release, check the katana documentation for details.\n\nThe Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.\n\nWait a bit until Vamp is running and check out the Kubernetes services:\n\nkubectl get services\n\nThe output should be similar to this:\n\nNAME                 CLUSTER-IP     EXTERNAL-IP      PORT(S)             AGE\nelasticsearch        10.3.242.188   none           9200/TCP            4m\netcd-client          10.3.247.112   none           2379/TCP            4m\netcd0                10.3.251.13    none           2379/TCP,2380/TCP   4m\netcd1                10.3.251.103   none           2379/TCP,2380/TCP   4m\netcd2                10.3.250.20    none           2379/TCP,2380/TCP   4m\nkubernetes           10.3.240.1     none           443/TCP             5m\nlogstash             10.3.254.16    none           10001/UDP           4m\nvamp                 10.3.242.93    146.148.118.45   8080/TCP            2m\nvamp-gateway-agent   10.3.254.234   146.148.22.145   80/TCP              2m\n\nNotice that the Vamp UI is exposed (in this example) on http://146.148.118.45:8080\nOn Minikube you can use the handy minikube service vamp command to open the Vamp UI in your browser.\nIn the Kubernetes dashboard under 'Services' you will see the external endpoint appear with a hyperlink icon behind it when the Vamp UI service is available.\n\n Deploy the Sava demo application\n\n,name: sava:1.0\ngateways:\n  9050: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          port: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\nBe sure that the cluster has enough resources (CPU, memory), otherwise deployments will be in pending state. Once it's running we can check if all Vamp Gateway Agent services are up:\n\nkubectl get services --show-labels -l vamp=gateway\n\nWe can see that for each gateway a new service is created:\n\nNAME                      CLUSTER-IP     EXTERNAL-IP     PORT(S)     AGE       LABELS\nhex1f8c9a0157c9fe3335e9   10.3.243.199   104.155.24.47   9050/TCP    2m        lookup_name=a7ad6869e65e9c047f956cf7d1b4d01a89e\nef486,vamp=gateway\nhex26bb0695e9a85ec34b03   10.3.245.85    23.251.143.62   40000/TCP   2m        lookup_name=6ace45cb2c155e85bd0c84123d1dab5a6cb\n12c97,vamp=gateway\n\n  \nIn this setup Vamp is deliberately configured to initiate exposure of all gateway and VGA ports. This would not be the case if the default and recommended setting are used.\n  \n\nNow we can access our sava service on http://104.155.24.47:9050\n\nIn the Kubernetes dashboard you will see the external endpoint appear with a hyperlink icon behind it in the \"Services\" overview when the external gateway of your deployment is available. You can recognise it by the port that's included in the endpoint that you have defined in your blueprint gateway definition.\n\nThe default Kubernetes service type can be set in configuration: vamp.container-driver.kubernetes.service-type, possible values are LoadBalancer or NodePort. We can also access gateways using virtual hosts. Vamp Gateway Agent service is on IP 146.148.22.145 in this example, so:\ncurl --resolve 9050.sava-1-0.vamp:80:146.148.22.145 -v http://9050.sava-1-0.vamp\n\nDon't forget to clean up your Kubernetes cluster and firewall rules  if you don't want to use them anymore (google.com - container engine quickstart: clean up).\n\n  \n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n  \n",
    "id": 41
  },
  {
    "path": "/documentation/installation/mesos-marathon",
    "date": "2016-09-30T12:00:00+00:00",
    "title": "Mesos/Marathon",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 40",
    "content": "\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nInstall\nThe instructions included on the DC/OS installation page will also work with Mesos/Marathon.\n\n set Marathon as the Vamp container driver\n\nSet up a DCOS cluster using Mesosphere's assisted install on AWS (mesosphere.com - product).  \nIf you prefer, you can build your own Mesos/Marathon cluster. Here are some tutorials and scripts to help you get started:\n\n  Mesos Ansible playbook (github.com/mhamrah - ansible mesos playbook)\n  Mesos Vagrant (github.com/everpeace - vagrant-mesos)\n  Mesos Terraform (github.com/ContainerSolutions - Terraform Mesos)\n\nWhichever way you set up Marathon, in the end you should be able to see something like this:  \n\nMake a note of the Marathon endpoint (host:port) and update the container-driver section in the Vamp application.conf config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf. Set the \"url\" option to the Marathon endpoint.\n\n        ...\n    container-driver  \n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\n  \nFind out how to configure Vamp\nCheck the configuration reference\nLook at some example configurations\nFollow the tutorials\n  \n\n",
    "id": 42
  },
  {
    "path": "/documentation/installation/overview",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Installation",
    "aliases": "aliases:",
    "menu": "menu:",
    "  main": " main:",
    "    name": "   name: \"Overview\"",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 10",
    "content": "Before you get Vamp up and running on your architecture, it is helpful to understand how vamp works and the role of each component and its preferred location in a typical architecture.  \n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\nRequirements\n\nVamp requirements\n\n Install Vamp\nThe Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.\n\nDC/OS 1.7 and 1.8\nMesos/Marathon\nKubernetes 1.x\nRancher\nDocker\nAzure Container Service\n\nConfiguration\n\nHow to configure Vamp\nConfiguration reference\nExample configurations\n\n Try Vamp\n\nWe've put together a hello world walkthrough to let you try out some of Vamp's core features in a local docker container. You can use this to work through the getting started tutorials.\n\n",
    "id": 43
  },
  {
    "path": "/documentation/installation/rancher",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Rancher",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Installation\"",
    "    weight": "   weight: 60",
    "content": "This installation will run Vamp together with Consul, Elasticsearch and Logstash on Rancher. (We'll also deploy our demo Sava application to give you something to play around on). Before you begin, it is advisable to try out the official Rancher Quick Start Guide tutorial first (rancher.com - quick start guide).\n\n  \nRancher support is still in Alpha.\n  \n\nTested against\nThis guide has been tested on Rancher version 1.1.x.\n\n Requirements\n\nRancher up and running\nKey-value store like ZooKeeper, Consul or etcd\nElasticsearch and Logstash\nIf you want to make a setup on your local VM based Docker, it's advisable to increase default VM memory size from 1GB to 4GB.\n\nRun Rancher locally\nBased on the official Rancher quickstart tutorial, these are a few simple steps to run Rancher locally:\n$ docker run -d --restart=always -p 8080:8080 rancher/server\nThe Rancher UI is exposed on port 8080, so go to http://SERVER_IP:8080 - for instance http://192.168.99.100:8080, http://localhost:8080 or something similar depending on your Docker setup.\n\nFollow the instructions on the screen to add a new Rancher host. Click on \"Add Host\" and then on \"Save\". You should get instructions (bullet point 5) to run an agent Docker image:  \n\n$ docker run \\\n  -d --privileged \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /var/lib/rancher:/var/lib/rancher \\\n  rancher/agent:v1.0.1 \\\n  http://192.168.99.100:8080/v1/scripts/E78EF5848B989FD4DA77:1466265600000:SYqIvhPgzKLonp8r0erqgpsi7pQ\n\n Create a Vamp stack\nNext we need to create a Vamp stack. This can be done either from catalog or from scratch (adding all dependencies manually - Consul, Elasticsearch, Logstash).\n\nCreate a Vamp stack from catalog\n\nYou can create a Vamp stack directly from the Rancher catalog. Just go to Catalog, find the Vamp entry and click the Details button. Now you are ready to run Vamp\n\n Create a Vamp stack from scratch\n\nIf you'd rather create a Vamp stack from scratch, our custom Docker image magneticio/elastic:2.2 contains Elasticsearch, Logstash and Kibana with the proper Logstash configuration for Vamp. More details can be found on the github project page (github.com/magneticio - elastic).\n\nGo to Add Stack and create a new stack vamp (lowercase).\nInstall Consul:  \n  Use the vamp stack and go to Add Service:  \n    Name ⇒ consul\n    Select Image ⇒ gliderlabs/consul-server\n    Set Command ⇒ -server -bootstrap\n    Go to the Networking tab\n    Under Hostname select Set a specific hostname: and enter consul\n    Click the Create button\n\nInstall Elasticsearch and Logstash:\n  Use the vamp stack and go to Add Service:  \n    Name ⇒ elastic\n    Select Image ⇒ magneticio/elastic:2.2\n    Go to the Networking tab\n    Under Hostname select Set a specific hostname: and enter elastic\n    Click Create\n\nRun Vamp\n\nFirst we'll run the Vamp Gateway Agent:\n\nUse the vamp stack and go to Add Service:\nSet scale to Always run one instance of this container on every host\nName ⇒ vamp-gateway-agent\nSelect Image ⇒ magneticio/vamp-gateway-agent:0.9.2\nSet Command ⇒ --storeType=consul --storeConnection=consul:8500 --storeKey=/vamp/gateways/haproxy/1.6 --logstash=elastic:10001\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter vamp-gateway-agent\nClick Create \n\nNow let's find a Rancher API endpoint that can be accessed from running container:\n\nGo to the API page and find the endpoint, e.g. http://192.168.99.100:8080/v1/projects/1a5\nGo to the Infrastructure/Containers and find the IP address of rancher/server, e.g. 172.17.0.2\nThe Rancher API endpoint should be then http://IP_ADDRESS:PORT/PATH based on values we have, e.g. http://172.17.0.2:8080/v1/projects/1a5\n\nNow we can deploy Vamp:\n\nUse the vamp stack and go to Add Service:\nName ⇒ vamp\nSelect Image ⇒ magneticio/vamp:0.9.2-rancher\nGo to Add environment variable VAMPCONTAINERDRIVERRANCHERURL with value of Rancher API endpoint, e.g. http://172.17.0.2:8080/v1/projects/1a5\n(optional) add a VAMPCONTAINERDRIVERRANCHERUSER variable with a Rancher API access key if your Rancher installation has access control enabled\n(optional) add a VAMPCONTAINERDRIVERRANCHERPASSWORD variable with a matching Rancher API secret key.\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter vamp\nClick Create\nGo to Add Load Balancer (click arrow next to Add Service button label)\nChoose a name (e.g. vamp-lb)\nSource IP/Port ⇒ 9090\nDefault Target Port ⇒ 8080 and Target Service ⇒ vamp\n\nIf you go to http://SERVER_IP:9090 (e.g http://192.168.99.100:9090), you should get the Vamp UI.  You should also notice that Vamp Gateway Agent is running (one instance on each node) and you can see some Vamp workflows running.\nThe Vamp UI includes mixpanel integration. We monitor data on Vamp usage solely to inform our ongoing product development. Feel free to block this at your firewall, or contact us if you’d like further details.  \n\nTo access HAProxy stats, go to Add Load Balancer (click the arrow next to Add Service). Choose a name (e.g. vamp-gateway-agent-lb) and set:\n \nSource IP/Port ⇒ 1988\nDefault Target Port ⇒ 1988 \nTarget Service ⇒ vamp-gateway-agent\n\nYou can access the HAProxy stats page with username/password haproxy.\n\n Deploy the Sava demo application\n\nLet's deploy our sava demo application:\n\n,name: sava:1.0\ngateways:\n  9050: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          port: 8080/http\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n\nIf you want the gateway port to be exposed outside of the cluster via a Rancher Load Balancer:\n\nGo to Add Load Balancer (click arrow next to Add Service)\nChoose name (e.g. gateway-9050),\n  Source IP/Port ⇒ 9050\n  Default Target Port ⇒ 9050\n  Target Service ⇒ vamp-gateway-agent\n\n  \n\nOnce you have Vamp up and running you can jump into the getting started tutorials\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n  \n",
    "id": 44
  },
  {
    "path": "/documentation/release notes/katana",
    "date": "2016-10-19T09:00:00+00:00",
    "title": "Katana",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Release notes\"",
    "    name": "   name: Katana",
    "    weight": "   weight: 10",
    "content": "\n  \nAll changes since the last official release are described below. This applies only to binaries built from source (master branch). \n  \n\nWhat is new\nno-store pulse storage 869\nConfigurable service network #730\nUsing Vamp namespace in Docker labels #679\nKey-value store for (additional) Vamp configuration and option to update Vamp configuration at runtime #872\nMultiple gateway marshallers and option to update VGA (HAProxy) templates at runtime #870.\nQuerying events by type #878\nShowing Vamp logs in UI #863\nKubernetes namespace support #667\nArtifact metadata #890\nVamp as reverse proxy to Vamp gateways #884\n\nWhat has changed\nPulse storage type needs to be explicitly specified vamp.pulse.type: elasticsearch or no-store 869\nRemoved redundant HAProxy configuration: vamp.gateway-driver.haproxy.virtual-hosts, vamp.gateway-driver.haproxy.tcp-log-format and vamp.gateway-driver.haproxy.http-log-format #763.\nDebug API endpoints have been removed #308\nAPI endpoint to get HAProxy configuration and key-value key for VGA #870\nConfiguration change: vamp.workflow-driver.workflow.deployables.\"application/javascript\" has been changed to vamp.workflow-driver.workflow.deployables.application.javascript\nDefault workflow parameters are applied only on application/javascript workflows #880\n\n  \nRead all release notes on github (github.com/magneticio - Vamp releases)\nTry out Vamp with our single container hello world package.\n  \n",
    "id": 45
  },
  {
    "path": "/documentation/release notes/older-versions",
    "date": "2016-11-02T09:00:00+00:00",
    "title": "Older versions",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Release notes\"",
    "    name": "   name: \"Older versions\"",
    "    weight": "   weight: 1000",
    "content": "\nAll release notes about older Vamp versions can be found in the Github release notes.\n",
    "id": 46
  },
  {
    "path": "/documentation/release notes/version-0-9-0",
    "date": "2016-10-19T09:00:00+00:00",
    "title": "Version 0.9.0",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Release notes\"",
    "    name": "   name: Version 0.9.0",
    "    weight": "   weight: 999",
    "content": "\n9th September 2016\n\nThe Vamp 0.9.0 release is a very important milestone in the lifecycle of Vamp, as we're removing the Alpha label and are moving to Beta! This means that we will do our utmost best to avoid breaking changes in our API's and DSL, focus even more on stabilising and optimising the current feature-set, while of course continuously introducing powerful new features.\n\nThe Vamp 0.9.0 release is the culmination of three months of hard work by our amazing team! This release incorporates nothing less than 115 issues and I'm very proud of what we've achieved.\n\nSome of the most notable new features are:\n\na brand new opensource UI with much better realtime graphs, sparklines, info, events panel and access to all relevant API objects like breeds, deployments but also new options like gateways and workflows.  \npowerful integrated workflows for automation and optimisation like autoscaling, automated canary-releasing etc. using efficient Javascript-based scripting.\nKubernetes and Rancher support.\nsupport for custom virtual host names in gateways.\nsupport for custom HAProxy templates.\na brand new Vamp Runner helper application for automated integration testing, mocking scenarios and educational purposes.  \n\nAnd, of course, there's a massive amount of improvements, bug fixes and other optimisations.\n\ngithub.com/magneticio - complete list of all the closed issues in this release\n\n  \nRead all release notes on github (github.com/magneticio - Vamp releases)\nYou can try out Vamp with our single container hello world package.\n  \n",
    "id": 47
  },
  {
    "path": "/documentation/release notes/version-0-9-1",
    "date": "2016-10-01T09:00:00+00:00",
    "title": "Version 0.9.1",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Release notes\"",
    "    name": "   name: Version 0.9.1",
    "    weight": "   weight: 998",
    "content": "\n1st November 2016\n\nThe new stuff\nThis release of Vamp introduces:\n\nThe biggie: We've added Websockets support to our HTTP API. And we're now using this heavily in our new UI to improve responsiveness, smoothness and speed. https://github.com/magneticio/vamp/issues/529\nWe've updated our UI to a nice dark theme due to public demand, we love it as it's much easier on the eyes, and of course we're very interested in hearing your thoughts!\nWe've updated our charts with the amazing Smoothie Charts library for smooth running charts and sparklines.\nYou can now configure Vamp to use a key-value store for persistence data storage. By default nothing is defined, and you need to choose either ElasticSearch or key-value. Take a look at the Vamp Quickstart configuration for possible settings. The design reasons for this addition are having less dependencies on Elasticsearch, better re-use of the available key-value stores that come with cluster-managers (like Zookeeper in DC/OS or Etcd in Kubernetes) and more robustness (i.e. if we temporarily loose ES the persistence data is still available in the K/V store, only the metrics data is temporarily unavailable). Possible issues might be the performance of the key-value store after some time. This is a known issue being investigated. https://github.com/magneticio/vamp/issues/750\nGateway stickyness is now editable through the UI\nScales and gateways are seperate entities exposed through the UI\nMulti-select delete actions in the UI\nYou can now filter health and/or metrics events in the EVENTS stream panel\nAnd of course lots of other improvements and bug-fixes that can be found here: https://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.9.1+is%3Aclosed\n\n What has changed\nBREAKING CHANGE: In the Vamp configuration the “rest-api” section has changed to “http-api”. When running Vamp 0.9.1 you need to change this setting accordingly. NB REST and websockets are both a part of our HTTP API. Check this Vamp configuration example.\nBREAKING CHANGE: The default value for Vamp Gateway Agent storeKey has /vamp/gateways/haproxy/1.6 - changed from /vamp/haproxy/1.6\nIn the Vamp configuration we set persistence caching by default to false. In our pre-build Vamp images we set this to true to make it easier on the persistence store load. https://github.com/magneticio/vamp/issues/792\nWe've changed the updating deployment service states. https://github.com/magneticio/vamp/issues/797\n\nKnown issues\nHealth in deployment detail screen stays at 100% even when a gateway error is measured. In the gateway detail screen this works correctly. \nMemory leak issues when running workflows.\nInstability when using Zookeeper in the Vamp quickstart packages after a period of time.\nWhen using Elasticsearch for persistence storage there can be stale data due to speed of the websockets implementation and slower / eventual indexing of ES. A browser refresh solves this.\nVamp-docker quickstart throws error when starting up.\n\nThe full list of improvements and bug-fixes that can be found here: https://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.9.1+is%3Aclosed\n\n  \nRead all release notes on github (github.com/magneticio - Vamp releases)\nYou can try out Vamp with our single container hello world package.\n  \n",
    "id": 48
  },
  {
    "path": "/documentation/release notes/version-0-9-2",
    "date": "2016-12-22T09:00:00+00:00",
    "title": "Version 0.9.2",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Release notes\"",
    "    name": "   name: Version 0.9.2",
    "    weight": "   weight: 997",
    "aliases": "aliases:",
    "content": "\n22nd December 2016\n\nThis v0.9.2 release of Vamp has plenty of improvements in the areas of installation, robustness and stability. We also give the workflows feature some TLC so they're even easier to use from the UI. Check out our \"Automated canary release with rollback\" tutorial to get a feeling for the power of Vamp workflows.\n\nWhat is new\n789 Workflows can now be suspended without deleting them and there is an option to restart them.  \n813 Workflow execution period (successive executions) and execution timeout (max allowed execution time) can now be set for each workflow.   \n  More details: using workflows - artifacts\n834 Kubernetes bearer can be added as an optional configuration parameter.  \n  More details: configuration reference - container driver\n840 Time out can now be disabled for deployment operations.  \n  More details: configuration reference - operation\n846 Proper HTTPS support (client side).  \n  More details: configuration reference - common\n762 The default container type can now be configured.  \n  More details: configuration reference - model\n761 Support for Kubernetes driver with rkt runtime.\n862 Workflow environment variable values can be parametrised with workflow name (e.g. as $workflow or $ ).\nVamp UI 13 We've added a help panel to the Vamp UI - click on the ? in the top right corner.\nWebsite documentation is now versioned - where available, you can select content for specific Vamp versions.\n\nWhat has changed\n830 Custom event types - only alphanumerics, '_' and '-' are allowed in type names.   \n  More details: using events - basic event rules\n831 Logstash URL (host, port) can now be configured (instead of just host).  \n  More details: how to configure Vamp - extra configuration\n771 The API now returns a 500 response code if any of /info response parts return an error.  \n  More details: API reference - get runtime info\n845 Spported workflow deployable types can be explicitly mapped.  \n  More details: configuration reference - workflow-driver.workflow\n862 All default workflow environment variables need to be specified, e.g. VAMPURL and VAMPKEYVALUESTORE_PATH.  \n  More details: using workflows - artifacts.\n\nKnown issues\n  On some infrastructures we sometimes noticed time-outs on the UI side. Accessing the API directly works fine. \n\n  \nRead all release notes on github (github.com/magneticio - Vamp releases)\nYou can try out Vamp with our single container hello world package.\n  \n",
    "id": 49
  },
  {
    "path": "/documentation/troubleshoot",
    "date": "2017-01-09T09:00:00+00:00",
    "title": "Troubleshooting",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Documentation\"",
    "    name": "   name: \"Troubleshoot\"",
    "    weight": "   weight: 50",
    "content": "\nThe pointers in this article will help you troubleshoot and resolve common issues in case Vamp is not running or not behaving as expected.\n\nTroubleshoot a full Vamp installation\nTroubleshoot the Vamp hello world quickstart\nTroubleshooting tips for using Vamp\nReport an issue\n\nIf you find some of the instructions not clear enough or lacking information, please raise an issue on GitHub.\n\n------------------,\nTroubleshoot a full Vamp installation\nThe steps below will help you debug problems encountered when following our full install instructions. \n\nCheck the Vamp components\nCheck Elasticsearch\nCheck the key value store\nCheck the Vamp workflows\n\n Check the Vamp components\nCheck the step by step instructions for your container scheduler DC/OS, Mesos/Marathon, Kubernetes, Rancher or Docker. \nCheck the information panel in the Vamp UI (click the i-icon in the top right corner). If some fields are empty, you may need to adjust your configuration. See the example Vamp configurations.\nCheck you are running supported versions of all components.\nCheck the Vamp release notes for known issues and breaking changes.\nCheck that the vamp and vamp-gateway-agent docker containers are running using docker ps. \nCheck the vamp and vamp-gateway-agent logs docker logs    \n  Any errors here should be clear, in case they aren't report the issue.\nCheck the Vamp configuration (/api/v1/config): \n  You should use http:// in front of TCP/HTTP addresses, but not in front of UDP connections. \n  Make sure there are no double ports (ie 9200:9200)\n \nCheck Elasticsearch\nVamp uses Elasticsearch to store HAProxy logs and calculated metrics from its event system. The Health and Metrics indices are generated by Vamp workflows. To do this, they must have access to a valid Elasticsearch Logstash index and the Vamp API. \n  \n  Check the Elasticsearch indices required by Vamp have been created:  \n    GET elasticsearch url/_cat/indices?v  \n     HAProxy logs: logstash-      \n     Health workflow events: vamp-pulse-health-   \n     Metrics workflow events: vamp-pulse-metrics-    \n\nProblem |  Action\n----|-, no indices at all  |  Check Vamp Gateway Agent (or your custom HAProxy) is storing logs in the expected format (HAproxy 1.7.1 log format).\n no Logstash index  |  Check Logstash is correctly configured (gituhub.com/magneticio - Logstash example configuration).\n   |  Check Logstash can read the HAProxy logs.\n   |  Check Logstash can connect to Elasticsearch.\n no Health or Metrics indices  |  SSH into a Vamp Workflow Agent container and check you can telnet to the Elasticsearch url and the Vamp API set in your Vamp configuration (available from /api/v1/inf or /api/v1/config or open a workflow and check the environment variables). \n\n Check the key value store \n  Check the key value store reports as connected in Vamp info GET vamp url/api/v1/info  \n  If not, correct the configuration in application.conf\n  Check you can communicate with the configured key value store from where Vamp is running.  \n  Zookeeper:  \n  Consul:  \n  etcd:  \n\nCheck the Vamp workflows\nThe default Vamp workflows should be running continuously and not restarting.\n\n  Check that the three default workflow containers are there using docker ps  \n  Check the logs for each vamp-workflow-agent container using docker logs    \n    If a failure is reported, the workflow may not be able to talk to Elasticsearch - check the Elasticsearch configuration in application.conf.  \n    Any errors here should be clear, in case they aren't report the issue.\n\nIf you're still hitting problems, please report the issue.\n\nDid this help? If you find some of the instructions not clear enough or lacking information, please raise an issue on GitHub.\n\n------------------,\n Troubleshoot the Vamp Hello world quickstart\nThe Vamp hello world quickstart is a self contained testing package. If you run into problems or unexpected behaviour, we advise that you clear everything out and reinstall.\n\nStop all running containers - for example using docker ps | awk ' ' | xargs docker stop 2/dev/null\nClean up  your docker environment (remove stopped containers, dangling images and volumes). A script to do this: \n        \n        docker rmi -f $(docker images -q -f dangling=true) 2/dev/null\n        \n        echo \"removing exited docker containers...\"\n        docker ps -a | grep 'Exited' | awk ' ' | xargs docker rm\n        docker ps -a | grep 'Created' | awk ' ' | xargs docker rm\n        \n        echo \"removing dangling docker volumes...\"\n        docker volume rm $(docker volume ls -qf dangling=true) 2/dev/null `\nRestart docker machine using docker-machine restart.\nReinstall Vamp hello world.\nIf everything is installed ok and you're running into problems using Vamp, check the troubleshooting tips for using Vamp.\n\nDid this help? If you find some of the instructions not clear enough or lacking information, please raise an issue on GitHub.\n\n------------------,\nTroubleshooting tips for using Vamp\nIf you encounter problems running services on an installed version of Vamp, check the following: \n\nConfirm that everything is installed ok:\n  Check the Vamp info (using GET vamp url/api/v1/info or in the info pane of the Vamp UI).  \n    If a component is not listed or reported as not connected, check the instructions to troubleshoot a full Vamp installation or troubleshoot the Vamp Hello world quickstart.\nTime-out errors in the Vamp UI? Check that the WebSocket connections to the Vamp API are open and stable\nDeployments hanging on status \"deploying\"? Make sure that there are enough resources (CPU and memory) available in your container cluster and that your key-value store is accessible by the Vamp Gateway Agent.\nTake some time to read through the documentation and tutorials.\nAsk us on gitter (gitter.im - magneticio/vamp)\nStill not working? Report an issue.\n\nDid this help? If you find some of the instructions not clear enough or lacking information, please raise an issue on GitHub.\n\n------------------,\n Report an issue\n\nIf you've tried the above steps and are still stuck, then let us know. We accept bug reports and pull requests on the GitHub repo for each project (github.com - magneticio).\n\n                                    \nQuestions about how to use Vamp? Please check the documentation first.\nTo suggest a change or new feature, create a GitHub issue and tag it with \"feature proposal\"\n  \n\nWhen reporting issues, please include the following details:\n\nDescription of issue.\nVamp info, from the API: GET vamp url/api/v1/info\nVamp config, from the API; GET vamp url/api/v1/config\nReproduction steps.\nWhich orchestrator are you using, DC/OS, Kubernetes, Rancher, etc.\nLog output from Vamp and it's various components. You can get these from the docker logs command on the relevant containers.\nAny other information that you might consider important.\n",
    "id": 50
  },
  {
    "path": "/documentation/tutorials/automate-a-canary-release",
    "date": "2016-12-05T09:00:00+00:00",
    "title": "Automate a canary release with rollback",
    "type": "tutorial",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    name": "   name: \"Automate a canary release with rollback\"",
    "    weight": "   weight: 70",
    "content": "\nOne of the most powerful features of Vamp are workflows. Vamp workflows are containers with injected Node.js scripts that access the Vamp API for all kinds of automation and optimisation tasks. Vamp manages workflows just like any other container inside your cluster, making them robust, scalable and dynamic. With very little effort you can use workflows to create valuable and powerful automation playbooks for testing and deployments. \n\nThis tutorial will show how workflows can be used to run an automated canary release, gradually introducing an updated service and initiating a rollback to the old version in case 500 errors are measured on the new service. We will use our nifty little demo and automation tool Vamp Runner to demonstrate this, but you could just as easily perform all the described actions manually in the Vamp UI or using the API.  \n\nIn this tutorial we will:\n\nSpin up Vamp Runner  \nCreate a Blueprint and deploy some services \nCreate workflows to:\n  Generate traffic requests\n  Automate a canary release\n  Force a rollback when detecting errors\n\nRequirements\n\nA running version of Vamp 0.9.x (this tutorial has been tested on the Vamp hello world set up using Vamp 0.9.2)\nAccess to the Docker hub\nYou might run into issues if your firewall is set to block connections in the ranges 31000-32000 (required by Mesos) or 40000-45000 (required by Vamp)\n  \n Spin up Vamp Runner\nVamp Runner is the tool we use to demonstrate how individual Vamp features can be combined to fit real world use cases. This unlocks the real power of Vamp. We developed Vamp Runner as an automated integration testing tool to make sure important patterns of Vamp worked as expected against all supported container scheduling stacks when building new versions of Vamp. After we realised how powerful the concept of recipes was, we added a graphical UI on top for demonstration purposes. Vamp Runner can still be used in CLI mode though for your automated integration testing purposes. All actions triggered by Vamp Runner can also be triggered by your CI or automation tool of choice, check out the recipes folder in the Github project (github.com/magneticio - Vamp Runner recipes).\n\nOnce Vamp is up and running, you can deploy Vamp Runner alongside it (if you don’t already have a running version of Vamp, check the Vamp hello world set up ). Vamp Runner connects to the Vamp API endpoint, specified as VAMPRUNNERAPI_URL in the below docker run command. Note that the IP of your Vamp API location might be different, change this accordingly.\ndocker run --net=host -e VAMPRUNNERAPI_URL=http://192.168.99.100:8080/api/v1 magneticio/vamp-runner:0.9.2   \n\nYou can access the Vamp Runner UI at port 8088. Go ahead and click through the left menu:\n\nVamp shows details of the Vamp API Vamp Runner is talking to\nRecipes lets you walk through the available demos - we're going to use Canary Release - Introducing New Service Version\nRunner shows the configuration and log for Vamp Runner\n\n                                                   \n\nCreate a blueprint and deploy some services\nWe can use Vamp Runner to quickly create and deploy all the artifacts required for our demo. If you prefer, you could always create each of these manually yourself - the required YAMLs for all the recipes are available in the github repo (github.com/magneticio - Vamp Runner recipes).\n\nGo to Recipes and select Canary Release - Introducing New Service Version from the RECIPES list.\n  The steps required to complete the selected recipe are listed in the middle box\n  The clean up steps are listed on the right (we'll use Vamp Runner to clean up for us at the end)\n  The Vamp events stream is displayed at the bottom of the page. We can use this to track our canary release as it happens\n  \nClick Run next to the first recipe step Create blueprint (in the middle box)\n  Each recipe step must be performed in sequence\n  The info button next to each step shows you the exact YAML being posted to the VAMP API, in this case it shows us the blueprint that will be created\n  Once a step has completed successfully, the circle next to it will be coloured green. If for whatever reason the desired state cannot be reached the circle will colour red. (NB check the recipe JSON definition file for each recipe the GitHub project recipes folder to examine the states that are defined to check if a step has been executed successfully.) You can try cleaning the entire recipe by clicking the “Cleanup” button in the right column, or check the events at the bottom of the Vamp Runner UI and find out if there are any specific errors happening.\n\nWait for the Create blueprint step to complete and the circle to turn green, then work through the next four steps in turn:\n  Create breed and scale - these are the artifacts needed to deploy the service sava:1.0 and referenced in our placeholder blueprint\n  Deploy blueprint - deploys the application sava:1.0 with a routing weight of 100% (all traffic)\n  Create gateway - exposes the external gateway 9050 mapped to our Sava deployment\n  Introduce new service version - merges an updated version of our service sava:1.1 with the running deployment. It is added to the existing Sava cluster and the routing weight of the new version is set to the default amount of 0% (no traffic)\n\n The EVENTS stream in Vamp Runner will show the process of each step until our services are deployed. The created artifacts and deployments are visible in the Vamp UI (or via the API) and, if everything worked as expected, the deployed service can be accessed at the external gateway Vamp Runner created (9050).\n\n Create workflows\nWith two versions of our service ready to go, we can get started with some automation. We are going to demonstrate our automated canary release using two workflows; one to generate traffic requests (so we can see metrics and introduce 500 errors) and one to automate the canary release and rollback. For each of our workflows, Vamp Runner will first create a breed of type: application/javascript containing the Node.js JavaScript to run, and then create a workflow referencing this breed.\n\nWhen a workflow is created, Vamp will deploy a workflow agent container and inject the provided JavaScript into it (github.com/magneticio - Vamp workflow agent). The JavaScript will then run according to the schedule defined in the workflow (as a daemon, triggered by specific Vamp events or following a set time schedule). The Vamp Node.js client library inside the Vamp workflow container enables JavaScript workflows to speak easily to the Vamp API, see the gitHub project for details (github.com/magneticio - Vamp Node.js Client).  \nRead more about workflows\n\nGenerate traffic requests\nContinuing with our Vamp Runner recipe, the next step is to get some traffic requests flowing into the deployed service. We can generate these using a workflow.\n\nClick Run next to Generate traffic requests and Vamp will create a breed and a workflow named traffic. \n\nThe traffic workflow will send traffic requests to our service at the defined port (9050).  You can see the exact YAML posted to the Vamp API to complete this by clicking on the info button. The traffic workflow is set to run as a daemon, so it will begin generating traffic requests as soon as it is created. You will see these show up in the EVENTS stream at the bottom of the Vamp Runner UI, or you can watch them arrive at the gateway in the Vamp UI.\n\nOur services have been deployed with the routing weights sava:1.0 - 100% and sava:1.1 - 0%, this means that all incoming traffic is currently being routed to sava:1.0. \n\n Automate a canary release\nThe next step in our Vamp Runer recipe is to initiate an automated canary release and gradually introduce sava:1.1 to the world. \n\nClick Run next to Automated canary release and Vamp will create a breed and a workflow named canary. \n\nOnce created, the canary workflow will begin to gradually rebalance traffic routing, introducing sava:1.1 while phasing out sava:1.0. Click the info button in Vamp Runner to check the exact YAML used for this. You can track progress of the canary release in the EVENTS stream at the bottom of the Vamp Runner UI and you will also see the weight distribution on the internal gateway updating in the Vamp UI.\n \n\nYou can also use the WEIGHT slider in the Vamp UI to adjust the weight manually, the canary workflow will kick back in and take over from your setting.\n\nForce a rollback\nOur Sava service has traffic requests flowing in to the 9050 gateway (generated by the traffic workflow). This allows the canary workflow to check the responses and initiate a rollback in case errors are detected on the new service (the error limit is set in the workflow to 500). We can demonstrate this by maliciously destroying the sava:1.1 deployment from the Marathon UI so 500 errors will appear, effectively forcing a full rollback to sava:1.0.\n\nGo the Marathon UI (on port 9090) and find the sava:1.1 container runninginside the Sava group.\nSelect destroy to kill the container\n\nYou will see the following happen. First of all, the canary workflow will pick up on the errors for traffic routed to sava:1.1 and rebalance the routing to send 100% of traffic to sava:1.0. \n\nmarathon will redeploy the sava:1.1 service again as soon as possible and Vamp will make sure its routing rules are updated correctly to respect the new location(s) of the Sava 1.1 container in the cluster. Vamp's default health workflow will show a drop in health to 0% caused by the 500 errors. After 30 seconds of no errors the health will return to 100% and the canary workflow will start the canary release process over again.\n\n Autoscale the services\nThe final step in this Vamp Runner recipe deploys a workflow to automatically scale the services as their weights are rebalanced. Go ahead and run this step too - we will explain more about how Vamp manages autoscaling in a future tutorial.\n\nClear up and move along\nYou can set Vamp back to its initial (clean) state at any step in a recipe. Click the Clean up button on the right of the Recipes screen to remove all deployments, gateways, workflows and artifacts that have been created by the selected recipe.  The status of each step will also be reset and you can start from the beginning again.\n\n Summing up\n\nNow you’ve got to grips with Vamp Runner and the power of workflows there’s surely no stopping you. You can try out the other Vamp Runner recipes (remember to clean up when you’re done) like autoscaling and mocking metrics. \n\nLooking for more of a challenge?\nJust for fun, you could try these:\n\nHow might you integrate workflow automated canary releasing into a CI pipeline?\nCould you apply the mock metrics and/or canary workflows used in this recipe to another project?\nCan you adapt the canary workflow so it is triggered by a new deployment of the Sava service?\n\n  \nWhat would you like to see for our next tutorial? let us know\nFind our more about using Vamp\n  \n\n",
    "id": 51
  },
  {
    "path": "/documentation/tutorials/create-a-workflow",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Create a workflow that generates events",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    name": "   name: \"Create a workflow that generates events\"",
    "    weight": "   weight: 80",
    "content": "\nIn an earlier tutorial we showed how Vamp workflows can be used to automate all kinds of useful behaviour, for example to automate canary releasing with rollbacks, or autoscaling. In this tutorial, we will explain the Vamp workflow system in more detail and create a workflow that accesses the Vamp events system. We will first create and retrieve some custom events manually using the Vamp REST API, then create a simple workflow to automate these actions. Let's get started!\n\nRequirements\n\nA running version of Vamp 0.9.x (this tutorial has been tested on the Vamp hello world set up using Vamp 0.9.2)\n\n The Vamp events system\nVamp is a distributed system tied together by a central events stream. Every action in Vamp creates events, which in turn can be used as triggers for new actions. For example, gateway updates are triggered by deployments (synchronisation events), while canary releases and autoscaling actions are based on calculated health and metrics events.  Vamp events consist of an event type, one or more event tags and an optional event value. They are described in the format:  \n\n['tag', 'tag1:tag2'], value, event_type \n\nEvent types\nVamp works with a number of default event types and custom event types can also be created using the /events API endpoint (more on that later). If no event type is specified when creating an event, the generic event type event will be used. It is advisable to always specify an event type to allow for easy filtering.\n\nDefault event types |  Description\n----------|-----,Archive     |    Store/update of static Vamp artifacts (breeds, blueprints etc.)\nSynchronisation | Successful matching of a desired state with an observed state (for example, a successful update from 3 to 5 running instances)\nEvent | Generic event type, will be used if no event type is specified when creating an event\nHealth | Generated by the health workflow and used by the Vamp UI\nMetrics | Generated by the metrics workflow and used by the Vamp UI\nWorkflow | Generated by workflows used in Vamp Runner\n\n Event tags\nEach event created by Vamp is given one or more tags. Tags provide meta-information for stored events and are used for filtering searches and listening. Tags can be either a single tag or a combination of two tags separated by a :. The Vamp convention is to use the first tag as a generic label (for example a group) and the second tag as a specific label (i.e. a specific item in a group). generictag:specifictag\n\nEvent value\nThe event value is optional and could be anything you choose to store along with an event. Values are not analysed and can't be used for search. If no value is specified, it will be blank\n\n Storing events\nVamp events are stored by default in Elasticsearch using the integrated Vamp Pulse module. Elasticsearch indexing is based on the event type and is updated for each event received by the API. Each event type will be indexed individually, including custom event types  .  \nRead more about events\n\n  \nCreate events with the REST API\n\nThe /events endpoint of the Vamp API can be used to create and retrieve events. We are going to use this endpoint to create an event with the custom type helloapi. Note that custom event type names may only include alphanumerics, ‘’ and ‘-’.\n\nUse Postman or curl to POST the below JSON to the /api/v1/events endpoint.\n\n \nIf all runs to plan, you will receive a response from the API with the accepted JSON in the body. \n\n \n\nYou will notice that a timestamp has been added and the the event tag has been expanded. This means we will be able to search for this event using both the generic tag (tag1) and the combined generic:specific tag (tag1:tag2). If you check in the Vamp UI, you will see the created event show up in the EVENTS stream at the bottom of the screen (of course you need to open the events stream panel first :).\n\n Create events with a simple workflow\nThat's great - we stored our first event! Now let's take this a step further and create a workflow to automate the process.\n\nA bit about workflows\n\nVamp workflows are a convenient way of creating Node JS based scripts that run as containers and access the Vamp API. JavaScript workflows run in Vamp workflow agent containers (github.com/magneticio - Vamp workflow agent) and are managed just like any other container inside your cluster, making them robust, scalable and dynamic. You can schedule workflows to run as a daemon, be triggered by Vamp events or to run at specified times. You could also create your own workflows using your language of preference - create an application or script that accesses the Vamp API and build it into a docker container that can be deployed by Vamp.\n\nWe are going to create a workflow that runs as a daemon (i.e. all the time). We'll start by creating a breed with the required workflow-script and then deploy this as a Vamp workflow. Our workflow will simply store an event using the /events API endpoint, but you could also use Vamp workflows to aggregate and calculate metrics and send these out as events again, like the default “metrics” and “health” workflows and events that are used by the Vamp UI to create graphs. \n\n Create a breed \nThe breed will hold the JavaScript to be run for our workflow. It is always advisable to create a Vamp breed artifact containing the workflow script and then reference the breed from a workflow definition. Note that we have included the Vamp Node.js Client library, this will allow the Node.js application to easily interact with the Vamp API, see the gitHub project for more details (github.com/magneticio - Vamp Node.js Client).\n\n  Go the BREEDS tab in the Vamp UI\n  Click ADD\n  Paste in the below breed YAML and click SAVE\n\nname: hello_workflow\ndeployable:\n  type: application/javascript\n  definition: |\n    'use strict';\n\n    var vamp = require('vamp-node-client'); // required to interact with the Vamp API\n\n    var api = new vamp.Api(); // a new Vamp API object\n\n    // create a Vamp event - tag, value, event_type\n    api.event('tag1:tag2', 'optionalvalue', 'helloworkflow');   \n  \n\nDeploy the hello_workflow breed as a workflow\nAfter successfully saving the hello_workflow breed, we can create a workflow that references it. Once the workflow is created, the script from the breed will be deployed inside a Vamp workflow agent container. We will schedule our workflow to run as a daemon, so it will immediately start running and use environment variables to set the run interval (execution period) to 5 seconds.\n\nGo to the WORKFLOWS tab in the Vamp UI \nClick ADD (top right)\nPaste in the below workflow YAML and click SAVE\n\nname: hello_workflow\nbreed: hello_workflow\nenvironment_variables:\n  VAMPWORKFLOWEXECUTION_PERIOD: '5' \n  VAMPWORKFLOWEXECUTIONTIMEOUT: '4'   should be less than VAMPWORKFLOWEXECUTIONPERIOD\nschedule: daemon\nThe workflow will be deployed and you will see the created events appearing in the Vamp UI EVENTS stream. \n\nUpdate the running workflow\nUpdates made to a breed or workflow will not be directly carried over to the running workflow. To apply updates to a running workflow, it must be restarted. Let's update our running workflow so it creates an event every second.\n \nGo the WORKFLOWS tab in the Vamp UI\nOpen the helloworkflow workflow and set the VAMPWORKFLOWEXECUTIONPERIOD environment variable to 1 (this will cause the events to be generated every second)\nClick SAVE\nCheck the events stream in the Vamp UI \n  hello_workflow events will continue to appear every 5 seconds\nClick RESTART on the hello_workflow workflow \n  The workflow will be stopped and restarted, applying the changes we made. \nCheck the events stream again \n  hello_workflow will now be generating events every second - that's a lot of events! Feel free to click SUSPEND to temporarily stop the workflow running or delete it completely.\n \n\n Retrieve and filter events using the REST API\nNow we have a lot of events stored we can use the same REST API /events endpoint to retrieve them. We can search for specific events and filter for specific types and tags. You can try this out in Postman or curl by sending a GET request to the /api/v1/events endpoint, you can even do this in your browser. Search through stored events using ?type= or ?tags= (following the tag format generic or generic:specifc). Note that it is not possible to filter for specific values.  \nSee using the Vamp API for details on pagination.\n\nThe API can also be used to retrieve an events stream /api/v1/events/stream\n\nSumming up\nYou should now know a bit more about the Vamp events system and how it is used by the distributed components of Vamp, such as workflows. Now you understand how to build, deploy and update your own workflows there should be no stopping you - what will you automate?\n\n Looking for more of a challenge?\nJust for fun, you could try these:\n\nCan you create a workflow that responds to a specific Vamp event?\nThe Vamp Node.js Client library also allows you to create workflows that measure and count metrics. How might you incorporate that functionality into an automation workflow? See the gitHub project for details (github.com/magneticio - Vamp Node.js Client) or check the examples used in the Vamp runner recipes (github.com/magneticio - Vamp Runner recipes)\n\n  \nWhat would you like to see for our next tutorial? let us know\nFind our more about using Vamp\nRead more about the Vamp API\n  \n\n",
    "id": 52
  },
  {
    "path": "/documentation/tutorials/deploy-and-canary-release-wordpress",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deploy and canary release Wordpress",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    name": "   name: \"Deploy and canary release Wordpress\"",
    "    weight": "   weight: 60",
    "aliases": "aliases:",
    "content": "Ths tutorial will demonstrate how Vamp builds deployments from artifacts and works with gateways. We'll do this by deploying Wordpress together with mySQL using official images from the Docker hub and setting up a gateway to run a canary release. We are going to use the Vamp UI, but you could just as easily perform all the described actions using the Vamp API.  \n\nIn this tutorial we will:\n\nCreate a Wordpress blueprint and deploy it  \n  Create breeds to describe the mySQL and Wordpress deployables and their requirements\n  Create a scale to specify the resources to be assigned at runtime\n  Create a blueprint that combines breeds with scales \n  Deploy two instances of mySQL and Wordpress\nCanary release the deployments using gateways \n  Add stable endpoints to access the running Wordpress deployments\n  Control traffic distribution between the two deployments \n  Run a canary release\n\nRequirements\n\nA running version of Vamp (this tutorial has been tested on the Vamp hello world set up)\nAccess to the Docker hub\nYou might run into issues if your firewall is set to block connections in the ranges 31000-32000 (required by Mesos) or 40000-45000 (required by Vamp)\n  \n Create a Wordpress blueprint and deploy it\nDeployments to be initiated by Vamp are described in blueprints using the Vamp DSL (Domain Specific Language). A Vamp blueprint combines breed and scale artifacts to make scalable services, then groups these services into clusters for traffic distribution. \nA blueprint can reference individually stored artifacts, or everything can be described inline in the blueprint. We are going to create individual breed and scale artifacts, then reference these from our blueprint.\n\nCreate breeds\nServices to be deployed by Vamp must first be described in a breed. Breeds define the deployable to run and the internal gateway(s) to create for a single service. Settings can be specified in a breed and passed to the service at runtime, this is done using environment variables.  \nRead more about environment variables...\n\nFor our deployment, we will need to create two breeds - one for the mySQL database and one for Wordpress. We can use environment variables to specify the login credentials for the database and add some Vamp magic to tell Wordpress where it can find the as-yet-undeployed mySQL database. The Vamp variable host will resolve to the host or IP address of a referenced service at runtime. Finally, we can declare the mySQL database as a dependency for the Wordpress service so Vamp will wait until the database is available before it deploys Wordpress and all environment variables can be resolved.\n\nThe mySQL breed  \nWe will use mySQL as the Wordpress database, but you could try another database if you prefer. A quick check of the official documentation on the Docker hub tells us that the official mySQL container (hub.docker.com - mySQL) exposes the standard mySQL port (3306). We can create a database and set up some user accounts with the variables MYSQLDATABASE, MYSQLROOTPASSWORD, MYSQLUSER and MYSQL_PASSWORD. That's all the details we need to create a breed for the mySQL service. \n\n  Go the BREEDS tab in the Vamp UI\n  Click ADD\n  Paste in the below breed YAML and click SAVE\n\nname: mysql                     name of our breed\ndeployable: mysql:5.6          # publicly available Docker image\nports:\n  mysql_port: 3306/tcp         # internal gateway - the default mySQL port (tcp) \nenvironment_variables:         # required settings\n  MYSQLROOTPASSWORD: \"root_password\"\n  MYSQL_DATABASE: \"wordpress\"\n  MYSQLUSER: \"wordpressuser\"\n  MYSQLPASSWORD: \"wordpresspassword\"\n\nThe Wordpress breed  \nNow let's do the same for our Wordpress service. The official Wordpress container (hub.docker.com - wordpress) runs apache on port 80. We can specify an external database using the variables WORDPRESSDBHOST, WORDPRESSDBUSER and WORDPRESSDBPASSWORD. \n\n  Go the BREEDS tab in the Vamp UI\n  Click ADD\n  Paste in the below breed YAML and click SAVE\n\nname: wp:1.0.0                          # name of our Wordpress service\ndeployable: wordpress:4.6-php7.0-apache # publicly available Docker image        \nports:\n  webport: 80/http                      # internal gateway - the default apache port\nenvironment_variables:         \n  WORDPRESSDBHOST: $db.host:$db.ports.mysql_port\n  WORDPRESSDBUSER: \"wordpress_user\"\n  WORDPRESSDBPASSWORD: \"wordpress_password\"\ndependencies:                           # required for the Wordpress service to run\n  db: mysql                             # the mysql service from the db cluster\n\nNothing has been deployed yet, but you will be able to see the two new breeds listed in the Vamp UI under the BREEDS tab:\n\nCreate a scale\nScales define the resources to assign to a service, that is the number of instances (servers) and allocated CPU and memory. We are going to create one scale, which will be used by both of our breeds.\n\n  Go the SCALES tab in the Vamp UI\n  Click ADD\n  Paste in the below scale YAML and click SAVE\n\nname: demo\ninstances: 1\ncpu: 0.2\nmemory: 380MB  \n Create a blueprint\nNow we can create a simple blueprint that references our artifacts ready for deployment. A Vamp blueprint creates scalable services by joining togehter breed descriptions and a scale. Services are grouped into clusters to allow easy distribution of incoming traffic - more on this later. \n\nOur blueprint will use two clusters - one for the database service and one for the Wordpress service.\n\nGo to the BLUEPRINTS tab in the Vamp UI \nClick ADD (top right)\nPaste in the below blueprint YAML and click SAVE\n\nname: wp_demo            # name of our blueprint\nclusters:\n  db:                    # name of our database cluster\n    services:\n      breed: mysql       # the mySQL breed we created\n      scale: demo        # the scale we created\n  wp:                    # name of our Wordpress cluster\n    services:\n      breed: wp:1.0.0    # the Wordpress breed we created\n      scale: demo        # the scale we created\n\nVamp will store the blueprint and make it available for deployment. You will see it listed under the BLUEPRINTS tab.\n\nDeploy the blueprint\n\nFrom the BLUEPRINTS tab, click DEPLOY AS on the wp_demo blueprint\nYou will be prompted to give your deployment a name, let’s call it wpdemo1\nClick DEPLOY to initiate the deployment\n\nVamp will now run the deployment. This might take some time, especially if the images need to be pulled from the Docker hub. You can track the status of the deployed clusters and their services under the DEPLOYMENTS tab by opening the wpdemo1 deployment. Successfully completed deployments show the service name highlighted in grey (on the left), the IP of the deployed instance(s) and any environment variables passed to the service. Note how Vamp won't start to deploy Wordpress until mySQL has fully deployed.\n\n Deploy the blueprint again\nWe can now quickly deploy a second instance of Wordpress and mySQL using our existing artifacts. Vamp breeds, scales and blueprints can be re-used to initiate as many deployments as you like. Each deployment will be treated as an individual entity and Vamp will manage all the internal gateways and routing, so you don't need to worry about creating conflicts with other running services or deployments - more on this later. \n\nGo to the BLUEPRINTS tab \nClick DEPLOY AS on the wp_demo blueprint\nName the deployment wpdemo2\nClick DEPLOY to initiate the deployment\n\nWe now have two separate Wordpress deployments running, they should both be listed under the DEPLOYMENTS tab:\n\nCanary release the deployments using gateways\nVamp exposes internal and external gateways to allow access to clusters of services. Internal gateways are automatically created dynamic endpoints, external gateways are declared stable endpoints. Weights and conditions can be applied to gateways to control the traffic distribution across multiple potential routes. For example, internal gateways can control traffic distribution across the services deployed in a cluster, whereas external gateways might control traffic distribution across routes not managed by Vamp.  \nRead more about gateway usage\n\nTo get back to our demonstration, we currently have two separate deployments running. In each of our deployments, Wordpress is connected to its own instance of mySQL via a mysqlport internal gateway. At deployment time, Vamp automatically creates new internal gateways for all the ports defined in the breed(s) deployed. This means that we have exposed a mysqlport and a webport internal gateway for each of our deployments. You can see all currently exposed gateways listed under the GATEWAYS tab, they are labelled in the format deployment/cluster/port. \n\n Add stable endpoints\nWe could use the internal gateway ports to access our deployments if we wanted (go ahead and connect to the assigned deployment/wp/webport ports, you should see your running Wordpress services). The ports assigned to internal gateways are, however, unpredictable, so it makes much more sense to declare external gateways and access the services through stable endpoints. \n\nWe can quickly add a new stable endpoint that maps to one of our existing webport internal gateways.\n\nGo to the GATEWAYS tab in the Vamp UI\nClick ADD (top right)\nPaste in the below gateway YAML and click SAVE\n\n,name: wpdemo1_gateway   # name of our gateway\nport: 9050/http           # external gateway to expose\nroutes:\n  wpdemo1/wp/webport:   # the internal gateway exposed by the Wordpress breed\n    weight: 100%          # all traffic will be sent here\n\nThe gateway will be exposed and you can directly - and forever - access your Wordpress service at the specified 9050 port. \n\nHello Wordpress! Go ahead and finish the installation - I hear it's very quick - then you can check out your new site.\n\nLet's do that again\nAs we are running two deployments, we also need two external gateways - one for each instance of Wordpress. Let's add a second external gateway to give our other Wordpress service a stable endpoint, this time we'll need to expose a different port.\n\nGo to the GATEWAYS tab again and click ADD (top right)\nPaste in the below gateway YAML and click SAVE\n\n,name: wpdemo2_gateway    name of our gateway\nport: 9060/http           # external gateway to expose\nroutes:\n  wpdemo2/wp/webport:   # the internal gateway exposed by the Wordpress breed\n    weight: 100%          # all traffic will be sent here\nThe new gateway will be listed under the GATEWAYS tab and you can access your second Wordpress site on the newly exposed 9060 port (the old Wordpress site will still be available at port 9050). Complete this second Wordpress installation and make some changes to the site - select a different theme or add some content to help you easily tell the two deployments apart.  \n\nUse a gateway to distribute incoming traffic\nWell that was fun, but Vamp can do so much more! We can use our deployments to demonstrate some of Vamp's traffic distribution features and show how these can be used to run a canary release.\n\nVamp distributes traffic between services at shared gateways. If our Wordpress services had been running in the same deployment and cluster with a shared internal gateway, we could have used the shared internal gateway's WEIGHT slider to distribute traffic between them. As we are working with two separate deployments, we need to create a shared external gateway first. This is much the same as exposing a simple gateway, only this time we will add two routes and use the weight setting to split traffic between them. You can add as many routes as you like to a gateway, just remember that the total weight must always add up to 100%. \n\nGo to the GATEWAYS tab in the Vamp UI\nClick ADD (top right)\nPaste in the below gateway YAML and click SAVE\n\n,name: wordpressdistributiongateway\nport: 9070/http\nroutes:\n  wpdemo1_gateway:\n    weight: 50%      send 50% of all traffic to this route\n  wpdemo2_gateway:\n    weight: 50%     # send 50% of all traffic to this route\n\nThe gateway wil be created. Go to port 9070 and see which Wordpress install you are sent to. \n\n  \nYou might notice that the URL in your browser switched to either :9060 or :9050 and that subsequent visits to the 9070 endpoint consistently send you back to the same port/Wordpress. So what's going on with our gateway? Visits to 9070 should be split 50% / 50% between our two Wordpress instances! Here's where things get sticky, but that's not down to Vamp.  \n\nOn your first visit via 9070, Wordpress picked up that you had been redirected and sent out the HTTP response 301, which means \"permanently moved\"\nYour browser cached this 301 redirect and is now automatically sending every subsequent attempt to visit 9070 to the same destination \n  \nSimply disabling the cache won't help, but you can get around this by adding, for example, a ? to the end of the URL (e.g. 192.168.99.100:9070?). If you prefer a more eloquent solution, you could install a Wordpress plugin to prevent the 301 from happening in the first place (wordpress.org - Permalink Fix & Disable Canonical Redirects will do the trick), just remember to install it on both of your Wordpress deployments.\n  \n\nOnce you do make it back to the Vamp external gateway at 9070 you will see that everything is working as expected and you will be sent to 9050 or 9060 alternately. Thanks Wordpress.\n\nRun a canary release  \nNow we have an external gateway set up with two routes, we can use the WEIGHT slider in the GATEWAYS section of the Vamp UI to adjust the distribution of traffic between them. This will allow us to canary release our new Wordpress site to users, introducing it slowly and quickly switching back if it doesn't performing as planned. Conditions can also be added to each route in the gateway, so we could target specific user groups to send to each version.  \nRead more about using conditions\n \n\n Summing up\nYou should now understand a bit more about how Vamp builds deployments from the various artifacts, resolves variables and handles internal routing. Obviously, this is not a production grade setup. The database is running in a containerised mySQL instance with no data persistence. If you're running in the Vamp hello world setup, you're also likely to hit resource problems pretty quickly. If things aren't working as expected you can try increasing the memory of Docker VirtualBox VM (3GB should be enough for this demo).\n\nLooking for more of a challenge?\nJust for fun, you could try these:\n\nCan you rewrite the Wordpress and mySQL deployment as a single blueprint (define artifacts inline)?\nCan you run this deployment using the Vamp API?\n  There are full instructions in deploy your first blueprint - using the API or you can check the API reference \nTry adding a condition to the 9070 external gateway - for example, to send all Firefox users to one version of your site and everyone else to the other\n  Read about using conditions\nCould you set up external gateways for the mySQL deployments and connect Wordpress to the mySQL server running in the other deployment?\n\n  \nWhat would you like to see for our next tutorial? let us know\nFind our more about using Vamp\n  \n\n",
    "id": 53
  },
  {
    "path": "/documentation/tutorials/deploy-your-first-blueprint",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deploy your first blueprint",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    weight": "   weight: 20",
    "content": "If everything went to plan, you should have your Vamp installation up and running. If not, please follow the Vamp hello world quick setup steps. Now we're ready to check out some of Vamp's features. In this tutorial we will:  \n\nDeploy a monolith, using either the Vamp UI or the Vamp API\nCheck out the deployed application  \nGet some metrics on the running application  \nChange the scale and load-balancing\nChaos monkey!    \n\nDeploy a monolith\n\nImagine you or the company you work for still use monolithic applications. I know, it sounds far fetched...\nThis application is conveniently called Sava monolith and is at version 1.0.  \n\nYou've managed to wrap your monolith in a Docker container, which lives in the Docker hub under magneticio/sava:1.0.0. Your app normally runs on port 8080 but you want to expose it under port 9050 in this case. Let's deploy this through Vamp using the following simple blueprint. Don't worry too much about what means what: we'll get there. You can choose to deploy this blueprint either using the Vamp UI or using the Vamp API.\n\n,name: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\n Deploy using the Vamp UI\n\nIn the Vamp UI, go to the BLUEPRINTS tab and click the ADD button (top right). Paste in the above blueprint and press the SAVE button. Vamp will store the blueprint and make it available for deployment. \nNow Press DEPLOY AS. You'll be prompted to give your deployment a name, let's call it sava, then press DEPLOY to start the deployment and you can skip to Check out the deployed application.\n\nDeploy using the Vamp API\n\nYou can use your favourite tools like Postman (getpostman.com), HTTPie (github.com/jakubroztocil - httpie) or Curl to post this blueprint directly to the api/v1/deployments endpoint of Vamp.\n\nTake care to set the correct Content-Type: application/x-yaml header on the POST request. Vamp is kinda\nstrict with regard to content types, because we support JSON and YAML so we need to know what you are sending.   \nIf you run on Docker machine, use docker-machine ip default instead of localhost.\n\nUsing curl\ncurl -v -X POST --data-binary @sava_1.0.yaml -H \"Content-Type: application/x-yaml\" http://localhost:8080/api/v1/deployments\n\nUsing httpie\nhttp POST http://localhost:8080/api/v1/deployments Content-Type:application/x-yaml < sava_1.0.yaml\n\nAfter POST-ing, Vamp should respond with a 202 Accepted message and return a JSON blob. This means Vamp is trying to deploy your container. You'll notice some parts are filled in for you, like a default scale, a default routing and of course a UUID as a name.\nYou can also use the RESTful API to create a deployment with a custom name - simple PUT request to http://localhost:8080/api/v1/deployments/DEPLOYMENTCUSTOMNAME\n\n Check out the deployed application \n\nYou can follow the deployment process of our container by checking the /api/v1/deployments endpoint and checking when the state field changes from ReadyForDeployment to Deployed. You can also check Marathon's GUI.\n\nWhen the application is fully deployed you can check it out at Vamp host address + the port that was assigned in the blueprint, e.g: http://10.26.184.254:9050/. It should report a refreshing hipster lorem ipsum (hipsterjesus.com) upon each reload.  \n\nSee check Vamp is up and running for a full ist of all services exposed in the hello world setup.\n\nGet some metrics on the running application\n\nWe can use a simple tool like Apache Bench (apache - ab) to put some load on our application and see some of the metrics flowing into the dashboard. Use the following command to send 10000 requests using 15 threads to our Sava app.\n\nab -k -c 15 -n 10000 http://localhost:9050/\nor\nab -k -c 15 -n 10000 http://docker-machine ip default:9050/\n\nYou should see the metrics spike and some pretty charts being drawn:\n\n Change scale and load-balancing\n\nVamp will automatically load-balance services. Let's change the scale of the service. Go to the DEPLOYMENTS tab and open the sava deployment. Click the edit icon under SCALE and enter 3** in the **instances field. click SAVE and Vamp will automatically scale up the number of running instances (of course permitting underlying resources) and load-balance these to the outside world using the gateway feature.\n\nChaos monkey\n\nNow let's try something fun. Go to the Marathon UI (on port 9090) and find the Sava container running. Now select destroy to kill the container. Watch Vamp detecting that issue and making sure that the defined number of instances is spun up again as soon as possible, while making sure the loadbalancing routing rules are also updated to reflect the changed IPs and ports of the instances.\n\n  \nLet's run a canary release in the second part of this getting started tutorial →\n  \n\n",
    "id": 54
  },
  {
    "path": "/documentation/tutorials/merge-and-delete",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Merge and delete",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    weight": "   weight: 50",
    "content": "\nIn the previous tutorial we \"over-engineered\" our service based solution a bit (on purpose of course). We don't really need two backends services, so in this tutorial we will introduce our newly engineered solution and transition to it using Vamp blueprints and canary releasing methods. In this tutorial we will:\n\nGet some background and theory on merging services\nPrepare our blueprint\nTransition from blueprints to deployments (and back)\nDelete parts of the deployment\nAnswer the all important question when would I use this?\n\nSome background and theory\n\nWhat we are going to do is create a new blueprint that is completely valid by itself and merge it\nwith our already running deployment. This might sound strange at first, but it makes sense. Why? Merging will enable us to slowly move from the previous solution to the next solution. Once moved over, we can\nremove any parts we no longer need, i.e. the former \"over-engineered\" topology.\n\nIn the diagram above, this is visualized as follows:\n\nWe have a running deployment (the blue circle with the \"1\"). To this we introduce a new blueprint\nwhich is merged with the running deployment (the pink circle with the \"2\").\nAt a point, both are active as we are transitioning from blue to pink.\nOnce we are fully on pink, we actively remove/decommission the blue part.\n\nIs this the same as a blue/green release? Yes, but we like pink better ;o)\n\n Prepare our blueprint\n\nThe below blueprint describes our more reasonable service topology. Again, this blueprint is completely\nvalid by itself. You could just deploy it somewhere separately and not merge it with our over-engineered\ntopology. Notice the following:\n\nThe blueprint only has one backend cluster with one service.\nThe blueprint does not specify a gateway using the gateways key because we are going to use the gateway already present and configured in the running deployment. However, it would be perfectly correct to specify the old gateway - the gateway would be updated as well.\n\n,name: sava:1.3\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava-frontend:1.3.0\n        deployable: magneticio/sava-frontend:1.3.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.webport/api/message\n        dependencies:\n          backend: sava-backend:1.3.0\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n  backend:\n    services:\n      breed:\n        name: sava-backend:1.3.0\n        deployable: magneticio/sava-backend:1.3.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n\nUpdating our deployment using the UI or a PUT to the /api/v1/deployments/  should yield a deployment with the following properties (we left some irrelevant\nparts out):\n\nTwo services in the sava cluster: the old one at 100% and the new one at 0% weight.\nThree backends in the cluster list: two old ones and one new one.\n\nSo what happened here? Vamp has worked out what parts were already there and what parts should be merged or added. This is done based on naming, i.e. the sava cluster already existed, so Vamp added a service to it at 0% weight. A cluster named \"backend\" didn't exist yet, so it was created. Effectively, we have merged\nthe running deployment with a new blueprint.\n\nTransition from blueprints to deployments and back\n\nMoving from the old to the new topology is now just a question of \"turning the weight dial\". You\ncould do this in one go, or slowly adjust it. The easiest and neatest way is to just update the deployment as you go.\n\nVamp's API has a convenient option for this: you can export any deployment as a blueprint! By appending ?as_blueprint=true to any deployment URI, Vamp strips all runtime info and outputs a perfectly valid blueprint of that specific deployment.\n\nThe default output will be in JSON format, but you can also get a YAML format. Just set the header Accept: application/x-yaml and Vamp will give you a YAML format blueprint of that deployment.\n\nWhen using the graphical UI, this is all taken care of.\n\nIn this specific example, we could export the deployment as a blueprint and update the weight to a 50% to\n50% split. Then we could do this again, but with a 80% to 20% split and so on. See the abbreviated example\nbelow where we set the weight keys to 50% in both routing sections.\n\n,name: eb2d505e-f5cf-4aed-b4ae-326a8ca54577\nclusters:\n  sava:\n    services:\n    breed:\n        name: sava-frontend:1.2.0\n        deployable: magneticio/sava-frontend:1.2.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND_1: http://$backend1.host:$backend1.ports.webport/api/message\n          BACKEND_2: http://$backend2.host:$backend2.ports.webport/api/message\n        constants: { \n        dependencies:\n          backend1: sava-backend1:1.2.0\n          backend2: sava-backend2:1.2.0\n      environment_variables: { \n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n      dialects: { \n    breed:\n        name: sava-frontend:1.3.0\n        deployable: magneticio/sava-frontend:1.3.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.webport/api/message\n        constants: { \n        dependencies:\n          backend: sava-backend:1.3.0\n      environment_variables: { \n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n      dialects: { \n    gateways:\n      port:\n        sticky: none\n        routes:\n          sava-frontend:1.2.0:\n            weight: 50%\n          sava-frontend:1.3.0:\n            weight: 50%\n\n Delete parts of the deployment\n\nVamp helps you transition between states and avoid \"hard\" switches, so deleting parts of a deployment is somewhat different than you might expect.\n\nIn essence, a delete is just another update of the deployment: you specify what you want to remove using a blueprint and send it to the deployment's URI using the DELETEHTTP verb: yes, it is HTTP Delete with a body, not just a URI and some id.\n\nThis means you can specifically target parts of your deployment to be removed instead of deleting the whole thing. For this tutorial we are going to delete the \"over-engineered\" old part of our deployment.\n\nCurrently, deleting works in two steps:\nSet all routings to weight: 0% of the services you want to delete with a simple update.\nExecute the delete.\n\n  \nYou need to explicitly set the routing weight of the service you want to deploy to zero before deleting. Here is why: When you have, for example, four active services divided in a 25/25/20/30 split and you delete the one with 30%, Vamp doesn't know how you want to redistribute the \"left over\" 30% of traffic. For this reason the user should first explicitly divide this and then perform the delete.\n  \n\nSetting to zero\n\nWhen you grab the YAML version of the deployment, just like above, you can set all the weight entries for the Sava 1.2.0 versions to 0 and update the deployment as usual. See the cleaned up example and make sure to adjust the name to your specific situation.\n\n,name: 125fd95c-a756-4635-8e1a-361085037870\nclusters:\n  backend1:\n    services:\n    breed:\n        ref: sava-backend1:1.2.0\n    gateways:\n      routes:\n        sava-backend1:1.2.0:\n          weight: 0%\n  backend2:\n    services:\n    breed:\n        ref: sava-backend2:1.2.0\n    gateways:\n      routes:\n        sava-backend2:1.2.0:\n          weight: 0%\n  sava:\n    services:\n    breed:\n        ref: sava-frontend:1.3.0\n\n    breed:\n        ref: sava-frontend:1.2.0\n    gateways:\n      routes:\n        sava-frontend:1.3.0:\n          weight: 100%\n        sava-frontend:1.2.0:\n          weight: 0%\n\nDoing the delete\n\nNow, you can take the exact same YAML blueprint or use one that's a bit cleaned up for clarity and send it in the body of the DELETE to the deployment resource, e.g. /api/v1/deployments/sava.\n\nUsing the Vamp UI you can delete parts of your deployment by using the REMOVE FROM function under the BLUEPRINTS tab.\n\n,name: sava:1.2\nclusters:\n  sava:\n    services:\n      breed:\n        ref: sava-frontend:1.2.0\n  backend1:\n    services:\n      breed:\n        ref: sava-backend1:1.2.0\n  backend2:\n    services:\n      breed:\n        ref: sava-backend2:1.2.0\n\nWe removed the deployable, environment_variables, ports and some other parts of the blueprint. These are actually not necessary for updating or deletion. Besides that, this is actually exactly the same blueprint we used to initially deploy the \"old\" topology.\n\nYou can check the result in the UI: you should be left with just one backend and one frontend:\n\nWhen would I use this?\n\nSounds cool, but when would I use this in practice? Well, basically anytime you release something new!\nFor example a bugfix release for a mobile API that \"didn't change anything significantly\"? You could test\nthis separately and describe it in its own blueprint. After testing, you would merge that exact same blueprint\nwith your already running production version (the one without the bugfix) and slowly move over to new version.\n\nNew major release of your customer facing app? You probably also have some new dependencies that come with that\nrelease. You create some containers and write up a blueprint that describes this new situation, run it in acceptance and test and what have you. Later, you merge it into your production setup, effectively putting it next to it and then slowly move from the old situation to the new situation, including dependencies.\n\n  \nThis is the end of this initial getting started tutorial. We haven't done anything with Vamp's SLA's yet, scaling or dictionary system, so there is much more to come!\nVamp use cases\nFind out how to install a production-grade set up of Vamp\n  \n\n",
    "id": 55
  },
  {
    "path": "/documentation/tutorials/overview",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Tutorials",
    "aliases": "aliases:",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    name": "   name: \"Overview\"",
    "    identifier": "   identifier: \"tutorial-overview\"",
    "    weight": "   weight: 10",
    "content": "\nGetting started\nWe’ve created a set of showcase applications, services and corresponding blueprints that demonstrate Vamp’s core features - together we call them “Sava”. Sava is a mythical vampire from Serbia (wiki), but in our case it is a Github repo full of examples to help us demonstrate Vamp.\nYou can work with Sava in the Vamp hello world setup (or any other Vamp installation).\n\nDeploy your first blueprint\nRun a canary release\nSplit a monolith into services\nMerge a changed topology\n\n Learn more\n\nDeploy and canary release Wordpress  \nThis tutorial shows how to deploy Wordpress together with mySQL and set up a gateway to run a canary release\n\nAutomate a canary release with rollback  \nUse workflows to automate a canary release and force a rollback (Vamp Runner)\n\nCreate a workflow that generates events  \nLearn about the Vamp events system and create a simple workflow that interacts with the Vamp API \n\n",
    "id": 56
  },
  {
    "path": "/documentation/tutorials/run-a-canary-release",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Run a canary release",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    weight": "   weight: 30",
    "content": "\nIn the previous tutorial we deployed our app sava 1.0. If you haven't walked through that part already, please do so before continuing.\n\nNow let's say we have a new version of this great application that we want to canary release into production. We have it containerised as magneticio/sava:1.1.0 and are ready to go. In this tutorial we will:\n\nPrepare our blueprint\nDeploy the new version of our application next to the old one\nUse conditions to target specific groups\nLearn a bit more about conditions\n\nPrepare our blueprint\n\nVamp allows you to do canary releases using blueprints. Take a look at the YAML example below. It is quite similar to the blueprint we initially used to deploy sava 1.0.0. However, there are two big differences.\n\nThe services key holds a list of breeds: one for v1.0.0 and one for v1.1.0 of our app. Breeds are Vamp's way of describing static artifacts that can be used in blueprints.\nWe've added the routes key which holds the weight of each service as a percentage of all requests. Notice we assigned 50% to our current version 1.0.0 and 50% to the new version 1.1.0 We could also start with a 100% to 0% split, a 99% to 1% split or whatever combination you want as long as all percentages add up to 100% in total. There is nothing stopping you from deploying three or more versions and distributing the weight among them. Just make sure that when doing a straight three-way split you give one service 34% as 33%+33%+33%=99%.\n\n,name: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    gateways:\n      routes:\n        sava:1.0.0:\n          weight: 50%   weight in percentage\n        sava:1.1.0:\n          weight: 50%\n    services: # services is now a list of breeds\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n      breed:\n          name: sava:1.1.0 # a new version of our service\n          deployable: magneticio/sava:1.1.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\nYou could also just leave out the whole routes section and use the UI to change the weights after we've done the deployment.\n\nDeploy the new version of our application next to the old one\n\nIt is our goal to update the already running deployment with the new blueprint. Vamp will figure out that v1.0.0\nis already there and just add v1.1.0 while setting the correct routing between these services. You could also create a second blueprint with the new service, and merge this new service to the Sava-cluster so it becomes available for routing traffic to it.\n\n Deploy using the UI\n\nGo to the DEPLOYMENTS tab, open the running deployment and click the EDIT button (top right). Copy the blueprint above and paste it over the the deployment that is there then click SAVE. Vamp will start working out the differences and update the deployment accordingly.\n\nDeploy using the API\n\nGet the running deployment's name (the UUID) from /api/v1/deployments (or use the explicit name that you used for the deployment) and PUT the blueprint to that resource, e.g: /api/v1/deployments/sava\n\n Check the deployment and routing\nWhen Vamp has finished deploying, you can start refreshing your browser at the correct endpoint, e.g. http://192.168.99.100:9050/. The application should switch between responding with a 1.0 page and a 1.1 page. Note that this works best with the \"Incognito\" or \"Anonymous\" mode of your browser because of the caching of static assets.\n\nUse conditions to target specific groups\n\nUsing percentages to divide traffic between versions is already quite powerful, but also very simplistic.\nWhat if, for instance, you want to specifically target a group of users? Or a specific channel of requests\nfrom an internal service? Vamp allows you to do this right from the blueprint DSL.\n\nLet's start simple: We will allow only Chrome users to access v1.1.0 of our application by inserting this routing scheme:\n\n,routes:\n  sava:1.1.0:\n    weight: 0%\n    condition_strength: 100%\n    condition: User-Agent = Chrome    \n\nNotice three things:\n\nWe inserted a list of conditions (with only one condition for now).\nWe set the condition strength to 100% (it would be also by default set to 100%). This is important because we want all Chrome users to access the new service - we could also say condition_strength: 50% to give access just to half of them (the other 50% would be redirected to weight rules and routed accordingly).\nWe set the weight to 0% because we don't want any other users to access sava:1.1.0\n\nThe first service where the condition matches the request will be used to handle the request.\nMore information about using conditions, weights, sticky sessions etc..  \n\nOur full blueprint now looks as follows:\n\n,name: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    gateways:\n      routes:\n        sava:1.0.0:\n          weight: 100%\n        sava:1.1.0:\n          weight: 0%\n          condition_strength: 100%\n          condition: User-Agent = Chrome\n    services:  services is now a list of breeds\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n      breed:\n          name: sava:1.1.0 # a new version of our service\n          deployable: magneticio/sava:1.1.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n\nUsing the UI, you can either use the EDIT button from the deployment details screen again and completely paste in this blueprint or just\nfind the right place in the blueprint and edit it by hand. The result should be the same as using our UI to insert a condition:\n\nAs we are not actually deploying anything but just reconfiguring routes, the update should be almost instantaneous. You can fire up a Chrome browser and a Safari browser and check the results. A hard refresh might be necessary because of your browser's caching routine.\n\nA bit more about conditions\n\nOur browser example is easily testable on a laptop, but of course a bit contrived. Luckily you can\ncreate much more powerful conditions quite easily. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv/configuration-1.5 - ACL basics) and you can use the exact ACL definition right in the blueprint in the 'condition` field.\n\n Vamp short codes\n\nACLs can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following, but we will be expanding on this in the future:\n\nUser-Agent = string\nHost = string\nCookie cookie name Contains string\nHas Cookie cookie name\nMisses Cookie cookie name\nHeader header name Contains string\nHas Header header name\nMisses Header header name\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nhdr_sub(user-agent) Android   # straight ACL\nuser-agent=Android            # lower case, no white space\nUser-Agent=Android            # upper case, no white space\nuser-agent = Android          # lower case, white space\n\nAdd multiple conditions\nMultiple conditions can be included using boolean expressions. For example, the following condition would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest and then it would check whether the request has the header \"X-VAMP-TUTORIAL\". So any request matching both conditions would go to this service.\n\n,gateways:\n  weight: 100%\n  condition: \"User-Agent = Chrome AND Has Header X-VAMP-TUTORIAL\"\n\nUsing a tool like httpie (github.com/jkbrzt/httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-TUTORIAL:stuff\n\n  \nCool stuff. But we are dealing here with single, monolithic applications. Where are the microservices?  We will chop up this monolith into services and deploy them with Vamp in the third part of our tutorial →\n  \n",
    "id": 57
  },
  {
    "path": "/documentation/tutorials/split-a-monolith",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Split a monolith",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Tutorials\"",
    "    weight": "   weight: 40",
    "content": "In the previous tutorial we did some basic canary releasing on two versions of a monolithic application. Very nice, but Vamp isn't\ncalled the Very Awesome Microservices Platform for nothing. The next step is to split our monolithic Sava application into separate services. In this tutorial we will:\n\ndefine a new service topology\nlearn about environment variables and service discovery\n\nDefine a new service topology\n\nTo prove our point, we are going to slightly \"over-engineer\" our services solution. This will also help\nus demonstrate how we can later remove parts of our solution using Vamp. For now, we'll split the\nmonolith into a topology of one frontend and two separate backend services. After our engineers\nare done with coding, we can catch this new topology in the following blueprint. Please notice a couple\nof things:\n\nWe now have three clusters: sava, backend1 and backend2. Each cluster could have multiple\nservices on which we could do separate canary releases and set separate filters.\nThe sava cluster has explicit dependencies on the two backends. Vamp will make sure these dependencies\nare checked and rolled out in the right order.\nUsing environment_variables we connect the dynamically assigned ports and hostnames of the backend\nservices to the \"customer facing\" sava service.\nWe've change the gateway port to 9060 so it doesn't collide with the  monolithic deployment.\n\n,name: sava:1.2\ngateways:\n  9060: sava/webport\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava-frontend:1.2.0\n        deployable: magneticio/sava-frontend:1.2.0\n        ports:\n          webport: 8080/http                \n        environment_variables:\n          BACKEND_1: http://$backend1.host:$backend1.ports.webport/api/message\n          BACKEND_2: http://$backend2.host:$backend2.ports.webport/api/message\n        dependencies:\n          backend1: sava-backend1:1.2.0\n          backend2: sava-backend2:1.2.0\n      scale:\n        cpu: 0.2      \n        memory: 64MB\n        instances: 1               \n  backend1:\n    services:\n      breed:\n        name: sava-backend1:1.2.0\n        deployable: magneticio/sava-backend1:1.2.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1              \n  backend2:\n    services:\n      breed:\n        name: sava-backend2:1.2.0\n        deployable: magneticio/sava-backend2:1.2.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\nDeploy this blueprint using either the UI or a REST call and when deployed check out the new topology in your browser (on port 9060 this time). When deployed it should yield something similar to:\n\n Learn about environment variables and service discovery\n\nIf you were to check out the Docker containers using docker inspect, you would see the environment variables that we set in the blueprint.\n\n docker inspect 66e64bc1c8ca\n...\n\"Env\": [\n    \"BACKEND_1=http://172.17.42.1:33021/api/message\",\n    \"BACKEND_2=http://172.17.42.1:33022/api/message\",\n    \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n],\n...\n\nHost names and ports are configured at runtime and injected in the right parts of your running deployment. Your service/app should pick up these variables to configure itself. Luckily, this is quite easy and common in almost all languages and frameworks.\n\nRemember, there is no \"point-to-point\" wiring. The exposed host and port are actually service\nendpoints. The location, amount and version of containers running behind that service endpoint can vary.\nLearn more about how Vamp does service discovery.\n\n  \nGreat! We just demonstrated that Vamp can handle dependencies between services and configure these services with host and port information at runtime. Now let's do a more complex migration to a new service based topology →.\n  \n\n",
    "id": 58
  },
  {
    "path": "/documentation/using vamp/v0.9.1/artifacts",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Artifacts",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 10",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nVamp has a few basic entities or artifacts you can work with, these can be classed as static resource descriptions and dynamic runtime entities. Note that API actions on static resource descriptions are mostly synchronous, while API actions on dynamic runtime entities are largely asychronous.\n\nDynamic runtime entities\n\nDeployments are running blueprints. You can have many deployments from one blueprint and perform actions on each at runtime. Plus, you can turn any running deployment into a blueprint.  Read more...  \nGateways are the \"stable\" routing endpoint - defined by a port (incoming) and routes (outgoing).  Read more... \nWorkflows are apps (services) deployed on cluster, used for dynamically changing the runtime configuration (e.g. SLA, scaling, condition weight update).  Read more...\n\n Static resource descriptions\n\nBlueprints are, well, blueprints! They describe how breeds work in runtime and what properties they should have.  Read more...  \nBreeds describe single services and their dependencies.  Read more...\nScales define the size of a deployed service Read more...\n\nWorking across multiple teams\n\nIn larger companies with multiple teams working together on a large project, all required information is often not available at the same time. To facilitate this style of working, Vamp allows you to set placeholders. Placeholders let you communicate with other teams using simple references and gradually build up a complicated deployment. Vamp will only check references at deployment time, this means:\n\nBreeds can be referenced in blueprints before they exist \nYou do not need to know the contents of an SLA when you reference it.\nYou can reference a variable that someone else should fill in.\n\nRead more about referencing artifacts and environment variables.\n\n  \nRead about Vamp breeds\nCheck the API documentation\nTry Vamp\n  ",
    "id": 59
  },
  {
    "path": "/documentation/using vamp/v0.9.1/blueprints",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Blueprints",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 30",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nBlueprints are execution plans - they describe how your services should be hooked up and what their topology should look like at runtime. This means you reference your breeds (or define them inline) and add runtime configuration to them.\n\nBlueprints allow you to add the following extra properties:\n\nGateways: a stable port where the service can be reached.\nClusters and services: a cluster is a grouping of services with one purpose, i.e. two versions (a/b) of one service.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDialects: a dialect is a set of native commands for the underlying container platform, i.e. Docker or Mesosphere Marathon.\nScale: the CPU and memory and the amount of instance allocate to a service.\nConditions: how traffic should be directed based on HTTP and/or TCP properties.\nSLA and escalations: SLA definition that controls autoscaling.\n\nExample - key concepts of blueprints\n\n,name: my_blueprint                         Custom blueprint name\ngateways:\n  8080/http: my_frontend/port\nclusters:\n  my_frontend:                            # Custom cluster name.\n  \n    gateways:                             # Gateway for this cluster services.\n      routes:                             # Makes sense only with\n        somecoolbreed:                  # multiple services per cluster.\n          weight: 95%\n          condition: User-Agent = Chrome\n        someotherbreed:                 # Second service.\n          weight: 5%\n          \n    services:                             # List of services\n      breed:\n          ref: somecoolbreed\n        scale:                            # Scale for this service.\n          cpu: 2                          # Number of CPUs per instance.\n          memory: 2048MB                  # Memory per instance (MB/GB units).\n          instances: 2                    # Number of instances\n      breed: \n          ref: someotherbreed           # Another service in the same cluster.  \n        scale: large                      # Notice we used a reference to a \"scale\". \n                                          # More on this later.\n\nGateways\n\nA gateway is a \"stable\" endpoint (or port in simplified sense) that almost never changes. When creating the mapping, it uses the definition (my_frontend/port in this case) from the \"first\" service in the cluster definition you reference. This service can of course be changed, but the gateway port normally doesn't.\n\nPlease take care of setting the /tcp or /http (default) type for the port. Using /http allows Vamp to record more relevant metrics like response times and metrics.\n\nRead more about gateways.\n\n  \ngateways are optional. You can just deploy services and have a home grown method to connect them to some stable, exposable endpoint.\n  \n\n Clusters and services\n\nIn essence, blueprints define a collection of clusters.\nA cluster is a group of different services, which will appear as a single service and serve a single purpose.\n\nCommon use cases would be service A and B in an A/B testing scenario - usually just different\nversions of the same service (e.g. canary release or blue/green deployment).\n\nClusters are configured by defining an array of services. A cluster can be given an arbitrary name. Services are just lists or arrays of breeds.\n\n,mycoolcluster\n  services\n   breed: \n      ref: mycoolservice_A      # reference to an existing breed\n   breed:                       # shortened inline breed\n       name: mycoolservice_B\n       deployable: some_container\n       ...\n\nClusters and services are just organisational items. Vamp uses them to order, reference and control the actual containers and gateways and traffic.\n\n This all seems redundant, right? We have a reference chain of blueprints - gateways - clusters - services - breeds - deployable. However, you need this level of control and granularity in any serious environment where DRY principles are taken seriously and where \"one size fits all\" doesn't fly.\n\nDialects\n\nVamp allows you to use container driver specific tags inside blueprints. We call this a “dialect”.  Dialects effectively enable you to make full use of, for instance, the underlying features like mounting disks, settings commands and providing access to private Docker registries.\n\nWe currently support the following dialects:\n\ndocker:\nmarathon:\n\n Docker dialect\n\nThe following example show how you can mount a volume to a Docker container using the Docker dialect.\n\nExample blueprint - using the Docker dialect\n\n,name: busybox\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox-breed\n        deployable: busybox:latest\n      docker:\n        Volumes:\n          \"/tmp\": ~\n\nVamp will translate this into the proper API call. Inspecting the container after it's deployed should show something similar to this:\n\n...\n\"Volumes\":  ,\n...    \n\n Marathon dialect\n\nThis is an example with Marathon that pulls an image from private repo, mounts some volumes, sets some labels and gets run with an ad hoc command: all taken care of by Marathon.\n  \nWe can provide the marathon: tag either on the service level, or the cluster level. Any marathon: tag set on the service level will override the cluster level as it is more specific. However, in 9 out of 10 cases the cluster level makes the most sense. Later, you can also mix dialects so you can prep your blueprint for multiple environments and run times within one description.\n\nexample blueprint - using the Marathon dialect\n\nNotice the following:\n\nUnder the marathon: tag, we provide the command to run in the container by setting the cmd: tag.\nWe provide a url to some credentials file in the uri array. As described in the Marathon docs (mesosphere.github.io/marathon - using a private Docker repository) this enables Mesos\nto pull from a private registry, in this case registry.example.com where these credentials are set up.\nWe set some labels with some arbitrary metadata.\nWe mount the /tmp to in Read/Write mode.\n\n,name: busy-top:1.0\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox\n        deployable: registry.example.com/busybox:latest\n      marathon:\n       cmd: \"top\"\n       uris:\n         \"https://somehost/somepath/somefilewithdockercredentials\"\n       labels:\n         environment: \"staging\"\n         owner: \"buffy the vamp slayer\"\n       container:\n         volumes:\n           containerPath: \"/tmp/\"\n             hostPath: \"/tmp/\"\n             mode: \"RW\"\n\n Scale\n\nScale is the \"size\" of a deployed service. Usually that means the number of instances (servers) and allocated CPU and memory.\n\nScales can be defined inline in a blueprint or they can defined separately and given a unique name. The following example is a scale named \"small\". POST-ing this scale to the /scales REST API endpoint will store it under that name so it can be referenced from other blueprints.\n\nExample scale\n\n,name: small    Custom name.\n\ncpu: 2        # Number of CPUs per instance.\nmemory: 2gb   # Memory per instance, MB/GB units.\ninstances: 2  # Number of instances.\n\n  \nRead about Vamp deployments\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 60
  },
  {
    "path": "/documentation/using vamp/v0.9.1/breeds",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Breeds",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 20",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nBreeds are static descriptions of applications and services available for deployment. Each breed is described by the DSL in YAML notation or JSON, whatever you like. This description includes name, version, available parameters, dependencies etc.\nTo a certain degree, you could compare a breed to a Maven artifact or a Ruby Gem description.\n\nBreeds allow you to set the following properties:\n\nDeployable: the name of actual container or command that should be run.\nPorts: a map of ports your container exposes.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDependencies: a list of other breeds this breed depends on.\n\nDeployable\n\nDeployables are pointers to the actual artifacts that get deployed. Vamp supports Docker containers or can support any other artifacts supported by your container manager. \n\n Example breed - deploy a Docker container\n\n,name: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http   \n\nThis breed, with a unique name, describes a deployable and the port it works on. \n\nDocker deployables\n\nBy default, the deployable is a Docker container. \nWe could also make this explicit by setting type to docker. The following statements are equivalent:\n\n,deployable: company/myfrontendservice:0.1\n\n,deployable: \n  type: container/docker\n  definition: company/myfrontendservice:0.1\n\nThis shows the full (expanded) deployable with type and definition.\n\nDocker images are pulled by your container manager from any of the repositories configured. By default that would be the public Docker hub, but it could also be a private repo.\n\n Other deployables\n\nRunning \"other\" artifacts such as zips or jars heavily depends on the underlying container manager.\nWhen Vamp is set up to run with Marathon (mesosphere.github.io - Marathon), command (or cmd) deployable types can be used.\nIn that case cmd (Marathon REST API - post v2/apps) parameter will have value of deployable.\n\nExample breed - run a custom jar after it has been downloaded \nCombining this definition and the Vamp Marathon dialect  uris parameter allows the requested jar to be downloaded from a remote location (Marathon REST API - uris Array of Strings). \n\n,name: location\nclusters:\n  api:\n    services:\n      breed:\n        name: location\n        deployable: \n          type: cmd\n          definition: java -jar location.jar\n      marathon:\n        uris: [\"https://myrepolocation_jar\"]\n\n JavaScript deployables \n\nBreeds can have type application/javascript and definition should be a JavaScript script:\n\n,name: hello-world\ndeployable:\n  type: application/javascript\n  definition: |\n    console.log('Hello World Vamp!');\n\nIt is possible to create or update breeds with the API request POST|PUT /api/v1/breeds/ , Javascript script as body and header Content-Type: application/javascript.\n\nPorts\n\nThe ports property is an array of named ports together with their protocol. It describes on what ports the deployables is offering services to the outside world. Let's look at the following breed:\n\n,name: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http\n  admin: 8081/http\n  redis: 9023/tcp   \n\nPorts come in two flavors:\n\n/http HTTP ports are the default type if none is specified. They are always recommended when dealing with HTTP-based services. Vamp can record a lot of interesting metrics like response times, errors etc. Of course, using /tcp will work but you miss out on cool data.\n/tcp Use TCP ports for things like Redis, MySQL etc.\n\n  \n/http notation for ports is required for use of filters.\n  \n\nNotice we can give the ports sensible names. This specific deployable has web port for customer traffic, an admin port for admin access and a redis port for some caching probably. These names come in handy when we later compose different breeds in blueprints.\n\n  \nRead about Vamp blueprints\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 61
  },
  {
    "path": "/documentation/using vamp/v0.9.1/conditions",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Conditions",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 70",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nConditions are used by gateways to filter incoming traffic for routing between services in a cluster.\nRead more about gateway usage. You can define conditions inline in a blueprint or store them separately under a unique name on the /conditions endpoint and just use that name to reference them from a blueprint. \n\nExample - simple inline condition\n\nThis would be used directly inside a blueprint.\n\n,condition_strength: 10%   Amount of traffic for this service in percents.\ncondition: User-Agent = IOS\n\nCreate a condition \n\nCreating conditions is quite easy. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv - 7.1 ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a condition.\n\nHowever, ACL's can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following:\n\n| description           | syntax                       | example                  |\n| ----------------------|:----------------------------:|:------------------------:|\n| match user agent      | user-agent == value          | user-agent == Firefox    |\n| mismatch user agent   | user-agent != value          | user-agent != Firefox    |\n| match host            | host == value                | host == localhost        |\n| mismatch host         | host != value                | host != localhost       |\n| has cookie            | has cookie value             | has cookie vamp          |\n| misses cookie         | misses cookie value          | misses cookie vamp       |\n| has header            | has header value             | has header ETag          |\n| misses header         | misses header value          | misses header ETag       |\n| match cookie value    | cookie name has value    | cookie vamp has 12345    |\n| mismatch cookie value | cookie name misses value | cookie vamp misses 12345 |\n| header has value      | header name has value   | header vamp has 12345    |\n| header misses value   | header name misses value | header vamp misses 12345 |\n\nAdditional syntax examples: github.com/magneticio/vamp - ConditionDefinitionParserSpec.scala.\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nIn order to specify plain HAProxy ACL, ACL needs to be between  :\n\ncondition: \" hdr_sub(user-agent) Chrome \"\n\nHaving multiple conditions in a condition is perfectly possible. For example, the following condition would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest and then it would check whether the request has the header\n\"X-VAMP-MY-COOL-HEADER\". So any request matching both conditions would go to this service.\n\n,gateways:\n  weight: 100%\n  condition: \"User-Agent = Chrome AND Has Header X-VAMP-MY-COOL-HEADER\"\n\nUsing a tool like httpie (github.com/jkbrzt/httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-MY-COOL-HEADER:stuff\n\n Boolean expression in conditions\n\nVamp supports AND, OR, negation NOT and grouping ( ):\n\n,gateways:\n  weight: 100%\n  condition: (User-Agent = Chrome OR User-Agent = Firefox) AND has cookie vamp\n\nAdditional boolean expression examples: github.com/magneticio/vamp - BooleanParserSpec.scala.\n\n  \nRead about Vamp events\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 62
  },
  {
    "path": "/documentation/using vamp/v0.9.1/deployments",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deployments",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 40",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nA deployment is a \"running\" blueprint. Over time, new blueprints can be merged with existing deployments or parts of the running blueprint can be removed from it. Each deployment can be exported as a blueprint and \ncopy / pasted to another environment, or even to the same environment to function as a clone.\n\nCreate a deployment\n\nYou can create a deployment in the following ways:\n\nSend a POST request to the /deployments endpoint.\nUse the UI to deploy a blueprint using the \"deploy\" button on the \"blueprints\" tab.\nUse the CLI vamp deploy command  \n $ vamp deploy my_blueprint.\n\nThe name of the deployment will be automatically assigned as a UUID (e.g. 123e4567-e89b-12d3-a456-426655440000).\n\n Vamp deployment process\n\nOnce we have issued the deployment, Vamp will do the following:\n\nUpdate Vamps internal model.\nIssue and monitor deployment commands to the container platform.\nUpdate the ZooKeeper entry.\nStart collecting metrics.\nMonitor the container platform for changes.\n\nVamp will add runtime information to the deployment model, like start times, resolved ports etc.\n\nDeployment scenarios\n\nA common Vamp deployment scenario is to introduce a new version of the service to an existing cluster, this is what we call a merge. After testing/migration is done, the old or new version can be removed from the cluster, simply called a removal. Let's look at each in turn.\n\n Merge\n\nMerging of new services is performed as a deployment update. You can merge in many ways:\n\nSend a PUT request to the /deployments/  endpoint.\nUse the UI to update a deployment using the \"Edit deployment\" button. \nUse the CLI with a combination of the vamp merge and vamp deploy commands.\n\nIf a service already exists then only the gateways and scale will be updated. Otherwise a new service will be added. If a new cluster doesn't exist in the deployment, it will be added.\n\nLet's deploy a simple service:\n\n,name: monarch_1.0\n\nclusters:\n  monarch:\n    # Specifying only a reference to the breed.\n    breed: monarch_1.0   \n\nAfter this point we may have another version ready for deployment and now instead of only one service, we have added another one:\n\n,name: monarch_1.1\n\nenvironment_variables:\n  # Some variable needed for our new recommendation engine,\n  # just as an example.\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    # Just a reference and this breed has one dependency:\n    # recommendation_1.0\n    breed: monarch_1.1\n\n  recommendation:\n    breed: recommendation_1.0\n \n\nNow our deployment (in simplified blueprint format) looks like this:\n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 100%\n      monarch_1.1:\n        weight: 0%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    \n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nNote that the route weight for monarch_1.1 is 0, i.e. no traffic is sent to it.\nLet's redirect some traffic to our new monarch_1.1 (e.g. 10%):\n\n,clusters:\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 90%\n      monarch_1.1:\n        weight: 10%\n\nNote that we can omit other fields like name, parameters and even other clusters (e.g. recommendation) if the change is not relevant to them. In this example we just wanted to update the weights.\n\nIn the last few examples we have shown the following:\n\nA fresh new deployment.\nA canary release with a cluster update and change of the topology (a new cluster was added).\nAn update of the gateways for a cluster - similar to a cluster scale update (instances, cpu, memory).\n\nRemoval\n\nRemoval is done using the REST API DELETE request together with the new blueprint as request body.\nIf a service exists it will be removed, otherwise the request is ignored. If a cluster has no more services left the cluster will be removed completely. Lastly, if a deployment has no more clusters it will be completely removed (destroyed).\n\nLet's use the example from the previous section. Notice the weight is evenly distributed (50/50). \n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 50%\n      monarch_1.1:\n        weight: 50%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100\n\nIf we are happy with the new monarch version 1.1, we can proceed with the removal of the old version.\nThis change is applied on the running deployment. We send the following YAML as the body of the DELETE request\nto the /deployments/deployment_UUID endpoint.\n\n,name: monarch_1.0\n\nclusters:\n  monarch:\n    breed: monarch_1.0\n\nNote that this is the same original blueprint we started with. What we are doing here is basically \"subtracting\" one blueprint from the other, although \"the other\" is a running deployment.\nAfter this operation our deployment is:\n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.1\n    gateways:\n      monarch_1.1:\n        weight: 100%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nIn a nutshell: If we say that the first version was A and the second B, then we just did the migration from A to B without downtime:\nA** - A + B - A + B - A - **B\n\nWe could also remove the newer version (monarch_1.1 with/without recommendation cluster) in case that it didn't perform as we expected.\n\n  \nRead about Vamp environment variables\nCheck the API documentation\nTry Vamp\n  ",
    "id": 63
  },
  {
    "path": "/documentation/using vamp/v0.9.1/environment-variables",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Environment variables",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 50",
    "content": "\n   \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nBreeds and blueprints can include lists of environment variables that will be injected into the container at runtime. You set environment variables with the environment_variables keyword or its shorter version env, e.g. both examples below are equivalent.\n\n,environment_variables:\n  PORT: 8080\n\n,env:\n  PORT: 8080\n\nDependencies\n\nBreeds can also have dependencies on other breeds. These dependencies should be stated explicitly, similar to how you would do in a Maven pom.xml, a Ruby Gemfile or similar package dependency systems, i.e:\n\n,dependencies:\n  cache: redis:1.1\n\nIn a lot of cases, dependencies coexist with interpolated environment variables or constants because exact values are not known untill deploy time.\n\n 'Hard' setting a variable\n\nYou want to \"hard set\" an environment variable, just like doing an export MYVAR=somevalue in a shell. This  variable could be some external dependency you have no direct control over: the endpoint of some service you use that is out of your control. It can also be some setting you want to tweak, like JVMHEAPSIZE or AWS_REGION.\n\n,name: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\n\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1'     \n\nIt is also possible to use wildcard * at the end of the name:\n\n,dependencies:\n  cache: redis:1.*\n\nThis will match any breed name that starts with redis:1.\n\nUsing place holders\n\nUse the ~ character to define a place holder for a variable that should be filled in at runtime (i.e. when this breed actually gets deployed), but for which you do not yet know the actual value. \n\n  \nWhen different roles in a company work on the same project. Developers can create place holders for variables that operations should fill in: it helps with separating responsibilities.\n  \n\n Example - ORACLE_PASSWORD designated as a place holder\n\n,name: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1' \n  ORACLE_PASSWORD: ~    \n\nResolving variables\n\nUse the $ character to reference other statements in a breed/blueprint. This allows you to dynamically resolve ports and hosts names that we don't know until a deployment is done. You can also resolve to hard coded and published constants from some other part of the blueprint or breed, typically a dependency.\n\n  \nThe $ value is escaped by $$. A more strict notation is $ \n  \n\n Vamp host variable\n\nVamp provides just one magic* variable: the host. This resolves to the host or ip address of the referenced service. Strictly speaking, the host reference resolves to the gateway agent endpoint, but users do not need to concern themselves with this. Users can think of one-on-one connections where Vamp actually does server-side service discovery to decouple services.\n\nExample - resolving variables from a dependency\n\n,name: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host         resolves to a host at runtime\n      MYSQL_PORT: $backend.ports.port  # resolves to a port at runtime\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n\nExample - resolving variables from a dependency's environment variables\nWhat if the backend is configured through some environment variable, but the frontend also needs that information? For example, the encoding type for our database. We can just reference that environment variable using the exact same syntax.\n\n,name: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host       \n      MYSQL_PORT: $backend.ports.port \n      BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n    environment_variables:\n      ENCODING_TYPE: 'UTF8'    injected into the backend MySQL container\n\nYou can do everything with environment_variables but constants (see below) allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\nEnvironment variable scope\n\nEnvironment variables can live on different scopes and can be overridden by scopes higher in the scope hierarchy.\nA scope is an area of your breed or blueprint definition that limits the visibility of variables and references inside that scope.\n\nBreed scope: The scope used in all the above examples is the default scope. If you never define any environment_variables in any other place, this will be used.\n\nCluster scope: Will override the breed scope and is part of the blueprint artifact. Use this to override environment variables for all services that belong to a cluster.\n\nService scope: Will override breed scope and cluster scope, and is part of the blueprint artifact. Use this to override all environment variables for a specific service within a cluster.\n\n   \nEffective use of scope is completely dependent on your use case. The various scopes help to separate concerns when multiple people and/or teams work on Vamp artifacts and deployments and need to decouple their effort.\n  \n\n Examples of scope use\n\nRun two of the same services with different configurations\nOverride the JVMHEAPSIZE in production\nUse a place holder\nJust for fun - combine all scopes and references\n\nExample 1\nRun two of the same services with different configurations\n\nUse case: As a devOps-er you want to test one service configured in two different ways at the same time. Your service is configurable using environment variables. In this case we are testing a connection pool setting. \n\nImplementation: In the below blueprint we just use the breed level environment variables. The traffic is split into a 50/50 divide between both services.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0-a\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 30                \n      breed:\n          name: frontend_app:1.0-b               different breed name, same deployable\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 60                 # different pool size\n\n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nExample 2\nOverride the JVM_HEAP_SIZE in production\n\nUse case: As a developer, you created your service with a default heap size you use on your development laptop and maybe on a test environment. Once your service goes \"live\", an ops guy/gal should be able to override this setting.\n\nImplementation: In the below blueprint we override the variable JVMHEAPSIZE for the whole frontend cluster by specifically marking it with .dot-notation cluster.variable \n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                   cluster level variable \n  frontend.JVMHEAPSIZE: 2800          # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            JVMHEAPSIZE: 1200         # will be overridden by deployment level: 2800\n\nExample 3\nUse a place holder\n\nUse case: As a developer, you might not know some value your service needs at runtime, say the Google Anaytics ID your company uses. However, your Node.js frontend needs it! \n\nImplementation: In the below blueprint the ~ place holder is used to explicitly demand that a variable is set by a higher scope. When this variable is NOT provided, Vamp will report an error at deploy time.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.GOOGLEANALYTICSKEY: 'UA-53758816-1'  # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/node_express:1.0\n          ports:\n            port: 8080/http\n          environment_variables:           \n            GOOGLEANALYTICSKEY: ~               # If not provided at higher scope, \n                                                  # Vamp reports error.\n\nExample 4\nCombine all scopes and references\n\nAs a final example, let's combine some of the examples above and include referenced breeds. In this case, we have two breed artifacts already stored in Vamp and include them by using the ref keyword.\n\nIn the below blueprint:  \n\nwe override all breed scope JVMHEAPSIZE variables with cluster scope environment_variables\nto further tweak the JVMHEAPSIZE for the service frontendapp:1.0-b, we also add service scope environmentvariables for that service.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.JVMHEAPSIZE: 2400                    # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          ref: frontend_app:1.0-a\n      breed:\n          ref: frontend_app:1.0-b        \n        environment_variables:           \n          JVMHEAPSIZE: 1800               # overrides the breed level AND cluster level\n          \n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nConstants\n\nSometimes you just want configuration information to be available in a breed or blueprint. You don't need that information to be directly exposed as an environment variable. As a convenience, Vamp allows you to set constants.\nThese are values that cannot be changed during deploy time.\n\nYou can do everything with environment_variables but constants allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\n Exammple - using constants\n\n,clusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      environment_variables:\n        MYSQL_HOST: $backend.host       \n        MYSQL_PORT: $backend.ports.port \n        BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n        SCHEMA: $backend.constants.SCHEMA_NAME\n      dependencies:\n        backend: mysql:1.0\n  backend:\n    breed:\n      name: mysql:1.0\n      environment_variables:\n        ENCODING_TYPE: 'UTF8'\n      constants:\n        SCHEMA_NAME: 'customers'    # NOT injected into the backend MySQL container\n\n  \nRead about Vamp gateways\nCheck the API documentation\nTry Vamp\n  ",
    "id": 64
  },
  {
    "path": "/documentation/using vamp/v0.9.1/escalations",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Escalations",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 100",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nAn escalation is a workflow triggered by an escalation event. Vamp checks for these escalation events using a continuous background process with a configurable interval time. If the events match the escalation handlers defined in the DSL, the action is executed.\n\nAgain: escalation events can be generated by third party systems and they will be handled in the same manner as events created by Vamp SLA workflows. \n\nEscalation handlers\n\nAny escalation that is triggered should be handled by an escalation handler\n\nVamp ships with the following set of escalation handlers - scaleinstances, scalecpu and scale_memory. These handlers can be composed into intricate escalation systems.\n\n scale_instances   \nScales up the number of running instances. It is applied only to the first service in the cluster (old or \"A\" version). You can set upper limits to how far you want to scale out or in, effectively guaranteeing a minimum set of running instances. This is very much like AWS auto-scaling.  \nExample - scale_instances\n,type: scale_instances\ntarget: monarch   Target cluster for the scale up/down.\n                 # If it's not specified, by default it's the \n                 # current cluster where SLA escalations are \n                 # specified.\nminimum: 1       # Minimum number of instances.\nmaximum: 3       # Maximum number of instances.\nscale_by: 1      # Increment/decrement to use on current \n                 # number of running instances.\nscale_cpu \nScales up the number of CPUs per instances. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_cpu\n,type: scale_cpu\ntarget: monarch  \nminimum: 1\nmaximum: 3 \nscale_by: 0.5\nscale_memory   \nScales up the memory per instance. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_memory  \n,type: scale_memory\ntarget: monarch  \nminimum: 512     # In MB.\nmaximum: 4096    # In MB.\nscale_by: 512    # In MB.\n\nComposing escalation handlers\n\nVamp has a set of predefined escalation handler types that deal with escalations. You can compose these handlers in the DSL to get the desired outcome of an escalation event. The escalation handlers toall and toone are supported:\n\n to_all  \nThis is a \"group\" escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to all escalation handlers from its list. No order or hierarchy.    \n\nExample - to_all  \n,to_all:\n  escalations:\n     Scale up/down.\n    scale_instances\n    # And notify for each event.\n    notify\nto_one  \nThis is a group escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to each escalation handler (from its list) until one can handle it. On an Escalate event, it will start at the head of the list. During a DeEscalate event is will start at the rear.  \nWhen is this useful? Well, Vamp could try to scale up the service and if that;s not possible anymore (e.g. reached the upper limit of allowed scale) then an email can be sent.  \n\n Example - simple use of to_one\n,to_one:\n  escalations:\n    # First try to escalate.\n    scale_instances\n    # If it's not possible, proceed with notifying.\n    notify\nExample - complex use of to_one  \n,name: monarch\n\ngateways:\n  80: monarch1/port\n\nenvironment_variables:\n  monarch2.password: secret\n\nclusters:\n\n  monarch1:\n    breed:\n      name: monarch1\n      deployable: vamp/monarch1\n      ports:\n        port: 80/http\n      environment_variables:\n        STORAGE_HOST: $storage.host\n        DB_PORT: $storage.ports.port\n        STORAGEPASS: $storage.environmentvariables.password\n      \n      dependencies:\n        storage: monarch2\n\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n    \n    sla:\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000\n        lower: 100\n      window:\n        interval: 600\n        cooldown: 600\n      escalations:\n        to_one:\n            escalations:\n              type: scale_instances\n                 First try to scale up storage service.\n                target: monarch2\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              type: scale_instances\n                # If we cannot scale up storage anymore, scale up this.\n                target: monarch1\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              \n  monarch2:\n    breed:\n      name: monarch2\n      deployable: vamp/monarch2\n      ports:\n        port: 3306\n      environment_variables:\n        STORAGE_PASS: password\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n\n  \nRead about Referencing artifacts in Vamp\nCheck the API documentation\nTry Vamp\n  ",
    "id": 65
  },
  {
    "path": "/documentation/using vamp/v0.9.1/events",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Events",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 80",
    "content": "\n   \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nVamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions.\n\nAll events are stored and retrieved using the Event API that is part of Vamp.\n\nExample - JSON \"deployment create\" event\n\n \n\n Basic event rules\n\nAll events stick to some basic rules:\n\nAll data in Vamp are events. \nValues can be any JSON object or it can be empty.\nTimestamps are in ISO8601/RFC3339.\nTimestamps are optional. If not provided, Vamp will insert the current time.\nTimestamps are inclusive for querying.\nEvents can be tagged with metadata. A simple tag is just single string.\nQuerying data by tag assumes \"AND\" behaviour when multiple tags are supplied, i.e. [\"one\", \"two\"] would only fetch records that are tagged with both.\nSupported event aggregations are: average, min, max and count.\n\nHow tags are organised\n\nIn all of Vamp's components we follow a REST (resource oriented) schema, for instance:\n/deployments/  \n/deployments/ /clusters/ /services/ \nTagging is done using a very similar schema: \" \", \" : \". For example:\n\n[\n    \"deployments\", \n    \"deployments: \", \n    \"clusters\", \n    \"clusters: \", \n    \"services\", \n    \"services: \"\n]\n\nThis schema allows querying per group and per specific name. Getting all events related to all deployments is done by using tag \"deployments\". Getting events for specific deployment \"deployments: \".\n\n Query events using tags\n\nUsing the tags schema and timestamps, you can do some powerful queries. Either use an exact timestamp or use special range query operators, described on the elastic.co site (elastic.co - Range query).\n\n  \nthe default page size for a set of returned events is 30.\n  \n\nExample queries\n\nGet all events\nResponse time for a cluster\nCurrent sessions for a service\nll known events for a service\n \n Example 1\nGet all events\n\nThe below query gets ALL metrics events up till now, taking into regard the pagination.\n\n  \nGET request with body - similar to approach used by Elasticsearch.\n  \n\nGET /api/v1/events\n\n \n \n\nExample 2 \nResponse time for a cluster\n\nThe below query gets the most recent response time events for the \"frontend\" cluster in the \"d9b42796-d8f6-431b-9230-9d316defaf6d\" deployment.\n\nNotice the \"gateways:UUID\", \"metrics:responseTime\" and \"gateways\" tags. This means \"give me the response time of this specific gateway at the gateway level\". The response will echo back the events in the time range with the original set of tags associated with the events. \n\nGET /api/v1/events\n\n \n \n\n[\n     ,\n     \n]    \n\n Example 3\nCurrent sessions for a service\n\nAnother example is getting the current sessions for a specific service, in this case the monarch_front:0.2 service that is part of the 214615ec-d5e4-473e-a98e-8aa4998b16f4 deployment and lives in the frontend cluster.\n\nNotice we made the search more specific by specifying the \"services\" and then \"service:SERVICE NAME\" tag.\nAlso, we are using relative timestamps: anything later or equal (lte) than \"now\".\n\nGET /api/v1/events\n\n \n \n\nExample 4\nAll known events for a service\n\nThis below query gives you all the events we have for a specific service, in this case the same service as in example 2. In this way you can get a quick \"health snapshot\" of service, server, cluster or deployment.\n\nNotice we made the search less specific by just providing the \"metrics\" tag and not telling the API which specific one we want.\n\nGET /api/v1/events\n\n \n \n\n Server-sent events (SSE)\n\nEvents can be streamed back directly from Vamp.\n\nGET /api/v1/events/stream\n\nIn order to narrow down (filter) events, list of tags could be provided in the request body.\n\n \n\nGET method can be also used with tag parameter (may be more convenient):\n\nGET /api/v1/events/stream?tag=archiving&tag=breeds\n\nArchiving\n\nAll changes in artifacts (creation, update or deletion) triggered by REST API calls are archived. We store the type of event and the original representation of the artifact. It's a bit like a Git log. \n\nHere is an example event:\n\n \n\nSearching through the archive is 100% the same as searching for events. The same tagging scheme applies.\nThe following query gives back the last set of delete actions executed in the Vamp API, regardless of the artifact type.\n\nGET /api/v1/events\n\n \n \n\n  \nRead about Vamp SLA (Service Level Agreement)\nCheck the API documentation\nTry Vamp\n  ",
    "id": 66
  },
  {
    "path": "/documentation/using vamp/v0.9.1/gateways",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Gateways",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 60",
    "content": "\n   \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nGateways are dynmic runtime entities in the Vamp eco-system. They represent load balancer rules to deployment, cluster and service instances. There are two types of gateways:\n\nInternal gateways are created automatically for each deployment cluster and updated using the gateway/deployment API\nExternal gateways are explicitly declared either in a deployment blueprint or using the gateway API\n\nExample - automatically created gateway \n\nThe below gateway is for deployment vamp, cluster sava and port port.  \nThe cluster contains two services sava:1.0.0 and sava:1.1.0, each with two running instances. \n,name: vamp/sava/port            name\nport: 40000/http               # port, either http or tcp, assigned by Vamp\nactive: true                   # is it running - not in case of non (yet) existing routes\nsticky: none\nroutes:                        # routes\n  vamp/sava/sava:1.0.0/port:\n    weight: 50%\n    instances:\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda3c376-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31463\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda2d915-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31292\n  vamp/sava/sava:1.1.0/port:\n    weight: 50%\n    instances:\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa3c9e4-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31634\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa37bc3-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31826\n\nGateway Usage\n\nThe gateway API allows for programmable routing. External gateways give an entry point to clusters (optionally specified in deployment blueprints), and allow for canary releasing and A/B testing across deployments.\n\nA gateway defines a set of rules for routing traffic between different services within the same cluster.\nVamp allows you to determine this in three ways:\n\nA condition will target specific traffic. Read more about conditions  \nfor example, IOS users\nThe condition_strength targets a percentage of traffic matching a condition  \nfor example, 10% of IOS users\nThe weight for each available route (%) defines the distribution of all remaining traffic (not matching or not targetted by a condition)  \nfor example, 100% of all traffic, except the targetted 10% of IOS users\n\n Routes, condition-strength and weights\n\nEach route can have a weight and one or more conditions (see boolean expression in conditions). Each condition has a condition-strength.\n\nRouting is calculated as followed:\n\nFind the first condition that matches the request. Read more about conditions  \nfor example, IOS users\nIf the route exists, send the request to it depending on the condition strength  \nfor example, 10% of IOS users are sent to the route\nIf, based on condition strength, the request should not follow that route, then send request to one from all routes based on their weight.  \nfor example, all non-IOS traffic and 90% of IOS users are routed according to route weight\n\n  \nVamp has to account for all traffic.  \nWhen defining weights, the total weight of all routes must always add up to 100%.\nThis means that in a straight three-way split one service must be given 34% as 33%+33%+33%=99%.  1% can be a lot of traffic in high volume environments.\n  \n\nExample - Route all Firefox and only Firefox users to route service_B:\n\nservice_A:\n  weight: 100%\nservice_B:\n  weight: 0%\n  condition_strength: 100%\n  condition: user-agent == Firefox\n\n Example - Route half of Firefox users to serviceB, other half to serviceA (80%) or service_B (20%):\nNon Firefox requests will be just sent to serviceA (80%) or serviceB (20%).\nservice_A:\n  weight: 80%\nservice_B:\n  weight: 20%\n  condition_strength: 50%\n  condition: user-agent == Firefox\n\nExample - A/B test two deployments using route weight\nBelow is a basic example, similar to putting both deployments (sava:1.0.0 and sava:1.1.0) in the same cluster.  \nIt is easy to imagine having an older legacy application and the new one and doing a full canary release (or A/B testing) in seamless way by using gateways like this.\n\nDeployment 1: PUT /api/v1/deployments/sava:1.0\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nDeployment 2: PUT /api/v1/deployments/sava:1.1\n\n,name: sava:1.1\ngateways:\n  9060/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nGateway (90% / 10%): POST /api/v1/gateways\n\n,name: sava\nport: 9070/http\nroutes:\n  sava:1.0/sava/port:\n    weight: 90%           condition can be used as well\n  sava:1.1/sava/port:\n    weight: 10%\n\nURL path rewrite\n\nVamp supports URL path rewrite. This can be a powerful solution in defining service APIs (e.g. RESTful) outside of application service.  Path rewrite is defined in the format path: NEW_PATH if CONDITION, where:\n\nNEW_PATH new path to be used; HAProxy variables are supported, e.g. %[path]\nCONDITION condition using HAProxy directives, e.g. matching path, method, headers etc.\n\n Example\nroutes:\n  web/port1:\n    rewrites:\n    path: a if b\n  web/port2:\n    weight: 100%\n\nVamp managed and external routes\n\nVamp managed routes are in the format:\n\ngateway - pointing to another gateway, e.g. it is possible to chain gateways\ndeployment/cluster - pointing to deployment cluster, i.e. services are not 'visible'\ndeployment/cluster/service - pointing to specific service within deployment cluster\n\nAll examples above cover only Vamp managed routes.\nIt is also possible to route traffic to specific IP or hostname and port.\nIn that case IP or hostname and port need to be specified between brackets, e.g. [hostname:port] (and double quotes due to Yaml syntax).\n\nname: mesos\nport: 8080/http\nsticky: route\n\nroutes:\n  \"[192.168.99.100:5050]\":\n    weight: 50%\n  \"[localhost:5050]\":\n    weight: 50%\n\n  \nRead about Vamp conditions\nCheck the API documentation\nTry Vamp\n  ",
    "id": 67
  },
  {
    "path": "/documentation/using vamp/v0.9.1/references",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Referencing artifacts",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 110",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nWith any artifact, Vamp allows you to either use an inline notation or reference the artifact by name. For references, you use the reference keyword or its shorter version ref. Think of it like either using actual values or pointers to a value. This has a big impact on how complex or simple you can make any blueprint, breed or deployment. It also impacts how much knowledge you need to have of all the different artifacts that are used in a typical deployment or blueprint.\n\nVamp assumes that referenced artifcats (the breed called my_breed in the example below) is available to load from its datastore at deploy time. This goes for all basic artifacts in Vamp: SLA's, gateways, conditions, escalations, etc.\n\nExample - reference notation\n\ninline notation\n\n,name: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          name: my_breed\n          deployable: registry.example.com/app:1.0\n        scale:\n          cpu: 2\n          memory: 1024MB\n          instances: 4\nreference notation\n\n,name: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          reference: my_breed\n        scale:\n          reference: medium  \n\n Working with references\n\nWhen you begin to work with Vamp, you will probably start with inline artifacts. You have everything in one place and can directly see what properties each artifact has. Later, you can start specialising and basically build a library of often used architectural components. \n\nExample use of references\n\nCreate a library of containers\nFix scales per environment\nReuse a complex condition\n\n Example 1 \nCreate a library of containers\n\nUse case: You have a Redis container you have tweaked and setup exactly the way you want it. You want to use that exact container in all your environments (dev, test, prod etc.). \n\nImplementation: Put all that info inside a breed and use either the Vamp UI or API to save it (below). Now you can just use the ref: redis:1.0 notation anywhere in a blueprint.\n\nPOST /api/v1/breeds\n\n,name: redis:1.0\ndeployable: redis\nports: 6379/tcp\n\nExample 2\nFix scales per environment\n\nUse case: You want to have a predetermined set of scales you can use per deployment per environment. For instance, a \"mediumproduction\" should be something else than a \"mediumtest\".\n\nImplementation: Put all that info inside a scale and use either the Vamp API to save it (below). Now you can use the ref: mediumtest or ref: mediumprod notation anywhere a scale type is required.\n\nPOST /api/v1/scales\n\n,name: medium_prod\ncpu: 2\nmemory: 4096MB\ninstances: 3\n\n,name: medium_test\ncpu: 0.5\nmemory: 1024MB\ninstances: 1\n\n Example 3\nReuse a complex condition\n\nUse case: You have created a complex condition to target a specific part of your traffic. In this case users with a cookie that have a specific session variable set in that cookie. You want to use that condition now and then to do some testing. \n\nImplementation: Put all that info inside a condition and use either the Vamp API to save it (below). Now you can use the  ref: conditionemptyshopping_cart anywhere that condition is required.\n\n,name: conditionemptyshopping_cart\ncondition: Cookie SHOPSESSION Contains shoppingbasketitems=0 \n\n  \nRead about Vamp workflows\nCheck the API documentation\nTry Vamp\n  ",
    "id": 68
  },
  {
    "path": "/documentation/using vamp/v0.9.1/service-discovery",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Service discovery",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"service-discovery-2\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 135",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nVamp uses a service discovery pattern called server-side service discovery, which allows for service discovery without the need to change your code or run any other daemon or agent (microservices.io - server side discovery). In addition to service discovery, Vamp also functions as a service registry (microservices.io - service registry).\n\nFor Vamp, we recognise the following benefits of this pattern:\n\nNo code injection needed.\nNo extra libraries or agents needed.\nplatform/language agnostic: it’s just HTTP.\nEasy integration using ENV variables.\n\nCreate and publish a service\n\n  \nServices do not register themselves. They are explicitly created, registered in the Vamp database and provisioned on the load balancer.\n  \n\nServices are created and published as follows:\n\nThe user describes a service and its desired endpoint port in the Vamp DSL.\nThe service is deployed to the configured container manager by Vamp.\nVamp instructs Vamp Gateway Agent (via ZooKeeper, etcd or Consul) to set up service endpoints.\nVamp Gateway Agent takes care of configuring HAProxy, making the services available.\n\nAfter this, you can scale the service up/down or in/out either by hand or using Vamp’s auto scaling functionality. The endpoint is stable.\n\n Discovering a service\n\nSo, how does one service find a dependent service? Services are found by just referencing them in the DSL. Take a look at the following example:\n,name: my_blueprint:1.0\nclusters:\n  myfrontendcluster:\n    services:\n      breed:\n        name: myfrontendservice:0.1\n        deployable: company/frontend:0.1\n        ports:\n          port: 8080/http\n        dependencies:\n          backend: mybackendservice:0.3\n        environment_variables:\n         BACKEND_HOST: $backend.host\n         BACKEND_PORT: $backend.ports.jdbc\n      scale:\n        instances: 3         \n  mybackendcluster:\n    services:\n      breed:\n        name: mybackendservice:0.3\n        deployable: company/backend:0.3\n        ports:\n          jdbc: 8080/tcp\n      scale:\n        instances: 4\n\nWe have a frontend cluster and a backend cluster. These are just organisational units.\nThe frontend cluster runs just one version of our service, consisting of three instances.\nThe frontend service has a hard dependency on a backend (tcp) service.\nWe reference the backend by name, my_backend:0.3, and assign it a label, in this case just backend\nWe use the label backend to get the host and a specific port (jdbc) from this backend.\nWe assign these values to environment variables that are exposed in the container runtime.\nAny frontend service now has access to the location of the dependent backend service.\n\nNote that there is no point-to-point wiring. The $backend.host and $backend.ports.jdbc variables resolve to service endpoints Vamp automatically sets up and exposes.\n\nEven though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc.\n\nConfigure the gateway-driver\n\nThe gateway-driver section of the Vamp configuration file application.conf configures how traffic should be routed through Vamp Gateway Agent. See the below example on how to configure this:\n\nvamp  \n     \n   \n   \n\nThe reason for the need to configure vamp.gateway-driver.host is that when services are deployed, they need to be able to find Vamp Gateway Agent in their respective networks. This can be a totally different network than where Vamp is running.\nLet's use an example: frontend and backend service, frontend depends on backend - in Vamp DSL that would be 2 clusters (assuming the same deployment).\nThere are different ways how frontend can discover its dependency backend, and to make things simpler Vamp supports using specific environment parameters.\n\n,name: my-web-app\nclusters:\n  frontend:\n    services:\n      breed:\n        name: my-frontend:1.0.0\n        deployable: magneticio/my-frontend:1.0.0\n        ports:\n          port: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.port\n        dependencies:\n          backend: my-backend:1.0.0\n  backend:\n    services:\n      breed:\n        name: my-backend:1.0.0\n        deployable: magneticio/my-backend:1.0.0\n        ports:\n          port: 8080/http\n\nIn this example $backend.host will have the value of the vamp.gateway-driver.host configuration parameter, while $backend.ports.port the next available port from vamp.operation.gateway.port-range.\nfrontend doesn't connect to backend directly but via Vamp Gateway Agent(s) - given on these host and port parameters.\nThis is quite simmilar to common pattern to access any clustered application.\nFor instance if you want to access DB server, you will have an address string based on e.g. DNS name or something simmilar.\nNote that even without Vamp, you would need to setup access to backend in some similar way.\nWith Vamp, access is via VGA's and that allows specific routing (conditions, weights) needed for A/B testing and canary releasing.\n\n  \nRead about using Vamp with virtual hosts\nCheck the API documentation\nTry Vamp\n  ",
    "id": 69
  },
  {
    "path": "/documentation/using vamp/v0.9.1/sla",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "SLA (Service Level Agreement)",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 90",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nSLA stands for \"Service Level Agreement\". Vamp uses it to define a pre-described set of boundaries to a service and the actions that should take place once the service crosses those boundaries. In essence, an SLA and its associated escalation is a workflow that is checked and controlled by Vamp based on the runtime behaviour of a service. SLAs and escalations are defined with the VAMP DSL.\n\nThe SLA event system\n\nYou can define an SLA for each cluster in a blueprint. A common example would be to check if the average response time of the cluster (averaged across all services) is higher or lower than a certain threshold. Under the hood, an SLA workflow creates two distinct events. These are are sent from Vamp and stored to Elasticsearch.\n\nEscalate for a specific deployment and cluster  \ne.g. if the response time is higher than the upper threshold.\nDeEscalate for a specific deployment and cluster  \ne.g. if the response time is lower than the lower threshold.\n\nSLA monitoring is a continuous background process with a configurable interval time. On each run an SLA workflow is executed for each deployment & cluster that has an SLA defined. Within the same SLA definition it's possible to define a list of escalations. Escalations are triggered by escalation events (Escalate/DeEscalate).\n\nThis means escalation events can be generated by the third party systems by sending them to Elasticsearch. This would allow scaling up or down to be triggered by basically any system that can POST a piece of JSON.\n\nSLA's are in essence pieces of code inside Vamp that stick to this event model and can use, if they want, the metrics and event data streaming out of Elasticsearch to make decisions on how things are and should be running.\n\n SLA types\n\nVamp currently ships with the following SLA types:\n\nresponsetimesliding_window\n\nResponse time with sliding window \n\nThe responsetimesliding_window SLA triggers events based on response times. \n\n Example - SLA defined inline in a blueprint.\n\nNotice the SLA is defined per cluster and acts on the first service in the cluster.\n\nNotice how the SLA is defined separately from the escalations. This is key to how Vamp approaches SLA's and how modular and extendable the system is.\n\n,name: sava\n\ngateways:\n  80: sava/webport\n\nclusters:\n\n  sava:                        # the sava cluster\n    services:\n      breed:\n        name: monarch\n        deployable: vamp/monarch\n        ports:\n          webport: 80\n\n      scale:\n        cpu: 1\n        memory: 1024MB\n        instances: 2\n\n    sla:                        # SLA applies to the first service in the sava cluster (monarch)\n      # Type of SLA.\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000   # Upper threshold in milliseconds.\n        lower: 100    # Lower threshold in milliseconds.\n      window:\n        interval: 600 # Time period in seconds used for\n                      # average response time aggregation.\n        cooldown: 600 # Time period in seconds. During this \n                      # period no new escalation events will \n                      # be generated. New event may be expected \n                      # not before cooldown + interval time has \n                      # been reached after the last event. \n     \n      # List of escalations.\n      escalations:\n        type: scale_instances\n          minimum: 1\n          maximum: 3\n          scale_by: 1\n\n  \nRead about Vamp escalations\nCheck the API documentation\nTry Vamp\n  ",
    "id": 70
  },
  {
    "path": "/documentation/using vamp/v0.9.1/sticky-sessions",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Sticky Sessions",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 130",
    "content": "\n   \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nVamp supports route and instance level sticky sessions.\n\nRoute Level\n\nA common use case is when the end users have to have the same experience in A/B testing setup thus they should get the same service always (either A or B).\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: route                            setting the route level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nInstance Level \n\nA common use case is when the end users need to be served by the same instance (e.g. stateful application).\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: instance                          setting the instance level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nOther Notes\n\nResetting the sticky value can be done by: sticky: none or sticky: ~ (setting it to null).\n\nSticky sessions can be also used for gateways:\n\n,name: sava:1.0\ngateways:\n  9050/http:\n    sticky: service\n    routes:            let's say we have 2 clusters: sava1 (90%) and sava2 (10%)\n      sava1/port:   \n        weight: 90%\n      sava2/port:\n        weight: 10%\nclusters:\n  sava1: \n    ...\n  sava2: \n    ...\n\n  \nRead about using Vamp for service discovery\nCheck the API documentation\nTry Vamp\n  ",
    "id": 71
  },
  {
    "path": "/documentation/using vamp/v0.9.1/virtual-hosts",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Virtual Hosts",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 140",
    "content": "\n   \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nVamp can be configured to support virtual host via HAProxy:\n\nvamp.operation.gateway  \n\nExample - Virtual hosts\n \n PUT $ /api/v1/deployments/runner with body:\n\n,name: runner\n\ngateways:\n  9070: runner1/port\n  9080: runner2/port\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nNow you can request:\n\n$ curl --resolve 9070.runner.vamp:80:$  http://9070.runner.vamp\n \n\n$ curl --resolve 9080.runner.vamp:80:$  http://9080.runner.vamp\n \n  \nIf you are running Vamp in one of the quick setups, $  should have value of $  - See the hello world quick setup instructions.\n  \n\nVamp creates a virtual host for each gateway - name of the gateway (/ replaced with .) appended to value from vamp.gateway-driver.virtual-hosts-domain.\nIn case of above example:\n\n9050.runner.vamp\n9060.runner.vamp\nport.runner1.runner.vamp\nport.runner2.runner.vamp\n\nUsing Gateway API it is possible to get virtual hosts for each gateway, e.g.\nGET $ /api/v1/gateways\n\n Custom virtual hosts\n\nAs you could see each gateways has virtual_hosts field.\nUsing that field it is also possible to set list of custom virtual hosts.\nLet's see that in the following example:\n\n,name: runner\n\ngateways:\n  9080:\n    virtual_hosts: [\n      \"run.vamp.run\"\n    ]\n    routes:\n      runner1/port:\n        weight: 50%\n      runner2/port:\n        weight: 50%\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nIf you deploy this blueprint as runner and check gateway 9080/runner:\n\nGET $ /api/v1/gateways/runner/9080\n  http://run.vamp.run\n \n\n9080.runner.vamp is added if configuration parameter vamp.operation.gateway.virtual-hosts is set, otherwise just custom virtual hosts if any.\n\n  \nCheck the API documentation\nTry Vamp\n  ",
    "id": 72
  },
  {
    "path": "/documentation/using vamp/v0.9.1/workflows",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Workflows",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 120",
    "content": "\n  \n\nSwitch to the latest version of this page.\nRead the release notes for the latest Vamp release.\n  \n\nA \"workflow\" is an automated change of the running system and its deployments and gateways. \nChanging the number of running instances based on metrics (e.g. SLA) is an example of a workflow. \nA workflow can be seen as a recipe or solution, however it has a more generic meaning not just related to \"problematic\" situations.\n\nAnother example is a workflow that will decide automatically if a new version, when doing a canary release, should be accepted or not. \nFor instance, push the route up to 50% of traffic to the new version, compare metrics over some time (e.g. frequency of 5xx errors, response time), change to 100% and remove the old version. \nThis workflow could define the rate of the transitions (e.g. 5% - 10% - 25%, ...) as well.\n\nRationale\n\nWorkflows allow closing the feedback loop: deploy, measure, react.\nVamp workflows are based on running separate services (breeds) and in its simplest form scripting can be used - e.g. application/javascript breeds. \nScripting allows experimentation with different features and if the feature is common and generic enough, it could be supported later in Vamp DSL.\nSince workflows are running breeds in similar way as in deployments (blueprints), all other breed features are supported - ports, environment variables etc.\n\n Workflow API\n\nEach workflow is represented as an artifact and they follow basic CRUD operation patterns as any other artifact:\n  /api/v1/workflows\n\nEach workflow has to have:\n\n name\n breed - either reference or inline definition, similar to blueprints\n schedule \n scale - optional\n environment_variables (or env)- overrides breed environment variables\n arguments- Docker arguments, overrides default configuration arguments and breed arguments\n\nExample:\n\n,name: metrics\nbreed: metrics   # breed reference, inline definition can be also used\nschedule: daemon\nscale:           # inline scale, reference definition can be also used, e.g. scale: small\n  cpu: 1\n  memory: 128MB\n  instances: 2\nenvironment_variables:\n  interval: 5s\n\nSchedule\n\nFollowing schedule types are supported:\n\ndaemon\nevent with tags (set)\ntime - period, start (optional, by default starts now) and repeat (optional, by default runs forever) \n\nExamples:\n\n,schedule: daemon\n  \n time schedule\n\nschedule:\n  time:\n    period: P1Y2M3DT4H5M6S\n    start: now # or e.g. start: 2016-12-03T08:15:30Z\n    repeat: 10\n\nevent schedule\n\nschedule:\n  event:  event with following tags will trigger the workflow\n  deployments:sava\n  cluster:runner\n  \nor shorten notation in case of single event (still array can be used as above)\n\nschedule:\n  event: archive:bluprints\n\n      \nTime schedule period is in ISO8601 repeating interval notation.\n\nExample:\n\n,name    : metrics\nschedule: daemon\nbreed   :\n  name: metrics\n  deployable:\n    type: application/javascript\n    definition: |\n      'use strict';\n      \n      var _ = require('lodash');\n      var vamp = require('vamp-node-client');\n      \n      var api = new vamp.Api();\n      var metrics = new vamp.Metrics(api);\n      \n      var period = 5;  // seconds\n      var window = 30; // seconds\n      \n      var process = function()  , 'Tt', window, function(total, rate, responseTime)  );\n           );\n         );ß\n       ;\n      \n      setInterval(process, period * 1000);\n\n  \nProbably it would be better to keep breed as a reference and create breed as shown below:\n  \n\nPUT Content-Type: application/javascript /api/v1/breeds/metrics\n'use strict';\n\nvar _ = require('lodash');\nvar vamp = require('vamp-node-client');\n\nvar api = new vamp.Api();\nvar metrics = new vamp.Metrics(api);\n\nvar period = 5;  // seconds\nvar window = 30; // seconds\n\nvar process = function()  , 'Tt', window, function(total, rate, responseTime)  );\n       );\n   );\n ;\n\nsetInterval(process, period * 1000);\n\nJavaScript breeds will be executed by Vamp Workflow Agent (github.com/magneticio - Vamp workflow agent).  \n\nFor additional JavaScript API check out Vamp Node Client (github.com/magneticio - Vamp node client) project.\n\n  \nRead about Sticky sessions\nCheck the API documentation\nTry Vamp\n  ",
    "id": 73
  },
  {
    "path": "/documentation/using vamp/v0.9.2/artifacts",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Artifacts",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"artifacts-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 10",
    "aliases": "aliases:",
    "content": "\nVamp has a few basic entities or artifacts you can work with, these can be classed as static resource descriptions and dynamic runtime entities. Note that API actions on static resource descriptions are mostly synchronous, while API actions on dynamic runtime entities are largely asychronous.\n\nDynamic runtime entities\n\nDeployments are running blueprints. You can have many deployments from one blueprint and perform actions on each at runtime. Plus, you can turn any running deployment into a blueprint.  Read more...  \nGateways are the \"stable\" routing endpoint - defined by a port (incoming) and routes (outgoing).  Read more... \nWorkflows are apps (services) deployed on cluster, used for dynamically changing the runtime configuration (e.g. SLA, scaling, condition weight update).  Read more...\n\n Static resource descriptions\n\nBlueprints are, well, blueprints! They describe how breeds work in runtime and what properties they should have.  Read more...  \nBreeds describe single services and their dependencies.  Read more...\nScales define the size of a deployed service Read more...\n\nWorking across multiple teams\n\nIn larger companies with multiple teams working together on a large project, all required information is often not available at the same time. To facilitate this style of working, Vamp allows you to set placeholders. Placeholders let you communicate with other teams using simple references and gradually build up a complicated deployment. Vamp will only check references at deployment time, this means:\n\nBreeds can be referenced in blueprints before they exist \nYou do not need to know the contents of an SLA when you reference it.\nYou can reference a variable that someone else should fill in.\n\nRead more about referencing artifacts and environment variables.\n\n  \nRead about Vamp breeds\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 74
  },
  {
    "path": "/documentation/using vamp/v0.9.2/blueprints",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Blueprints",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"blueprints-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 30",
    "aliases": "aliases:",
    "content": "\nBlueprints are execution plans - they describe how your services should be hooked up and what their topology should look like at runtime. This means you reference your breeds (or define them inline) and add runtime configuration to them.\n\nBlueprints allow you to add the following extra properties:\n\nGateways: a stable port where the service can be reached.\nClusters and services: a cluster is a grouping of services with one purpose, i.e. two versions (a/b) of one service.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDialects: a dialect is a set of native commands for the underlying container platform, i.e. Docker or Mesosphere Marathon.\nScale: the CPU and memory and the amount of instance allocate to a service.\nConditions: how traffic should be directed based on HTTP and/or TCP properties.\nSLA and escalations: SLA definition that controls autoscaling.\n\nExample - key concepts of blueprints\n\n,name: my_blueprint                         Custom blueprint name\ngateways:\n  8080/http: my_frontend/port\nclusters:\n  my_frontend:                            # Custom cluster name.\n  \n    gateways:                             # Gateway for this cluster services.\n      routes:                             # Makes sense only with\n        somecoolbreed:                  # multiple services per cluster.\n          weight: 95%\n          condition: User-Agent = Chrome\n        someotherbreed:                 # Second service.\n          weight: 5%\n          \n    services:                             # List of services\n      breed:\n          ref: somecoolbreed\n        scale:                            # Scale for this service.\n          cpu: 2                          # Number of CPUs per instance.\n          memory: 2048MB                  # Memory per instance (MB/GB units).\n          instances: 2                    # Number of instances\n      breed: \n          ref: someotherbreed           # Another service in the same cluster.  \n        scale: large                      # Notice we used a reference to a \"scale\". \n                                          # More on this later.\n\nGateways\n\nA gateway is a \"stable\" endpoint (or port in simplified sense) that almost never changes. When creating the mapping, it uses the definition (my_frontend/port in this case) from the \"first\" service in the cluster definition you reference. This service can of course be changed, but the gateway port normally doesn't.\n\nPlease take care of setting the /tcp or /http (default) type for the port. Using /http allows Vamp to record more relevant metrics like response times and metrics.\n\nRead more about gateways.\n\n  \ngateways are optional. You can just deploy services and have a home grown method to connect them to some stable, exposable endpoint.\n  \n\n Clusters and services\n\nIn essence, blueprints define a collection of clusters.\nA cluster is a group of different services, which will appear as a single service and serve a single purpose.\n\nCommon use cases would be service A and B in an A/B testing scenario - usually just different\nversions of the same service (e.g. canary release or blue/green deployment).\n\nClusters are configured by defining an array of services. A cluster can be given an arbitrary name. Services are just lists or arrays of breeds.\n\n,mycoolcluster\n  services\n   breed: \n      ref: mycoolservice_A      # reference to an existing breed\n   breed:                       # shortened inline breed\n       name: mycoolservice_B\n       deployable: some_container\n       ...\n\nClusters and services are just organisational items. Vamp uses them to order, reference and control the actual containers and gateways and traffic.\n\n This all seems redundant, right? We have a reference chain of blueprints - gateways - clusters - services - breeds - deployable. However, you need this level of control and granularity in any serious environment where DRY principles are taken seriously and where \"one size fits all\" doesn't fly.\n\nDialects\n\nVamp allows you to use container driver specific tags inside blueprints. We call this a “dialect”.  Dialects effectively enable you to make full use of, for instance, the underlying features like mounting disks, settings commands and providing access to private Docker registries.\n\nWe currently support the following dialects:\n\ndocker:\nmarathon:\n\n Docker dialect\n\nThe following example show how you can mount a volume to a Docker container using the Docker dialect.\n\nExample blueprint - using the Docker dialect\n\n,name: busybox\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox-breed\n        deployable: busybox:latest\n      docker:\n        Volumes:\n          \"/tmp\": ~\n\nVamp will translate this into the proper API call. Inspecting the container after it's deployed should show something similar to this:\n\n...\n\"Volumes\":  ,\n...    \n\n Marathon dialect\n\nThis is an example with Marathon that pulls an image from private repo, mounts some volumes, sets some labels and gets run with an ad hoc command: all taken care of by Marathon.\n  \nWe can provide the marathon: tag either on the service level, or the cluster level. Any marathon: tag set on the service level will override the cluster level as it is more specific. However, in 9 out of 10 cases the cluster level makes the most sense. Later, you can also mix dialects so you can prep your blueprint for multiple environments and run times within one description.\n\nexample blueprint - using the Marathon dialect\n\nNotice the following:\n\nUnder the marathon: tag, we provide the command to run in the container by setting the cmd: tag.\nWe provide a url to some credentials file in the uri array. As described in the Marathon docs (mesosphere.github.io/marathon - using a private Docker repository) this enables Mesos\nto pull from a private registry, in this case registry.example.com where these credentials are set up.\nWe set some labels with some arbitrary metadata.\nWe mount the /tmp to in Read/Write mode.\n\n,name: busy-top:1.0\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox\n        deployable: registry.example.com/busybox:latest\n      marathon:\n       cmd: \"top\"\n       uris:\n         \"https://somehost/somepath/somefilewithdockercredentials\"\n       labels:\n         environment: \"staging\"\n         owner: \"buffy the vamp slayer\"\n       container:\n         volumes:\n           containerPath: \"/tmp/\"\n             hostPath: \"/tmp/\"\n             mode: \"RW\"\n\n Scale\n\nScale is the \"size\" of a deployed service. Usually that means the number of instances (servers) and allocated CPU and memory.\n\nScales can be defined inline in a blueprint or they can defined separately and given a unique name. The following example is a scale named \"small\". POST-ing this scale to the /scales REST API endpoint will store it under that name so it can be referenced from other blueprints.\n\nExample scale\n\n,name: small    Custom name.\n\ncpu: 2        # Number of CPUs per instance.\nmemory: 2gb   # Memory per instance, MB/GB units.\ninstances: 2  # Number of instances.\n\n  \nRead about Vamp deployments\nCheck the API documentation\nTry Vamp\n  \n\n",
    "id": 75
  },
  {
    "path": "/documentation/using vamp/v0.9.2/breeds",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Breeds",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"breeds-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 20",
    "aliases": "aliases:",
    "content": "\nBreeds are static descriptions of applications and services available for deployment. Each breed is described by the DSL in YAML notation or JSON, whatever you like. This description includes name, version, available parameters, dependencies etc.\nTo a certain degree, you could compare a breed to a Maven artifact or a Ruby Gem description.\n\nBreeds allow you to set the following properties:\n\nDeployable: the name of actual container or command that should be run.\nPorts: a map of ports your container exposes.\nDependencies: a list of other breeds this breed depends on.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\n\nDeployable\n\nDeployables are pointers to the actual artifacts that get deployed. Vamp supports Docker containers or can support any other artifacts supported by your container manager. \n\n Example breed - deploy a Docker container\n\n,name: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http   \n\nThis breed, with a unique name, describes a deployable and the port it works on. \n\nDocker deployables\n\nDocker images are pulled by your container manager from any of the repositories configured. By default that would be the public Docker hub, but it could also be a private repo.\nThe default deployable type is a Docker container, but we could also make this explicit by setting type to docker. The following statements are equivalent:\n\n,deployable: company/myfrontendservice:0.1\n\n,deployable: \n  type: container/docker\n  definition: company/myfrontendservice:0.1\n\nThis shows the full (expanded) deployable with type and definition.\n\n Other deployables\n\nRunning \"other\" artifacts such as zips or jars heavily depends on the underlying container manager.\nWhen Vamp is set up to run with Marathon (mesosphere.github.io - Marathon), command (or cmd) deployable types can be used.\nIn that case cmd (Marathon REST API - post v2/apps) parameter will have value of deployable.\n\nExample breed - run a custom jar after it has been downloaded \nCombining this definition and the Vamp Marathon dialect  uris parameter allows the requested jar to be downloaded from a remote location (Marathon REST API - uris Array of Strings). \n\n,name: location\nclusters:\n  api:\n    services:\n      breed:\n        name: location\n        deployable: \n          type: cmd\n          definition: java -jar location.jar\n      marathon:\n        uris: [\"https://myrepolocation_jar\"]\n\n JavaScript deployables \n\nBreeds can have type application/javascript and definition should be a JavaScript script:\n\n,name: hello-world\ndeployable:\n  type: application/javascript\n  definition: |\n    console.log('Hello World Vamp!');\n\nIt is possible to create or update breeds with the API request POST|PUT /api/v1/breeds/ , Javascript script as body and header Content-Type: application/javascript.\n\nPorts\n\nThe ports property is an array of named ports together with their protocol. It describes on what ports the deployables is offering services to the outside world. Let's look at the following breed:\n\n,name: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http\n  admin: 8081/http\n  redis: 9023/tcp   \n\nPorts come in two flavors:\n\n/http HTTP ports are the default type if none is specified. They are always recommended when dealing with HTTP-based services. Vamp can record a lot of interesting metrics like response times, errors etc. Of course, using /tcp will work but you miss out on cool data.\n/tcp Use TCP ports for things like Redis, MySQL etc.\n\n  \n/http notation for ports is required for use of filters.\n  \n\nNotice we can give the ports sensible names. This specific deployable has web port for customer traffic, an admin port for admin access and a redis port for some caching probably. These names come in handy when we later compose different breeds in blueprints.\n\n Dependencies\n\nBreeds can also have dependencies on other breeds. These dependencies should be stated explicitly, similar to how you would do in a Maven pom.xml, a Ruby Gemfile or similar package dependency systems, i.e:\n\n,dependencies:\n  cache: redis:1.1\n\nIt is also possible to use wildcard * at the end of the name. This will match any breed name that starts with redis:1.:\n\n,dependencies:\n  cache: redis:1.*\n\nIn a lot of cases, dependencies coexist with interpolated environment variables or constants because exact values are not known untill deploy time.  \nRead more about environment variables\n\n  \nRead about Vamp blueprints\nCheck the API documentation\nTry Vamp\n  \n\n",
    "id": 76
  },
  {
    "path": "/documentation/using vamp/v0.9.2/conditions",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Conditions",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"conditions-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 70",
    "aliases": "aliases:",
    "content": "\nConditions are used by gateways to filter incoming traffic for routing between services in a cluster.\nRead more about gateway usage. You can define conditions inline in a blueprint or store them separately under a unique name on the /conditions endpoint and just use that name to reference them from a blueprint. \n\nExample - simple inline condition\n\nThis would be used directly inside a blueprint.\n\n,condition_strength: 10%   Amount of traffic for this service in percents.\ncondition: User-Agent = IOS\n\nCreate a condition \n\nCreating conditions is quite easy. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv - 7.1 ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a condition.\n\nHowever, ACL's can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following:\n\n| description           | syntax                       | example                  |\n| ----------------------|:----------------------------:|:------------------------:|\n| match user agent      | user-agent == value          | user-agent == Firefox    |\n| mismatch user agent   | user-agent != value          | user-agent != Firefox    |\n| match host            | host == value                | host == localhost        |\n| mismatch host         | host != value                | host != localhost       |\n| has cookie            | has cookie value             | has cookie vamp          |\n| misses cookie         | misses cookie value          | misses cookie vamp       |\n| has header            | has header value             | has header ETag          |\n| misses header         | misses header value          | misses header ETag       |\n| match cookie value    | cookie name has value    | cookie vamp has 12345    |\n| mismatch cookie value | cookie name misses value | cookie vamp misses 12345 |\n| header has value      | header name has value   | header vamp has 12345    |\n| header misses value   | header name misses value | header vamp misses 12345 |\n\nAdditional syntax examples: github.com/magneticio/vamp - ConditionDefinitionParserSpec.scala.\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nIn order to specify plain HAProxy ACL, ACL needs to be between  :\n\ncondition: \" hdr_sub(user-agent) Chrome \"\n\nHaving multiple conditions in a condition is perfectly possible. For example, the following condition would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest and then it would check whether the request has the header\n\"X-VAMP-MY-COOL-HEADER\". So any request matching both conditions would go to this service.\n\n,gateways:\n  weight: 100%\n  condition: \"User-Agent = Chrome AND Has Header X-VAMP-MY-COOL-HEADER\"\n\nUsing a tool like httpie (github.com/jkbrzt/httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-MY-COOL-HEADER:stuff\n\n Boolean expression in conditions\n\nVamp supports AND, OR, negation NOT and grouping ( ):\n\n,gateways:\n  weight: 100%\n  condition: (User-Agent = Chrome OR User-Agent = Firefox) AND has cookie vamp\n\nAdditional boolean expression examples: github.com/magneticio/vamp - BooleanParserSpec.scala.\n\n  \nRead about Vamp events\nCheck the API documentation\nTry Vamp\n  \n\n",
    "id": 77
  },
  {
    "path": "/documentation/using vamp/v0.9.2/deployments",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Deployments",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"deployments-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 40",
    "aliases": "aliases:",
    "content": "\nA deployment is a \"running\" blueprint. Over time, new blueprints can be merged with existing deployments or parts of the running blueprint can be removed from it. Each deployment can be exported as a blueprint and \ncopy / pasted to another environment, or even to the same environment to function as a clone.\n\nCreate a deployment\n\nYou can create a deployment in the following ways:\n\nSend a POST request to the /deployments endpoint.\nUse the UI to deploy a blueprint using the \"deploy\" button on the \"blueprints\" tab.\nUse the CLI vamp deploy command  \n $ vamp deploy my_blueprint.\n\nThe name of the deployment will be automatically assigned as a UUID (e.g. 123e4567-e89b-12d3-a456-426655440000).\n\n Vamp deployment process\n\nOnce we have issued the deployment, Vamp will do the following:\n\nUpdate Vamps internal model.\nIssue and monitor deployment commands to the container platform.\nUpdate the ZooKeeper entry.\nStart collecting metrics.\nMonitor the container platform for changes.\n\nVamp will add runtime information to the deployment model, like start times, resolved ports etc.\n\nDeployment scenarios\n\nA common Vamp deployment scenario is to introduce a new version of the service to an existing cluster, this is what we call a merge. After testing/migration is done, the old or new version can be removed from the cluster, simply called a removal. Let's look at each in turn.\n\n Merge\n\nMerging of new services is performed as a deployment update. You can merge in many ways:\n\nSend a PUT request to the /deployments/  endpoint.\nUse the UI to update a deployment using the \"Edit deployment\" button. \nUse the CLI with a combination of the vamp merge and vamp deploy commands.\n\nIf a service already exists then only the gateways and scale will be updated. Otherwise a new service will be added. If a new cluster doesn't exist in the deployment, it will be added.\n\nLet's deploy a simple service:\n\n,name: monarch_1.0\n\nclusters:\n  monarch:\n    # Specifying only a reference to the breed.\n    breed: monarch_1.0   \n\nAfter this point we may have another version ready for deployment and now instead of only one service, we have added another one:\n\n,name: monarch_1.1\n\nenvironment_variables:\n  # Some variable needed for our new recommendation engine,\n  # just as an example.\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    # Just a reference and this breed has one dependency:\n    # recommendation_1.0\n    breed: monarch_1.1\n\n  recommendation:\n    breed: recommendation_1.0\n \n\nNow our deployment (in simplified blueprint format) looks like this:\n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 100%\n      monarch_1.1:\n        weight: 0%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    \n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nNote that the route weight for monarch_1.1 is 0, i.e. no traffic is sent to it.\nLet's redirect some traffic to our new monarch_1.1 (e.g. 10%):\n\n,clusters:\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 90%\n      monarch_1.1:\n        weight: 10%\n\nNote that we can omit other fields like name, parameters and even other clusters (e.g. recommendation) if the change is not relevant to them. In this example we just wanted to update the weights.\n\nIn the last few examples we have shown the following:\n\nA fresh new deployment.\nA canary release with a cluster update and change of the topology (a new cluster was added).\nAn update of the gateways for a cluster - similar to a cluster scale update (instances, cpu, memory).\n\nRemoval\n\nRemoval is done using the REST API DELETE request together with the new blueprint as request body.\nIf a service exists it will be removed, otherwise the request is ignored. If a cluster has no more services left the cluster will be removed completely. Lastly, if a deployment has no more clusters it will be completely removed (destroyed).\n\nLet's use the example from the previous section. Notice the weight is evenly distributed (50/50). \n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 50%\n      monarch_1.1:\n        weight: 50%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100\n\nIf we are happy with the new monarch version 1.1, we can proceed with the removal of the old version.\nThis change is applied on the running deployment. We send the following YAML as the body of the DELETE request\nto the /deployments/deployment_UUID endpoint.\n\n,name: monarch_1.0\n\nclusters:\n  monarch:\n    breed: monarch_1.0\n\nNote that this is the same original blueprint we started with. What we are doing here is basically \"subtracting\" one blueprint from the other, although \"the other\" is a running deployment.\nAfter this operation our deployment is:\n\n,name: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.1\n    gateways:\n      monarch_1.1:\n        weight: 100%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nIn a nutshell: If we say that the first version was A and the second B, then we just did the migration from A to B without downtime:\nA** - A + B - A + B - A - **B\n\nWe could also remove the newer version (monarch_1.1 with/without recommendation cluster) in case that it didn't perform as we expected.\n\n  \nRead about Vamp environment variables\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 78
  },
  {
    "path": "/documentation/using vamp/v0.9.2/environment-variables",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Environment variables",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"environment-variables-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 50",
    "aliases": "aliases:",
    "content": "\nBreeds, blueprints and workflows can include a list of environment variables to be injected into the container at runtime. You set environment variables with the environment_variables keyword or its shorter version env, e.g. both examples below are equivalent.\n\n,environment_variables:\n  PORT: 8080\n\n,env:\n  PORT: 8080\n\n'Hard' setting a variable\n\nYou want to \"hard set\" an environment variable, just like doing an export MYVAR=somevalue in a shell. This  variable could be some external dependency you have no direct control over: the endpoint of some service you use that is out of your control. It can also be some setting you want to tweak, like JVMHEAPSIZE or AWS_REGION.\n\n,name: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\n\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1'     \n\n Place holders\n\nPlace holders can be used to separate responsibilities across a company where different roles are working on the same project. For example, developers can create place holders for variables to be filled in by operations.\n\nUse the ~ character to define a place holder for a variable that should be filled in at runtime (i.e. when this breed actually gets deployed), but for which you do not yet know the actual value. \n\nExample - ORACLE_PASSWORD designated as a place holder\n\n,name: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1' \n  ORACLE_PASSWORD: ~    \n\n Resolving variables\n\nUse the $ character to reference other statements in a breed/blueprint. This allows you to dynamically resolve ports and hosts names that we don't know until a deployment is done. You can also resolve to hard coded and published constants from some other part of the blueprint or breed, typically a dependency.\n\n  \nThe $ value is escaped by $$. A more strict notation is $ \n  \n\nVamp host variable\n\nVamp provides just one magic* variable: host. This resolves to the host or ip address of the referenced service. Strictly speaking, the host reference resolves to the gateway agent endpoint, but users do not need to concern themselves with this. Users can think of one-on-one connections where Vamp actually does server-side service discovery to decouple services.\n\n Example - resolving variables from a dependency\n\n,name: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host        # resolves to a host at runtime\n      MYSQL_PORT: $backend.ports.port  # resolves to a port at runtime\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n\nExample - resolving variables from a dependency's environment variables\nWhat if the backend is configured through some environment variable, but the frontend also needs that information? For example, the encoding type for our database. We can just reference that environment variable using the exact same syntax.\n\n,name: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host       \n      MYSQL_PORT: $backend.ports.port \n      BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n    environment_variables:\n      ENCODING_TYPE: 'UTF8'    injected into the backend MySQL container\n\nYou can do everything with environment_variables but constants (see below) allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\nEnvironment variable scope\n\nEnvironment variables can live on different scopes and can be overridden by scopes higher in the scope hierarchy.\nA scope is an area of your breed or blueprint definition that limits the visibility of variables and references inside that scope.\n\nBreed scope: The scope used in all the above examples is the default scope. If you never define any environment_variables in any other place, this will be used.\n\nCluster scope: Will override the breed scope and is part of the blueprint artifact. Use this to override environment variables for all services that belong to a cluster.\n\nService scope: Will override breed scope and cluster scope, and is part of the blueprint artifact. Use this to override all environment variables for a specific service within a cluster.\n\n   \nEffective use of scope is completely dependent on your use case. The various scopes help to separate concerns when multiple people and/or teams work on Vamp artifacts and deployments and need to decouple their effort.\n  \n\n Examples of scope use\n\nRun two of the same services with different configurations\nOverride the JVMHEAPSIZE in production\nUse a place holder\nJust for fun - combine all scopes and references\n\nExample 1\nRun two of the same services with different configurations\n\nUse case: As a devOps-er you want to test one service configured in two different ways at the same time. Your service is configurable using environment variables. In this case we are testing a connection pool setting. \n\nImplementation: In the below blueprint we just use the breed level environment variables. The traffic is split into a 50/50 divide between both services.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0-a\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 30                \n      breed:\n          name: frontend_app:1.0-b               different breed name, same deployable\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 60                 # different pool size\n\n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nExample 2\nOverride the JVM_HEAP_SIZE in production\n\nUse case: As a developer, you created your service with a default heap size you use on your development laptop and maybe on a test environment. Once your service goes \"live\", an ops guy/gal should be able to override this setting.\n\nImplementation: In the below blueprint we override the variable JVMHEAPSIZE for the whole frontend cluster by specifically marking it with .dot-notation cluster.variable \n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                   cluster level variable \n  frontend.JVMHEAPSIZE: 2800          # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            JVMHEAPSIZE: 1200         # will be overridden by deployment level: 2800\n\nExample 3\nUse a place holder\n\nUse case: As a developer, you might not know some value your service needs at runtime, say the Google Anaytics ID your company uses. However, your Node.js frontend needs it! \n\nImplementation: In the below blueprint the ~ place holder is used to explicitly demand that a variable is set by a higher scope. When this variable is NOT provided, Vamp will report an error at deploy time.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.GOOGLEANALYTICSKEY: 'UA-53758816-1'  # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/node_express:1.0\n          ports:\n            port: 8080/http\n          environment_variables:           \n            GOOGLEANALYTICSKEY: ~               # If not provided at higher scope, \n                                                  # Vamp reports error.\n\nExample 4\nCombine all scopes and references\n\nAs a final example, let's combine some of the examples above and include referenced breeds. In this case, we have two breed artifacts already stored in Vamp and include them by using the ref keyword.\n\nIn the below blueprint:  \n\nwe override all breed scope JVMHEAPSIZE variables with cluster scope environment_variables\nto further tweak the JVMHEAPSIZE for the service frontendapp:1.0-b, we also add service scope environmentvariables for that service.\n\n,name: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.JVMHEAPSIZE: 2400                    # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          ref: frontend_app:1.0-a\n      breed:\n          ref: frontend_app:1.0-b        \n        environment_variables:           \n          JVMHEAPSIZE: 1800               # overrides the breed level AND cluster level\n          \n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nConstants\n\nSometimes you just want configuration information to be available in a breed or blueprint. You don't need that information to be directly exposed as an environment variable. As a convenience, Vamp allows you to set constants.\nThese are values that cannot be changed during deploy time.\n\nYou can do everything with environment_variables but constants allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\n Exammple - using constants\n\n,clusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      environment_variables:\n        MYSQL_HOST: $backend.host       \n        MYSQL_PORT: $backend.ports.port \n        BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n        SCHEMA: $backend.constants.SCHEMA_NAME\n      dependencies:\n        backend: mysql:1.0\n  backend:\n    breed:\n      name: mysql:1.0\n      environment_variables:\n        ENCODING_TYPE: 'UTF8'\n      constants:\n        SCHEMA_NAME: 'customers'    # NOT injected into the backend MySQL container\n\n  \nRead about Vamp gateways\nCheck the API documentation\nTry Vamp\n  ",
    "id": 79
  },
  {
    "path": "/documentation/using vamp/v0.9.2/escalations",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Escalations",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"escalations-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 100",
    "aliases": "aliases:",
    "content": "\nAn escalation is a workflow triggered by an escalation event. Vamp checks for these escalation events using a continuous background process with a configurable interval time. If the events match the escalation handlers defined in the DSL, the action is executed.\n\nAgain: escalation events can be generated by third party systems and they will be handled in the same manner as events created by Vamp SLA workflows. \n\nEscalation handlers\n\nAny escalation that is triggered should be handled by an escalation handler\n\nVamp ships with the following set of escalation handlers - scaleinstances, scalecpu and scale_memory. These handlers can be composed into intricate escalation systems.\n\n scale_instances   \nScales up the number of running instances. It is applied only to the first service in the cluster (old or \"A\" version). You can set upper limits to how far you want to scale out or in, effectively guaranteeing a minimum set of running instances. This is very much like AWS auto-scaling.  \nExample - scale_instances\n,type: scale_instances\ntarget: monarch   Target cluster for the scale up/down.\n                 # If it's not specified, by default it's the \n                 # current cluster where SLA escalations are \n                 # specified.\nminimum: 1       # Minimum number of instances.\nmaximum: 3       # Maximum number of instances.\nscale_by: 1      # Increment/decrement to use on current \n                 # number of running instances.\nscale_cpu \nScales up the number of CPUs per instances. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_cpu\n,type: scale_cpu\ntarget: monarch  \nminimum: 1\nmaximum: 3 \nscale_by: 0.5\nscale_memory   \nScales up the memory per instance. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_memory  \n,type: scale_memory\ntarget: monarch  \nminimum: 512     # In MB.\nmaximum: 4096    # In MB.\nscale_by: 512    # In MB.\n\nComposing escalation handlers\n\nVamp has a set of predefined escalation handler types that deal with escalations. You can compose these handlers in the DSL to get the desired outcome of an escalation event. The escalation handlers toall and toone are supported:\n\n to_all  \nThis is a \"group\" escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to all escalation handlers from its list. No order or hierarchy.    \n\nExample - to_all  \n,to_all:\n  escalations:\n     Scale up/down.\n    scale_instances\n    # And notify for each event.\n    notify\nto_one  \nThis is a group escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to each escalation handler (from its list) until one can handle it. On an Escalate event, it will start at the head of the list. During a DeEscalate event is will start at the rear.  \nWhen is this useful? Well, Vamp could try to scale up the service and if that;s not possible anymore (e.g. reached the upper limit of allowed scale) then an email can be sent.  \n\n Example - simple use of to_one\n,to_one:\n  escalations:\n    # First try to escalate.\n    scale_instances\n    # If it's not possible, proceed with notifying.\n    notify\nExample - complex use of to_one  \n,name: monarch\n\ngateways:\n  80: monarch1/port\n\nenvironment_variables:\n  monarch2.password: secret\n\nclusters:\n\n  monarch1:\n    breed:\n      name: monarch1\n      deployable: vamp/monarch1\n      ports:\n        port: 80/http\n      environment_variables:\n        STORAGE_HOST: $storage.host\n        DB_PORT: $storage.ports.port\n        STORAGEPASS: $storage.environmentvariables.password\n      \n      dependencies:\n        storage: monarch2\n\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n    \n    sla:\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000\n        lower: 100\n      window:\n        interval: 600\n        cooldown: 600\n      escalations:\n        to_one:\n            escalations:\n              type: scale_instances\n                 First try to scale up storage service.\n                target: monarch2\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              type: scale_instances\n                # If we cannot scale up storage anymore, scale up this.\n                target: monarch1\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              \n  monarch2:\n    breed:\n      name: monarch2\n      deployable: vamp/monarch2\n      ports:\n        port: 3306\n      environment_variables:\n        STORAGE_PASS: password\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n\n  \nRead about Referencing artifacts in Vamp\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 80
  },
  {
    "path": "/documentation/using vamp/v0.9.2/events",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Events",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"events-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 80",
    "aliases": "aliases:",
    "content": "\nVamp is a distributed system tied together by a central events stream. Every action in Vamp creates events, which in turn can be used as triggers for new actions. For example, gateway updates are triggered by deployments (synchronisation events), while canary releases and autoscaling actions are based on calculated health and metrics events. Vamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions. All events are stored and retrieved using the Event API that is part of Vamp.\n\nExample - JSON \"deployment create\" event\n\n \n Storing events\nVamp events are stored by default in Elasticsearch using the integrated Vamp Pulse module. Elasticsearch indexing is based on the event type and is updated for each event received by the API. Each event type will be indexed individually, including custom event types  .\n\nBasic event rules\n\nAll events stick to some basic rules:\n\nAll data in Vamp are events. \nEvents consist of an event type, one or more event tags and an optional event value. They are described in the format ['tag', 'tag1:tag2'], value, event_type \nEvent values can be empty or any JSON object.\nEvent timestamps are in ISO8601/RFC3339.\nTimestamps are optional. If not provided, Vamp will insert the current time.\nTimestamps are inclusive for querying.\nEvents can be tagged with metadata. A simple event tag is just single string.\nQuerying data by tag assumes \"AND\" behaviour when multiple tags are supplied, i.e. [\"one\", \"two\"] would only fetch records that are tagged with both.\nSupported event aggregations are: average, min, max and count.\nYou can create custom event types, names should only include alphanumerics, ‘_’ and ‘-’\n\n Event type\nVamp works with a number of default event types and custom event types can also be created using the /events API endpoint (more on that later). If no event type is specified when creating an event, the generic event type event will be used. It is advisable to always specify an event type to allow for easy filtering.\n\nDefault event types |  Description\n----------|-----,Archive     |    Store/update of static Vamp artifacts (breeds, blueprints etc.)\nSynchronisation | Successful matching of a desired state with an observed state (for example, a successful update from 3 to 5 running instances)\nEvent | Generic event type, will be used if no event type is specified when creating an event\nHealth | Generated by the health workflow and used by the Vamp UI\nMetrics | Generated by the metrics workflow and used by the Vamp UI\nWorkflow | Generated by workflows used in Vamp Runner\n\nEvent value\nThe event value is optional and could be anything you choose to store along with an event. Values are not analysed and can't be used for search. If no value is specified, it will be blank\n\n Event tags\nEach event created by Vamp is given one or more tags. Tags provide meta-information for stored events and are used for filtering searches and listening. Tags can be either a single tag or a combination of two tags separated by a :. The Vamp convention is to use the first tag as a generic label (for example a group) and the second tag as a specific label (i.e. a specific item in a group). generictag:specifictag\n\nHow tags are organised\n\nIn all of Vamp's components we follow a REST (resource oriented) schema, for instance:\n/deployments/  \n/deployments/ /clusters/ /services/ \nTagging is done using a very similar schema: \" \", \" : \". For example:\n\n[\n    \"deployments\", \n    \"deployments: \", \n    \"clusters\", \n    \"clusters: \", \n    \"services\", \n    \"services: \"\n]\n\nThis schema allows querying per group and per specific name. Getting all events related to all deployments is done by using tag \"deployments\". Getting events for specific deployment \"deployments: \".\n\n Query events using tags\n\nUsing the tags schema and timestamps, you can do some powerful queries. Either use an exact timestamp or use special range query operators, described on the elastic.co site (elastic.co - Range query).\n\n  \nthe default page size for a set of returned events is 30.\n  \n\nExample queries\n\nGet all events\nResponse time for a cluster\nCurrent sessions for a service\nAll known events for a service\n \n Example 1\nGet all events\n\nThe below query gets ALL metrics events up till now, taking into regard the pagination.\n\n  \nGET request with body - similar to approach used by Elasticsearch.\n  \n\nGET /api/v1/events\n\n \n \n\nExample 2 \nResponse time for a cluster\n\nThe below query gets the most recent response time events for the \"frontend\" cluster in the \"d9b42796-d8f6-431b-9230-9d316defaf6d\" deployment.\n\nNotice the \"gateways:UUID\", \"metrics:responseTime\" and \"gateways\" tags. This means \"give me the response time of this specific gateway at the gateway level\". The response will echo back the events in the time range with the original set of tags associated with the events. \n\nGET /api/v1/events\n\n \n \n\n[\n     ,\n     \n]    \n\n Example 3\nCurrent sessions for a service\n\nAnother example is getting the current sessions for a specific service, in this case the monarch_front:0.2 service that is part of the 214615ec-d5e4-473e-a98e-8aa4998b16f4 deployment and lives in the frontend cluster.\n\nNotice we made the search more specific by specifying the \"services\" and then \"service:SERVICE NAME\" tag.\nAlso, we are using relative timestamps: anything later or equal (lte) than \"now\".\n\nGET /api/v1/events\n\n \n \n\nExample 4\nAll known events for a service\n\nThis below query gives you all the events we have for a specific service, in this case the same service as in example 2. In this way you can get a quick \"health snapshot\" of service, server, cluster or deployment.\n\nNotice we made the search less specific by just providing the \"metrics\" tag and not telling the API which specific one we want.\n\nGET /api/v1/events\n\n \n \n\n Server-sent events (SSE)\n\nEvents can be streamed back directly from Vamp.\n\nGET /api/v1/events/stream\n\nIn order to narrow down (filter) events, list of tags could be provided in the request body.\n\n \n\nGET method can be also used with tag parameter (may be more convenient):\n\nGET /api/v1/events/stream?tag=archiving&tag=breeds\n\nArchiving\n\nAll changes in artifacts (creation, update or deletion) triggered by REST API calls are archived. We store the type of event and the original representation of the artifact. It's a bit like a Git log. \n\nHere is an example event:\n\n \n\nSearching through the archive is 100% the same as searching for events. The same tagging scheme applies.\nThe following query gives back the last set of delete actions executed in the Vamp API, regardless of the artifact type.\n\nGET /api/v1/events\n\n \n \n\n  \nTry the tutorial Create a workflow that generates events\nRead about Vamp SLA (Service Level Agreement)\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 81
  },
  {
    "path": "/documentation/using vamp/v0.9.2/gateways",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Gateways",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"gateways-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 60",
    "aliases": "aliases:",
    "content": "\nGateways are dynmic runtime entities in the Vamp eco-system. They represent load balancer rules to deployment, cluster and service instances. There are two types of gateways:\n\nInternal gateways are created automatically for each deployment cluster and updated using the gateway/deployment API\nExternal gateways are explicitly declared either in a deployment blueprint or using the gateway API\n\nExample - automatically created gateway \n\nThe below gateway is for deployment vamp, cluster sava and port port.  \nThe cluster contains two services sava:1.0.0 and sava:1.1.0, each with two running instances. \n,name: vamp/sava/port            name\nport: 40000/http               # port, either http or tcp, assigned by Vamp\nactive: true                   # is it running - not in case of non (yet) existing routes\nsticky: none\nroutes:                        # routes\n  vamp/sava/sava:1.0.0/port:\n    weight: 50%\n    instances:\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda3c376-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31463\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda2d915-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31292\n  vamp/sava/sava:1.1.0/port:\n    weight: 50%\n    instances:\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa3c9e4-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31634\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa37bc3-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31826\n\nGateway Usage\n\nThe gateway API allows for programmable routing. External gateways give an entry point to clusters (optionally specified in deployment blueprints), and allow for canary releasing and A/B testing across deployments.\n\nA gateway defines a set of rules for routing traffic between different services within the same cluster.\nVamp allows you to determine this in three ways:\n\nA condition will target specific traffic. Read more about conditions  \nfor example, IOS users\nThe condition_strength targets a percentage of traffic matching a condition  \nfor example, 10% of IOS users\nThe weight for each available route (%) defines the distribution of all remaining traffic (not matching or not targetted by a condition)  \nfor example, 100% of all traffic, except the targetted 10% of IOS users\n\n Routes, condition-strength and weights\n\nEach route can have a weight and one or more conditions (see boolean expression in conditions). Each condition has a condition-strength.\n\nRouting is calculated as followed:\n\nFind the first condition that matches the request. Read more about conditions  \nfor example, IOS users\nIf the route exists, send the request to it depending on the condition strength  \nfor example, 10% of IOS users are sent to the route\nIf, based on condition strength, the request should not follow that route, then send request to one from all routes based on their weight.  \nfor example, all non-IOS traffic and 90% of IOS users are routed according to route weight\n\n  \nVamp has to account for all traffic.  \nWhen defining weights, the total weight of all routes must always add up to 100%.\nThis means that in a straight three-way split one service must be given 34% as 33%+33%+33%=99%.  1% can be a lot of traffic in high volume environments.\n  \n\nExample - Route all Firefox and only Firefox users to route service_B:\n\nservice_A:\n  weight: 100%\nservice_B:\n  weight: 0%\n  condition_strength: 100%\n  condition: user-agent == Firefox\n\n Example - Route half of Firefox users to serviceB, other half to serviceA (80%) or service_B (20%):\nNon Firefox requests will be just sent to serviceA (80%) or serviceB (20%).\nservice_A:\n  weight: 80%\nservice_B:\n  weight: 20%\n  condition_strength: 50%\n  condition: user-agent == Firefox\n\nExample - A/B test two deployments using route weight\nBelow is a basic example, similar to putting both deployments (sava:1.0.0 and sava:1.1.0) in the same cluster.  \nIt is easy to imagine having an older legacy application and the new one and doing a full canary release (or A/B testing) in seamless way by using gateways like this.\n\nDeployment 1: PUT /api/v1/deployments/sava:1.0\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nDeployment 2: PUT /api/v1/deployments/sava:1.1\n\n,name: sava:1.1\ngateways:\n  9060/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nGateway (90% / 10%): POST /api/v1/gateways\n\n,name: sava\nport: 9070/http\nroutes:\n  sava:1.0/sava/port:\n    weight: 90%           condition can be used as well\n  sava:1.1/sava/port:\n    weight: 10%\n\nURL path rewrite\n\nVamp supports URL path rewrite. This can be a powerful solution in defining service APIs (e.g. RESTful) outside of application service.  Path rewrite is defined in the format path: NEW_PATH if CONDITION, where:\n\nNEW_PATH new path to be used; HAProxy variables are supported, e.g. %[path]\nCONDITION condition using HAProxy directives, e.g. matching path, method, headers etc.\n\n Example\nroutes:\n  web/port1:\n    rewrites:\n    path: a if b\n  web/port2:\n    weight: 100%\n\nVamp managed and external routes\n\nVamp managed routes are in the format:\n\ngateway - pointing to another gateway, e.g. it is possible to chain gateways\ndeployment/cluster - pointing to deployment cluster, i.e. services are not 'visible'\ndeployment/cluster/service - pointing to specific service within deployment cluster\n\nAll examples above cover only Vamp managed routes.\nIt is also possible to route traffic to specific IP or hostname and port.\nIn that case IP or hostname and port need to be specified between brackets, e.g. [hostname:port] (and double quotes due to Yaml syntax).\n\nname: mesos\nport: 8080/http\nsticky: route\n\nroutes:\n  \"[192.168.99.100:5050]\":\n    weight: 50%\n  \"[localhost:5050]\":\n    weight: 50%\n\n  \nRead about Vamp conditions\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 82
  },
  {
    "path": "/documentation/using vamp/v0.9.2/references",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Referencing artifacts",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"referencing-artifacts-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 110",
    "aliases": "aliases:",
    "content": "\nWith any artifact, Vamp allows you to either use an inline notation or reference the artifact by name. For references, you use the reference keyword or its shorter version ref. Think of it like either using actual values or pointers to a value. This has a big impact on how complex or simple you can make any blueprint, breed or deployment. It also impacts how much knowledge you need to have of all the different artifacts that are used in a typical deployment or blueprint.\n\nVamp assumes that referenced artifcats (the breed called my_breed in the example below) is available to load from its datastore at deploy time. This goes for all basic artifacts in Vamp: SLA's, gateways, conditions, escalations, etc.\n\nExample - reference notation\n\ninline notation\n\n,name: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          name: my_breed\n          deployable: registry.example.com/app:1.0\n        scale:\n          cpu: 2\n          memory: 1024MB\n          instances: 4\nreference notation\n\n,name: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          reference: my_breed\n        scale:\n          reference: medium  \n\n Working with references\n\nWhen you begin to work with Vamp, you will probably start with inline artifacts. You have everything in one place and can directly see what properties each artifact has. Later, you can start specialising and basically build a library of often used architectural components. \n\nExample use of references\n\nCreate a library of containers\nFix scales per environment\nReuse a complex condition\n\n Example 1 \nCreate a library of containers\n\nUse case: You have a Redis container you have tweaked and setup exactly the way you want it. You want to use that exact container in all your environments (dev, test, prod etc.). \n\nImplementation: Put all that info inside a breed and use either the Vamp UI or API to save it (below). Now you can just use the ref: redis:1.0 notation anywhere in a blueprint.\n\nPOST /api/v1/breeds\n\n,name: redis:1.0\ndeployable: redis\nports: 6379/tcp\n\nExample 2\nFix scales per environment\n\nUse case: You want to have a predetermined set of scales you can use per deployment per environment. For instance, a \"mediumproduction\" should be something else than a \"mediumtest\".\n\nImplementation: Put all that info inside a scale and use either the Vamp API to save it (below). Now you can use the ref: mediumtest or ref: mediumprod notation anywhere a scale type is required.\n\nPOST /api/v1/scales\n\n,name: medium_prod\ncpu: 2\nmemory: 4096MB\ninstances: 3\n\n,name: medium_test\ncpu: 0.5\nmemory: 1024MB\ninstances: 1\n\n Example 3\nReuse a complex condition\n\nUse case: You have created a complex condition to target a specific part of your traffic. In this case users with a cookie that have a specific session variable set in that cookie. You want to use that condition now and then to do some testing. \n\nImplementation: Put all that info inside a condition and use either the Vamp API to save it (below). Now you can use the  ref: conditionemptyshopping_cart anywhere that condition is required.\n\n,name: conditionemptyshopping_cart\ncondition: Cookie SHOPSESSION Contains shoppingbasketitems=0 \n\n  \nRead about Vamp workflows\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 83
  },
  {
    "path": "/documentation/using vamp/v0.9.2/service-discovery",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Service discovery",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"service-discovery-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 135",
    "aliases": "aliases:",
    "content": "Vamp uses a service discovery pattern called server-side service discovery, which allows for service discovery without the need to change your code or run any other daemon or agent (microservices.io - server side discovery). In addition to service discovery, Vamp also functions as a service registry (microservices.io - service registry).\n\nFor Vamp, we recognise the following benefits of this pattern:\n\nNo code injection needed.\nNo extra libraries or agents needed.\nplatform/language agnostic: it’s just HTTP.\nEasy integration using ENV variables.\n\nCreate and publish a service\n\n  \nServices do not register themselves. They are explicitly created, registered in the Vamp database and provisioned on the load balancer.\n  \n\nServices are created and published as follows:\n\nThe user describes a service and its desired endpoint port in the Vamp DSL.\nThe service is deployed to the configured container manager by Vamp.\nVamp instructs Vamp Gateway Agent (via ZooKeeper, etcd or Consul) to set up service endpoints.\nVamp Gateway Agent takes care of configuring HAProxy, making the services available.\n\nAfter this, you can scale the service up/down or in/out either by hand or using Vamp’s auto scaling functionality. The endpoint is stable.\n\n Discovering a service\n\nSo, how does one service find a dependent service? Services are found by just referencing them in the DSL. Take a look at the following example:\n,name: my_blueprint:1.0\nclusters:\n  myfrontendcluster:\n    services:\n      breed:\n        name: myfrontendservice:0.1\n        deployable: company/frontend:0.1\n        ports:\n          port: 8080/http\n        dependencies:\n          backend: mybackendservice:0.3\n        environment_variables:\n         BACKEND_HOST: $backend.host\n         BACKEND_PORT: $backend.ports.jdbc\n      scale:\n        instances: 3         \n  mybackendcluster:\n    services:\n      breed:\n        name: mybackendservice:0.3\n        deployable: company/backend:0.3\n        ports:\n          jdbc: 8080/tcp\n      scale:\n        instances: 4\n\nWe have a frontend cluster and a backend cluster. These are just organisational units.\nThe frontend cluster runs just one version of our service, consisting of three instances.\nThe frontend service has a hard dependency on a backend (tcp) service.\nWe reference the backend by name, my_backend:0.3, and assign it a label, in this case just backend\nWe use the label backend to get the host and a specific port (jdbc) from this backend.\nWe assign these values to environment variables that are exposed in the container runtime.\nAny frontend service now has access to the location of the dependent backend service.\n\nNote that there is no point-to-point wiring. The $backend.host and $backend.ports.jdbc variables resolve to service endpoints Vamp automatically sets up and exposes.\n\nEven though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc.\n\nConfigure the gateway-driver\n\nThe gateway-driver section of the Vamp configuration file application.conf configures how traffic should be routed through Vamp Gateway Agent. See the below example on how to configure this:\n\nvamp  \n     \n   \n   \n\nThe reason for the need to configure vamp.gateway-driver.host is that when services are deployed, they need to be able to find Vamp Gateway Agent in their respective networks. This can be a totally different network than where Vamp is running.\nLet's use an example: frontend and backend service, frontend depends on backend - in Vamp DSL that would be 2 clusters (assuming the same deployment).\nThere are different ways how frontend can discover its dependency backend, and to make things simpler Vamp supports using specific environment parameters.\n\n,name: my-web-app\nclusters:\n  frontend:\n    services:\n      breed:\n        name: my-frontend:1.0.0\n        deployable: magneticio/my-frontend:1.0.0\n        ports:\n          port: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.port\n        dependencies:\n          backend: my-backend:1.0.0\n  backend:\n    services:\n      breed:\n        name: my-backend:1.0.0\n        deployable: magneticio/my-backend:1.0.0\n        ports:\n          port: 8080/http\n\nIn this example $backend.host will have the value of the vamp.gateway-driver.host configuration parameter, while $backend.ports.port the next available port from vamp.operation.gateway.port-range.\nfrontend doesn't connect to backend directly but via Vamp Gateway Agent(s) - given on these host and port parameters.\nThis is quite simmilar to common pattern to access any clustered application.\nFor instance if you want to access DB server, you will have an address string based on e.g. DNS name or something simmilar.\nNote that even without Vamp, you would need to setup access to backend in some similar way.\nWith Vamp, access is via VGA's and that allows specific routing (conditions, weights) needed for A/B testing and canary releasing.\n\n  \nRead about using Vamp with virtual hosts\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 84
  },
  {
    "path": "/documentation/using vamp/v0.9.2/sla",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "SLA (Service Level Agreement)",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"sla-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 90",
    "aliases": "aliases:",
    "content": "\nSLA stands for \"Service Level Agreement\". Vamp uses it to define a pre-described set of boundaries to a service and the actions that should take place once the service crosses those boundaries. In essence, an SLA and its associated escalation is a workflow that is checked and controlled by Vamp based on the runtime behaviour of a service. SLAs and escalations are defined with the VAMP DSL.\n\nThe SLA event system\n\nYou can define an SLA for each cluster in a blueprint. A common example would be to check if the average response time of the cluster (averaged across all services) is higher or lower than a certain threshold. Under the hood, an SLA workflow creates two distinct events. These are are sent from Vamp and stored to Elasticsearch.\n\nEscalate for a specific deployment and cluster  \ne.g. if the response time is higher than the upper threshold.\nDeEscalate for a specific deployment and cluster  \ne.g. if the response time is lower than the lower threshold.\n\nSLA monitoring is a continuous background process with a configurable interval time. On each run an SLA workflow is executed for each deployment & cluster that has an SLA defined. Within the same SLA definition it's possible to define a list of escalations. Escalations are triggered by escalation events (Escalate/DeEscalate).\n\nThis means escalation events can be generated by the third party systems by sending them to Elasticsearch. This would allow scaling up or down to be triggered by basically any system that can POST a piece of JSON.\n\nSLA's are in essence pieces of code inside Vamp that stick to this event model and can use, if they want, the metrics and event data streaming out of Elasticsearch to make decisions on how things are and should be running.\n\n SLA types\n\nVamp currently ships with the following SLA types:\n\nresponsetimesliding_window\n\nResponse time with sliding window \n\nThe responsetimesliding_window SLA triggers events based on response times. \n\n Example - SLA defined inline in a blueprint.\n\nNotice the SLA is defined per cluster and acts on the first service in the cluster.\n\nNotice how the SLA is defined separately from the escalations. This is key to how Vamp approaches SLA's and how modular and extendable the system is.\n\n,name: sava\n\ngateways:\n  80: sava/webport\n\nclusters:\n\n  sava:                        # the sava cluster\n    services:\n      breed:\n        name: monarch\n        deployable: vamp/monarch\n        ports:\n          webport: 80\n\n      scale:\n        cpu: 1\n        memory: 1024MB\n        instances: 2\n\n    sla:                        # SLA applies to the first service in the sava cluster (monarch)\n      # Type of SLA.\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000   # Upper threshold in milliseconds.\n        lower: 100    # Lower threshold in milliseconds.\n      window:\n        interval: 600 # Time period in seconds used for\n                      # average response time aggregation.\n        cooldown: 600 # Time period in seconds. During this \n                      # period no new escalation events will \n                      # be generated. New event may be expected \n                      # not before cooldown + interval time has \n                      # been reached after the last event. \n     \n      # List of escalations.\n      escalations:\n        type: scale_instances\n          minimum: 1\n          maximum: 3\n          scale_by: 1\n\n  \nRead about Vamp escalations\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 85
  },
  {
    "path": "/documentation/using vamp/v0.9.2/sticky-sessions",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Sticky Sessions",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"sticky-sessions-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 130",
    "aliases": "aliases:",
    "content": "\nVamp supports route and instance level sticky sessions.\n\nRoute Level\n\nA common use case is when the end users have to have the same experience in A/B testing setup thus they should get the same service always (either A or B).\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: route                            setting the route level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nInstance Level \n\nA common use case is when the end users need to be served by the same instance (e.g. stateful application).\n\n,name: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: instance                          setting the instance level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nOther Notes\n\nResetting the sticky value can be done by: sticky: none or sticky: ~ (setting it to null).\n\nSticky sessions can be also used for gateways:\n\n,name: sava:1.0\ngateways:\n  9050/http:\n    sticky: service\n    routes:            let's say we have 2 clusters: sava1 (90%) and sava2 (10%)\n      sava1/port:   \n        weight: 90%\n      sava2/port:\n        weight: 10%\nclusters:\n  sava1: \n    ...\n  sava2: \n    ...\n\n  \nRead about using Vamp for service discovery\nCheck the API documentation\nTry Vamp\n  \n",
    "id": 86
  },
  {
    "path": "/documentation/using vamp/v0.9.2/virtual-hosts",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Virtual Hosts",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"virtual-hosts-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 140",
    "aliases": "aliases:",
    "content": "\nVamp can be configured to support virtual host via HAProxy:\n\nvamp.operation.gateway  \n\nExample - Virtual hosts\n \n PUT $ /api/v1/deployments/runner with body:\n\n,name: runner\n\ngateways:\n  9070: runner1/port\n  9080: runner2/port\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nNow you can request:\n\n$ curl --resolve 9070.runner.vamp:80:$  http://9070.runner.vamp\n \n\n$ curl --resolve 9080.runner.vamp:80:$  http://9080.runner.vamp\n \n  \nIf you are running Vamp in one of the quick setups, $  should have value of $  - See the hello world quick setup instructions.\n  \n\nVamp creates a virtual host for each gateway - name of the gateway (/ replaced with .) appended to value from vamp.gateway-driver.virtual-hosts-domain.\nIn case of above example:\n\n9050.runner.vamp\n9060.runner.vamp\nport.runner1.runner.vamp\nport.runner2.runner.vamp\n\nUsing Gateway API it is possible to get virtual hosts for each gateway, e.g.\nGET $ /api/v1/gateways\n\n Custom virtual hosts\n\nAs you could see each gateways has virtual_hosts field.\nUsing that field it is also possible to set list of custom virtual hosts.\nLet's see that in the following example:\n\n,name: runner\n\ngateways:\n  9080:\n    virtual_hosts: [\n      \"run.vamp.run\"\n    ]\n    routes:\n      runner1/port:\n        weight: 50%\n      runner2/port:\n        weight: 50%\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nIf you deploy this blueprint as runner and check gateway 9080/runner:\n\nGET $ /api/v1/gateways/runner/9080\n  http://run.vamp.run\n \n\n9080.runner.vamp is added if configuration parameter vamp.operation.gateway.virtual-hosts is set, otherwise just custom virtual hosts if any.\n\n  \nCheck the API documentation\nTry Vamp\n  \n",
    "id": 87
  },
  {
    "path": "/documentation/using vamp/v0.9.2/workflows",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Workflows",
    "menu": "menu:",
    "  main": " main:",
    "    identifier": "   identifier: \"workflows-v092\"",
    "    parent": "   parent: \"Using Vamp\"",
    "    weight": "   weight: 120",
    "aliases": "aliases:",
    "content": "\nVamp workflows are a convenient way to run Node JS based scripts that access the Vamp API to monitor and interact with running services. JavaScript workflows run in Vamp workflow agent containers (github.com/magneticio - Vamp workflow agent) and are managed just like any other container inside your cluster, making them robust, scalable and dynamic. Workflows can be scheduled to run as a daemon, be triggered by Vamp events or to run at specified times. \n\nVamp ships with three default workflows:\n\nHealth  - checks and stores the status of running services. Events stored by the health workflow are used by the Vamp UI.\nMetrics - stores metrics on running services. Events stored by the metrics workflow are used by the Vamp UI.\nKibana - supports the easy creation of Kibana dashboards.\n\nYou can create your own workflows using Node JS based scripts running inside a Vamp workflow agent container, or use another language of preference - create an application or script that accesses the Vamp API and build it into a Docker container to be deployed by Vamp.\n\nSchedules\n\nWorkflows can be scheduled to run as a daemon, be triggered by specific events or rn according to a time schedule. See the examples for each below.\n\n Scheduled as a daemon\nFor example:\nschedule: daemon\n\nTriggered by events\nFor example:\nschedule:\n  event:  event with following tags will trigger the workflow\n  deployments:sava\n  cluster:runner\n\nschedule:  # shortened notation in case of single event (still array can be used as above)\n  event: archive:bluprints\n\nScheduled by time\nperiod, start (optional, by default starts now) and repeat (optional, by default runs forever). The time schedule period is in ISO8601 repeating interval notation.   \nFor example:\nschedule:\n  time:\n    period: P1Y2M3DT4H5M6S\n    start: now  or e.g. start: 2016-12-03T08:15:30Z\n    repeat: 10\n\nExample JavaScript workflow\nThe below examples show a JavaScript workflow with a separately stored JavaScript breed.\nJavaScript breeds are executed by Vamp Workflow Agent (github.com/magneticio - Vamp workflow agent).  For additional information on using JavaScript to access the Vamp API check out the Vamp Node Client project (github.com/magneticio - Vamp node client).\n\n Metrics workflow\nname: metrics\nbreed: metrics\nstatus: running\nschedule: daemon\nenvironment_variables:\n  VAMPWORKFLOWEXECUTION_TIMEOUT: '7'\n  VAMPKEYVALUESTORECONNECTION: 192.168.99.100:2181\n  VAMPKEYVALUESTOREPATH: /vamp/workflows/metrics\n  VAMPWORKFLOWEXECUTION_PERIOD: '5'\n  VAMPKEYVALUESTORETYPE: zookeeper\n  VAMP_URL: http://192.168.99.100:8080\nscale:\n  cpu: 0.1\n  memory: 128.00MB\n  instances: 1\n\nMetrics breed\nThe JavaScript breed will be executed by Vamp Workflow Agent (github.com/magneticio - Vamp workflow agent).  \nYou can send the required JavaScript directly to the API to store as a breed:\n\nRequest syntax:\n\nPUT  \n/api/v1/breeds/metrics  \nContent-Type: application/javascript\nRequest body:\n\n'use strict';\n\nlet _ = require('highland');\nlet vamp = require('vamp-node-client');\n\nlet api = new vamp.Api();\nlet logger = new vamp.Log();\nlet metrics = new vamp.ElasticsearchMetrics(api);\n\nlet window = 30; // seconds\n\nfunction publish(tags, metrics)  \n\napi.gateways().each(function (gateway)  , 'Tt', window).each(function (response)  );\n\n  api.namify(gateway.routes).each(function (route)  , 'Tt', window).each(function (response)  );\n   );\n );\n\n  \nTry the tutorial Create a workflow that generates events\nRead about Sticky sessions\nCheck the API documentation\nTry Vamp\n  ",
    "id": 88
  },
  {
    "path": "/resources/community",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Join the Vamp community",
    "menu": "menu:",
    "  main": " main:",
    "    name": "   name: \"Community\"",
    "    parent": "   parent: \"Resources\"",
    "    weight": "   weight: 20",
    "content": "Vamp is an open source project, actively developed by Magnetic.io. We encourage anyone to pitch in with pull requests, bug reports etc. \n\nContribute to Vamp \nVamp is split into separate repos and projects. Check the source on Github for an overview of all key repos (github.com - magneticio).   \nFeel free to contribute with Github pull requests.\n\n Submit change or feature requests \nLet us know your change or feature requests.  \nSubmit an issue on github tagged \"feature proposal\". \n\nReport a bug \nif you find  bug, please report it!  \nSubmit an issue on github, including details of the environment you are running Vamp in.\n",
    "id": 89
  },
  {
    "path": "/resources/downloads/",
    "date": "2016-10-19T09:00:00+00:00",
    "title": "Downloads",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Resources\"",
    "    weight": "   weight: 10",
    "content": "\nBinaries\nVamp\nVamp Gateway Agent (VGA)\nVamp CLI\n\n Homebrew\nVamp CLI for MacOS X\n\nDocker images\nVamp Gateway Agent (VGA) and HAProxy\nVamp workflow agent\n\n Build from source\nBuild Vamp\nBuild Vamp Gateway Agent (VGA)\n\n--------,\nBinaries\n\n Vamp\nDownload: bintray.com/magnetic-io - Vamp  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher\n\nExample\nLet's assume that the Vamp binary is vamp.jar.\njava -Dlogback.configurationFile=logback.xml -Dconfig.file=application.conf -jar vamp.jar\n\nlogback.xml is the log configuration file (example logback.xml file)  \nVamp uses the Logback library. Additional information about using Logback and the log file configuration format can be found on the Logback project page (logback.qos.ch).\napplication.conf is the main Vamp configuration file (example application.conf file)  \nDefault values (github.com/magneticio - reference.conf) are loaded on start and application.conf may override any of them.\nProcessing configuration is based on the typesafe library. Additional information about syntax and usage can be found on the project page (github.com/typesafehub - config).\n\n Vamp Gateway Agent (VGA)\n\nDownload: bintray.com/magnetic-io - Vamp Gateway Agent\n\nDocumentation can be found on the project page (github.com/magneticio - Vamp Gateway Agent).\nVamp CLI\n\nDownload: bintray.com/magnetic-io - Vamp CLI  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher\n\n Manual install - Windows and Linux\nInside the extracted Vamp CLI binary package (bintray.com/magnetic-io - Vamp CLI) is a bin directory. Add it to your PATH statement, open a Console/CMD window and type vamp.  \nAfter installation, set Vamp’s host location:\n\nVamp’s host location specified as a command line option ( --host )\n\nvamp list breeds --host=http://192.168.59.103:8080\n\nVamp’s host location specified via the environment variable VAMP_HOST\n\nexport VAMP_HOST=http://192.168.59.103:8080\n\nHomebrew\n Vamp CLI for MacOS X\nDownload: We have Homebrew support to install the Vamp CLI on MacOS X  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher  \n\nHomebrew install - MacOS X\n\nbrew tap magneticio/vamp\nbrew install vamp\n\nAfter installation, check if everything is properly installed with vamp version, then export the location of the Vamp host and check that the CLI can talk to Vamp:\nexport VAMP_HOST=http://localhost:8080\nvamp info\n\n Docker images\n\nVamp Gateway Agent (VGA) and HAProxy\nVamp Gateway Agent (VGA) Docker images with HAProxy can be pulled from the Docker hub (hub.docker.com - magneticio Vamp Gateway Agent).\n\n Vamp Workflow Agent\nA container for running small JavaScript-based workflows can be pulled from the Docker hub (hub.docker.com - magneticio Vamp workfow agent).\n Usually this will be pulled automatically.\n\nBuild from source\n\n Build Vamp\nRequirements: OpenJDK or Oracle Java version of 1.8.0_40 or higher, git (git-scm.com), sbt (scala-sbt.org), npm (npmjs.com) and Gulp (gulpjs.com)  \n  \nIf you build from source (master branch) without a specific tag, you will build katana not the official release. Check the katana documentation for details of all changes since the last official release.\n  \n\nCheckout the source from the official repo (github.com/magneticio - Vamp):   \n    \n  master branch contains the latest released version (e.g. 0.9.2). Versions are tagged.\n  vamp-ui is a separate project added as a git submodule to Vamp (ui subdirectory) it is, therefore, necessary to also checkout the submodule  \n    \n  git clone --recursive git@github.com:magneticio/vamp.git  \n  or specific branch: git clone --recursive --branch 0.9.2 git@github.com:magneticio/vamp.git\n\nRun ./build-ui.sh && sbt test assembly\nAfter the build ./bootstrap/target/scala-2.11 directory will contain the binary with name matching vamp-assembly-*.jar\n\nCheck this example: github.com/magneticio - Vamp docker quick start make.sh.\n\nRunning Vamp\n\nLet’s assume that Vamp binary is vamp.jar. OpenJDK or Oracle Java version of 1.8.0_40 or higher needs to be installed. Check the version with java -version\n\nExample:\n\n`java -Dlogback.configurationFile=logback.xml -Dconfig.file=application.conf -jar vamp.jar\n\nlogback.xml` is log configuration. Vamp uses the Logback library and additional information about using the Logback and log file configuration format can be found on the Logback project page. An example file can be found here.\n\napplication.conf is the main Vamp configuration file. Default values are loaded on start and application.conf may override any of them. Processing configuration is based on this library. Additional information about syntax and usage can be found on the library project page. An example of configuration can be found here.\n\n Build Vamp Gateway Agent (VGA)\n\nRequirements: Go (golang.org) and git (git-scm.com)\n  \nIf you build from source (master branch) without a specific tag, you will build katana not the official release. Check the katana documentation for details of all changes since the last official release.\n  \n\nCheckout the source from the official repo (github.com/magneticio - Vamp gateway agent). Current master branch is backward compatible with the latest 0.9.2 Vamp build.\nSet Go variables depending on target environment\nRun:\n\ngo get github.com/tools/godep\ngodep restore\ngo install\nCGO_ENABLED=0 go build -v -a -installsuffix cgo\n\nCheck this example: github.com/magneticio - clique-base make.sh. More details can found on the project page: github.com/magneticio - Vamp gateway agent.\n",
    "id": 90
  },
  {
    "path": "/search",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Search",
    "type": "\"page\"",
    "layout": "\"search\"",
    "content": "\n\n",
    "id": 91
  },
  {
    "path": "/support",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "support   ",
    "content": "Vamp is actively developed by Magnetic.io. Vamp Community Edition is open source and Apache 2.0 licensed. For details of the Vamp Enterprise Edition please check the Vamp feature matrix or contact us to discuss your requirements, pricing and features.\n\nCommunity support\nIf you have a question about Vamp, please check the Vamp documentation first  - we're always adding new resources, tutorials and examples.\n\nBug reports: If you found a bug, please report it! Create an issue on GitHub and provide as much info as you can, specifically the version of Vamp you are running and the container driver you are using.\nGitter: You can post questions directly to us on our public Gitter channel  \nTwitter: You can also follow us on Twitter: @vamp_io\n\n Professional support\nWe offer five extended professional support plans. Contact us for pricing information, and details of professional services and consultancy: info@magnetic.io or call +31(0)88 555 33 99\n\n  \nTry Vamp\nLearn how Vamp works\nGet your teeth into the Vamp documentation\n  \n",
    "id": 92
  },
  {
    "path": "/why use vamp/enterprise-edition",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Vamp Enterprise Edition",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"why-use-vamp-top\"",
    "    name": "   name: \"Enterprise edition (EE)\"",
    "    weight": "   weight: 60",
    "content": "\nThe Vamp Enterprise Edition (EE) extends the features available in the open source Vamp Community Edition.  \n\nVamp feature matrix\n\nContact us for further details and pricing: info@magnetic.io or call +31(0)88 555 33 99",
    "id": 93
  },
  {
    "path": "/why use vamp/feature-list",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Feature list",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"why-use-vamp-top\"",
    "    weight": "   weight: 50",
    "content": "\nVamp 0.9.2 includes:\n\nContainer-scheduler agnostic API\nPercentage and condition based programmable routing\nYAML based configuration blueprints with support for dependencies, clusters and environment variables\nGraphical UI and dashboard\nIntegrated javascript-based workflow system\nMetric-driven autoscaling, canary releasing and other optimisation and automation patterns\nAutomatic loadbalancing for autoscaled services\nAPI gateway routing features like conditional rewrites\nCLI for integration with common CI/CD pipelines\nOpen source (Apache 2.0)\nEvent API and server-side events (SSE) stream\nMulti-level metric aggregation\nPort-based, virtual host names or external service (consul etc) based service discovery support\nLightweight design to run in high-available mission-critical architectures\nIntegrates with ELK stack (Elastic Search, Logstash, Kibana) for custom Kibana dashboards\nVamp Runner provides automated integration and workflows testing\n\n  \nTry Vamp\nUse cases: Vamp solutions to practical problems\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n  \n",
    "id": 94
  },
  {
    "path": "/why use vamp/get-started",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Get started",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"why-use-vamp-top\"",
    "    weight": "   weight: 70",
    "content": "\nTry Vamp\nTo make the most of your 'my first Vamp' experience, we suggest you start by installing our single, all-in-one Vamp Docker Hello World package. This will set up everything you need to play around with Vamp on a local or remote dev machine - the container package includes Mesos/Marathon and Elastic Stack (ELK), as well as all the necessary Vamp components. We have some nice tutorials to get a feel for the supernatural powers of Vamp.\n\n Install a production-grade Vamp setup\nOf course our Hello World package is no production-grade setup. We suggest your next step should be to understand the Vamp architecture and then find the Vamp version for your favorite container scheduler. We support most common container schedulers, so you should be able to find one to your liking in our installation docs. If you're still not sure which container scheduler to work with, our 'what to choose' guide can help you make an informed decision.\n\nFine tune and integrate\nAfter you've successfully installed a production-grade Vamp on your preferred container-cluster manager/scheduler (if you need help here, find us in our public Gitter channel), it's time to either dive into the ways you can use Vamp or investigate how you can configure and fine-tune Vamp to match your specific requirements. It might also be interesting to integrate Vamp into your CI pipeline to create a CD pipeline with Vamp's canary-releasing features. You can check out our CLI and REST API documentation for integrations.\n\n Get your teeth into the fun stuff!\nAt this point you've become a real Vamp guru. The next step could be to start playing around with our Vamp Runner tool to investigate typical recipes, such as automated canary-releasing, auto-scaling and more. You can use the JavaScript-based workflows in the recipes as a reference to create your own recipes and workflows. Once you've created some cool workflows and recipes we would of course like to hear from you!\n\n  \nTry the Vamp Docker Hello World package and tutorials\nRead about the Vamp architecture\nCheck the installation docs\n  ",
    "id": 95
  },
  {
    "path": "/why use vamp/see-vamp-in-action",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "See Vamp in action",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"why-use-vamp-top\"",
    "    weight": "   weight: 40",
    "content": "\nWatch this video of running Vamp on DC/OS an doing a canary release.\n\nThe Vamp UI and DC/OS versions in this video are a little older than currently available. We will upload a new recording shortly.\n\n  \n",
    "id": 96
  },
  {
    "path": "/why use vamp/use cases/create-responsive-website",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Canary test and release a responsive frontend",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        name": "       name: \"Create responsive website\"",
    "        weight": "       weight: 20",
    "content": "\n“We need to upgrade our website frontend to make it responsive”\n  \nDeveloping a responsive web frontend is often a major undertaking, requiring a large investment of hours and extensive testing. Until you go live, it's difficult to predict how the upgrade will be received by users - will it actually improve important metrics, will it work on all browser, devices and resolutions, etc.?   \n\nBut why develop this new responsive frontend in one go, having to go for the dreaded and risky big-bang release? Using Vamp you can apply a canary release to introduce the new frontend to a selected cohort of users, browsers and/or devices. This would require a minimal investment of development and delivering real usage data:\n\nStart small: Build the new frontend for only one specific browser/resolution first to measure effectiveness. Vamp can deploy the new responsive frontend and route a percentage of supported users with this specific browser and screen-resolution there. All other users will continue to see the old version of your website.\nOptimise: With the new responsive frontend in the hands of real users with this specific browser/resolution, you can measure actual data and optimise accordingly without negatively affecting the majority of your users.\nScale up: Once you are satisified with the performance of the new frontend, you can use Vamp to scale up the release, developing and canary releasing one browser/resolution at a time. Of course other cohort combinations are also possible, Vamp is open and supports all HAProxy ACL rules.\n\n  \nRead about using Vamp to resolve client-side incompatibilities after an upgrade\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  ",
    "id": 97
  },
  {
    "path": "/why use vamp/use cases/modernise-architecture",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Test and modernise architecture",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        weight": "       weight: 40",
    "        name": "       name: \"Modernise architecture\"",
    "content": "\n\"We want to switch to a NoSQL database for our microservices, but don't know which solution will run best for our purposes\"\n\nWith multiple NoSQL database options available, it's hard to know which is the best fit for your specific circumstances. You can try things out in a test lab, but the real test comes when you go live with production load.\n\nWhy guess? Using Vamp you could A/B test different versions of your services with different NoSQL backends, in production, and then use real data to make an informed and data-driven decision.   \n\nDeploy two versions: Vamp can deploy multiple versions of your architecture, each with a different database solution (or other configuration settings), then distribute incoming traffic across each.\nStress test: Use the metrics reported by Vamp to measure which option performs best in production.\nKeep the best performing option: Once you have made your decision, Vamp can route all traffic to your chosen architecture. Services from the alternative options will be drained to ensure customer experience is not impacted by the test.\n\n  \nRead about using Vamp to simulate and test scaling behaviour\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  \n",
    "id": 98
  },
  {
    "path": "/why use vamp/use cases/refactor-monolithic-to-microsystems",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Move from monoliths and VM's to microservices",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        name": "       name: \"Refactor to microservices\"",
    "        weight": "       weight: 40",
    "content": "\n“We want to move to microservices, but we can’t upgrade all components at once and want to do a gradual migration”\n  \nRefactoring a monolithic application to a microservice architecture is a major project. A big bang style re-write to upgrade all components at once is a risky approach and requires a large investment in development, testing and refactoring.\n\nWhy not work incrementally? Using Vamp's routing you could introduce new services for specific application tiers like the frontend or business logic layers, and move traffic with specific conditions to these new services. These services in turn can connect to your legacy systems again, using Vamp's proxying. A typical example would be introducing an angular based frontend or node.js based API microservice. You can send 2% of your incoming traffic to this new microservice frontend, which in turn connects with the legacy backend system. This way you can test your new microservices in a small and controlled way and avoid a big bang release. You can introduce new services one by one, test them in production and increase traffic until you migrated your entire application from a monolith to microservices.\n\nStart small: You can build e.g. one new frontend component. Vamp will deploy this to run alongside the legacy monolithic system.\nActivate smart routing: Vamp can route traffic behind the scenes, so a small percentage of visitors is sent to the new frontend service, while the new frontend is routed by Vamp to the legacy backend. You can continue transferring components from the legacy monolithic system to new microservices and Vamp can adapt the routing as you go.\nRemove legacy components: Once all services have been transferred from the legacy monolith, you can start removing components from the legacy system.\n\n  \nRead about using Vamp to test and modernise architecture\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  ",
    "id": 99
  },
  {
    "path": "/why use vamp/use cases/resolve-incompatibilities-after-upgrade",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Resolve client-side incompatibilities after an upgrade",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        name": "       name: \"Resolve incompatibilities\"",
    "        weight": "       weight: 30",
    "content": "\n“We upgraded our website self-management portal, but our biggest client is running an unsupported old browser version”\n  \nLeaving an important client unable to access your services after a major upgrade is a big and potentially costly problem. The traditional response would be to rollback the upgrade asap - if that's even possible.  \n\nWhy rollback? Using Vamp's smart conditional routing you could send specific clients or browser-versions to an older version of your portal while others can enjoy the benefits of your new upgraded portal. Because Vamp supports SLA based autoscaling for (Docker) containers, you can deploy the old version on the same infrastructure as the new version is running on. This also avoids having to provision costly over dimensioned DTAP environments for only a small user-base, leveraging your existing infrastructure efficiently.\n\nRe-deploy: Vamp can (re)deploy a (containerised) compatible version of your portal to run side-by-side with the upgraded version.\nActivate smart routing: Vamp can route all users with e.g. a specific IP, browser or location to a compatible version of the portal. Other clients will continue to see the new upgraded portal.\nResolve the incompatibility: Once the client upgrades to a compatible browser Vamp can automatically route them to the new portal version.\n\n  \nRead about using Vamp to move from monoliths and VMs to microservices\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  ",
    "id": 100
  },
  {
    "path": "/why use vamp/use cases/self-healing-and-self-optimising",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Self-healing and self-optimising",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        weight": "       weight: 60",
    "content": "\n\"Our website traffic can be unpredictable, it's hard to plan and dimension the exact resources we're going to need to run within SLA's\"\n\nWhy overdimension your whole system? Using Vamp you can auto-scale individual services based on clearly defined SLAs (Service Level Agreements). It's also easy to create advanced workflows for up and down scaling, based on your application or business specific requirements. Vamp can also make sure that unhealthy and failing services are corrected based on clearly defined metrics and treshholds.\n\nSet SLAs: You can define SLA metrics, tresholds and escalation workflows. You can do this in Vamp YAML blueprints, modify our packaged workflows, or create your own workflow scripts for advanced use-cases.\nOptimise: Vamp workflows can automatically optimise your running system based on metrics that are relevant to your application or services.\nSleep easy: Vamp will track troughs and spikes in activity and automatically scale services up and down to match your SLAs. All scaling events will be logged. Unhealthy services can be healed by Vamp.\n\n  \nRead about using Vamp for service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  ",
    "id": 101
  },
  {
    "path": "/why use vamp/use cases/service-discovery",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Service discovery",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        identifier": "       identifier: \"use-cases-service-disc\"",
    "        weight": "       weight: 70",
    "content": "\nVamp uses a service discovery pattern called server-side service discovery. This pattern allows service discovery without the need to change your code or run any other service-discovery daemon or agent. In addition to service discovery, Vamp also functions as a service registry. We recognise the following benefits of this pattern:\n\nNo code injection needed\nNo extra libraries or agents needed\nPlatform/language agnostic: it’s just HTTP\nEasy integration using ENV variables\n\nVamp doesn't need point-to-point wiring. Vamp uses environment variables that resolve to service endpoints Vamp automatically sets up and exposes. Even though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc. Vamp can also integrate with common service discovery solutions like Consul and read from these to setup the required routing automatically.\n\nSmartStack\nVamp can automatically deploy a VampGatewayAgent(VGA)+HAProxy on every node of your container cluster. This creates a so-called Layer 7 intra service network mesh which enables you to create a \"SmartStack\", an automated service discovery and registration framework originally coined and developed by AirBnB. More on the history and advantages of the SmartStack approach can be read here (nerds.airbnb.com - SmartStack: Service Discovery in the Cloud).\n\n  \nRead more about Vamp service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  \n",
    "id": 102
  },
  {
    "path": "/why use vamp/use cases/simulate-and-test-scaling-behaviour",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Simulate and test scaling behaviour",
    "menu": "menu:",
    "    main": "   main: ",
    "        parent": "       parent: \"Use cases\"",
    "        weight": "       weight: 50",
    "content": "\n\"How would our system react if... the number of users increased x10 ... the response time of a service increased with 20 seconds ... an entire tier of our application would be killed ...\"  \n\nYour company might dream of overnight success, but what if it actually happened? Stress tests rarely cater to extreme real world circumstances and usage patterns, and are often done on systems that are not identical to production environments. It's not uncommon the bottleneck sits in the system generating the load itself, so it's difficult to predict how your microservices would actually scale or to know if your planned responses will really help.\n\nWhy not find out for sure? Using Vamp you can test your services and applications against difficult to predict or simulate situations, mocking all kinds of metrics, and then validate and optimise the workflows that handle the responses, like for example auto up and down scaling. With the same workflows as would be running in production, on the same infrastructure, with the same settings.\n\nMock required load: Vamp can simulate (mock) high-stress situations for any kind of metric your system needs to respond to, without actually having to generate real traffic.\nOptimise: You can optimise your resource allocation and autoscaling configurations based on real validated behaviour under stress.\nIterate until you're certain: Vamp can repeat the tests until you're confident with the outcome. Then you can use the same scaling and optimising workflows in production.\n\n  \nRead about using Vamp for self healing and self optimising\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n  ",
    "id": 103
  },
  {
    "path": "/why use vamp/use cases/use-cases",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Use cases",
    "aliases": "aliases:",
    "menu": "menu:",
    "    main": "   main:",
    "      parent": "     parent: \"Use cases\"",
    "      identifier": "     identifier: \"use-cases-overview\"",
    "      weight": "     weight: 10",
    "content": "\nThe integrated deployment, routing and workflow features of Vamp support a broad range of scenarios and industry verticals. We specifically see powerful use cases in the areas of testing in production, migrating to microservices, and realtime system optimisation. In this section we describe specific scenarios and how Vamp can effectively solve these.\n\nTesting in production \nUse canary testing and releasing to introduce a responsive web frontend. Read more ...\nResolve client-side incompatibilities after an upgrade. Read more ...\nA/B test architectural changes in production. Read more ...\n\n Migrating to microservices\nMove from VM based monoliths to modern microservices. Read more ...\n\nRealtime system optimisation\n\nWhat would happen if...... Simulate and test auto scaling behaviour. Read more ...\nSelf-healing and self-optimising. Read more ...\n\nWe're always interested to hear specific use-cases. If you have one to share, send us an email at info@magnetic.io",
    "id": 104
  },
  {
    "path": "/why use vamp/vamp-compared-to/frameworks-and-tools",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Frameworks and tools",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Vamp compared to...\"",
    "    weight": "   weight: 30",
    "content": "\nVamp compared to CI/CD tools\nSpinnaker, Jenkins, Wercker, Travis, Bamboo    \nVamp closes the loop between the development and operations elements of a CI/CD pipeline by enabling the controlled introduction of a deployable into production (canary-test and canary-release) and feeding back runtime technical and business metrics to power automated optimisation workflows (such as autoscalers). Vamp integrates with CI/CD tools like Travis, Jenkins or Wercker to canary-release and scale the built deployables they provide. The initial deployment setup is defined in a YAML blueprint (e.g. deployable details, required resources, routing filters) and is typically provided by the CI tool as a template to the Vamp API. Vamp will then run, canary-release, monitor and scale the deployment based on the filters and conditions specified in the blueprint.\n\n Vamp compared to feature toggle frameworks\nLaunchDarkly, Togglz, Petri  \nFeature toggle frameworks use code level feature toggles to conditionally test new functionality in an application or service. Tools such as LaunchDarkly and Togglz work with these toggles to enable, for example, A/B testing and canary functionality. While there are many cases for using feature toggles, there are also times when feature toggling isn't the smartest choice.\nVamp allows controlled testing of new features without the need to adjust your code. To achieve this, Vamp controls traffic routing towards and between your applications and services based on blueprint descriptions of individual (micro)services and their dependencies. This makes sense on an application code level, offering increased security and reduced technical debt compared to maintaining toggles in your code, and provides a mature alternative for cases when feature toggles are not the appropriate choice.\n\nVamp compared to configuration management and provisioning tools\nPuppet, Ansible, Chef, Terraform    \nThe responsbilities of configuration management and infrastructure provisioning tools are often stretched to cover container deployment features. These tools were not intended for handling container deployments or for the dynamic management and routing traffic over these containers. Vamp has been designed and developed from the ground up specifically to fit these use cases.  \n\n Vamp compared to custom built solutions\nBuilding and maintaining a scalable and robust enterprise-grade system for canary-testing and releasing is not trivial. Vamp delivers programmable routing and automatic load balancing, deployment orchestration and workflows, as well as a powerful event system, REST API, graphical UI, integration testing tools and a CLI.  \n\nVamp compared to A/B and MVT testing tools\nOptimizely, VisualWebsiteOptimizer, Google Analytics, Planout  \nVamp enables canary testing versions of applications, effectively providing A/B and MVT testing of applications and services by deploying two or more versions of an application or service and dividing incoming traffic between the running versions. Vamp doesn't have a built-in analytics engine though, so the analysing of the relevant metrics needs to be done with a specific Vamp workflow or an external analytics engine. Results can be fed back to Vamp to automatically update routing rules and deployments to push a winning version to a full production release. Because of the flexible programmable routing and use of environment variables, Vamp can be used to canary test almost everything, from content and business logic to configuration settings and architectural changes.  \n\n Vamp compared to DevOps tools\nDeis, Flynn, Dokku  \nVamp has no ambition to provide a Heroku-like environment for containers. Vamp integrates programmable routing and load balancing, container deployments and orchestration to enable canary testing and canary releasing features. Vamp also adds metrics-driven workflows for auto-scaling and other optimisations. Vamp sees business as a first class citizen in DevOps teams, providing a graphical UI and tools for non-technical roles.   \n\n  \nTry Vamp\nUse cases -  some Vamp solutions to practical problems\nFind out how Vamp works\n  \n\n",
    "id": 105
  },
  {
    "path": "/why use vamp/vamp-compared-to/paas-and-container-systems",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "PaaS and container systems",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Vamp compared to...\"",
    "    weight": "   weight: 20",
    "content": "\nVamp compared to container schedulers and container clouds (CPaaS)\nDocker Swarm, DC/OS, Mesos/Marathon, Kubernetes, Nomad, Rancher, AWS ECS, Azure CS, Mantl, Apollo  \nContainer cluster managers and schedulers like Marathon, DC/OS, Kubernetes, Nomad or Docker Swarm provide great features to run containers in clustered setups. What they don't provide are features to manage the lifecycle of a microservices or container based system. How to do continuous delivery, how to gradually introduce and upgrade versions in a controlled and risk-free way, how to aggregate metrics and how to use these metrics to optimise and scale your running system. Vamp adds these features on top of well-used container schedulers by dynamically managing routing and load balancing, deployment automation and metric driven workflows. Vamp also adds handy features like dependencies, ordering of deployments and resource management.\n\n Vamp compared to PaaS systems\nCloud foundry, OpenStack, IBM Bluemix, Openshift  \nVamp adds an experimentation layer to PaaS infrastructures by providing canary-releasing features that integrate with common PaaS proxies like HAProxy. For continuous delivery and auto-scaling features, Vamp integrates with common container-schedulers included in PaaS systems, like Kubernetes in Openshift V3.   \n\n  \nRead about Vamp compared to common frameworks and tools\nTry Vamp\nFind out how Vamp works\n  \n\n",
    "id": 106
  },
  {
    "path": "/why use vamp/vamp-compared-to/proxies-and-load-balancers",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Proxies and load balancers",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"Vamp compared to...\"",
    "    weight": "   weight: 10",
    "content": "\nVamp compared to software based proxies and load balancers\nHAProxy, NGINX, linkerd, Traefik   \nVamp adds programmable routing (percentage and condition based) and load balancing to the battle-tested HAProxy proxy, as well as a REST API, graphical UI and CLI.  This means you can use Vamp together with all common container-schedulers to provide continuous delivery and auto-scaling features using automatic load balancing and clustering of scaled out container instances. By default Vamp is packaged with HAProxy, but you could also integrate the Vamp magic with other programmable proxies such as NGINX, linkerd or Traefik.\n\n  \nRead about Vamp compared to PaaS and container systems\nTry Vamp\nFind out how Vamp works\n  \n\n",
    "id": 107
  },
  {
    "path": "/why use vamp/why-use-vamp",
    "date": "2016-09-13T09:00:00+00:00",
    "title": "Why use Vamp?",
    "aliases": "aliases:",
    "menu": "menu:",
    "  main": " main:",
    "    parent": "   parent: \"why-use-vamp-top\"",
    "    identifier": "   identifier: \"why-use-vamp-page\"",
    "    weight": "   weight: 5",
    "content": "\nWe recognise the pain and risk involved with delivering microservice applications.  We've been there too - facing downtime and unexpected issues while transitioning from one release to the next. \nIn microservice architectures, these concerns can quickly multiply. It's all too easy to get stuck dealing with the added complexities and miss out on the potential benefits. \n\nWhat is Vamp?\n\nVamp is an open source, self-hosted platform for managing (micro)service oriented architectures that rely on container technology. Vamp provides a DSL to describe services, their dependencies and required runtime environments in blueprints and a runtime/execution engine to deploy these blueprints (similar to AWS Cloudformation). Planned deployments and running services can be managed from your choice of Vamp interface - graphical UI, command line interface or RESTful API. \n\nVamp takes care of route updates, metrics collection and service discovery, so you can easily orchestrate complex deployment patterns, such as architecture level A/B testing and canary releases.\nAfter deployment, Vamp workflows monitor running applications and can act automatically based on defined SLAs.  \n\n Vamp facts\n\nVamp is platform agnostic\nVamp is functionality agnostic, but functions well in an API centric, event driven and stateless environment. \nVamp is not a strict container platform, but uses the power of container platforms under the hood.\nVamp is written in Scala, Go and ReactJS \nVamp includes clear SLA management and service level enforcement out of the box\nVamp is an open source project, actively developed by Magnetic.io\n\n  \nTry Vamp\nRead the full Vamp feature list\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n  \n\n",
    "id": 108
  }
]
